"""
Agent-as-a-Judge: ë‹µë³€ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ (Phase 7.2)
===================================================
ë©”ì¸ ì—ì´ì „íŠ¸ê°€ ì‘ì„±í•œ í…ìŠ¤íŠ¸ ë‹µë³€ì„ í‰ê°€ ì—ì´ì „íŠ¸ê°€ ê°€ì´ë“œë¼ì¸ê³¼ ë¹„êµí•˜ì—¬
ì ìˆ˜í™”í•˜ê³ , ê¸°ì¤€ì¹˜ ë¯¸ë‹¬ ì‹œ ìë™ìœ¼ë¡œ ì¬ì¶”ë¡ ì„ ëª…ë ¹í•©ë‹ˆë‹¤.

Evaluation Criteria (v2.9):
    1. Factual Accuracy (ì‚¬ì‹¤ ì •í™•ì„±) - 35%
    2. Logical Consistency (ë…¼ë¦¬ì  ì¼ê´€ì„±) - 20%
    3. Guideline Compliance (ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜) - 15%
    4. Completeness (ì™„ì „ì„±) - 15%
    5. Clinical Insight (ì„ìƒì  í†µì°°) - 15%  â† NEW in Phase 7.2

Workflow:
    Answer â†’ Judge Evaluation â†’ Score < Threshold â†’ Re-reason â†’ Re-evaluate
"""

import json
import re
import logging
from typing import Optional, Dict, Any, List, Tuple
from dataclasses import dataclass
from enum import Enum
import requests

logger = logging.getLogger(__name__)


class JudgeVerdict(Enum):
    """í‰ê°€ íŒì •"""
    APPROVED = "approved"           # ìŠ¹ì¸: ë‹µë³€ í’ˆì§ˆ ì¶©ë¶„
    REVISION_REQUIRED = "revision_required"  # ìˆ˜ì • í•„ìš”
    REJECTED = "rejected"           # ê±°ë¶€: ì¬ìƒì„± í•„ìš”


@dataclass
class EvaluationCriteria:
    """í‰ê°€ ê¸°ì¤€ë³„ ì ìˆ˜ (v2.9 - Phase 7.2)"""
    factual_accuracy: float      # 0-100: ì‚¬ì‹¤ ì •í™•ì„±
    logical_consistency: float   # 0-100: ë…¼ë¦¬ì  ì¼ê´€ì„±
    guideline_compliance: float  # 0-100: ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜
    completeness: float          # 0-100: ì™„ì „ì„±
    clinical_insight: float      # 0-100: ì„ìƒì  í†µì°° (Phase 7.2 NEW)


@dataclass
class JudgeResult:
    """í‰ê°€ ê²°ê³¼"""
    verdict: JudgeVerdict
    total_score: float           # 0-100
    criteria: EvaluationCriteria
    issues_found: List[str]      # ë°œê²¬ëœ ë¬¸ì œì 
    suggestions: List[str]       # ê°œì„  ì œì•ˆ
    reasoning: str               # í‰ê°€ ê·¼ê±°


@dataclass
class JudgeConfig:
    """í‰ê°€ ì‹œìŠ¤í…œ ì„¤ì • (Phase 7.4: íƒ€ì„ì•„ì›ƒ í™•ì¥)"""
    ollama_url: str = "http://localhost:11434"
    judge_model: str = "glm4:9b"  # ë¹ ë¥¸ í‰ê°€ìš© SLM
    timeout: int = 120                 # Phase 7.4: 2ë¶„ (ë³µì¡í•œ ë‹µë³€ í‰ê°€)
    approval_threshold: float = 80.0   # ì´ ì ìˆ˜ ì´ìƒì´ë©´ ìŠ¹ì¸
    revision_threshold: float = 60.0   # ì´ ì ìˆ˜ ë¯¸ë§Œì´ë©´ ê±°ë¶€
    weights: Dict[str, float] = None   # í‰ê°€ ê¸°ì¤€ ê°€ì¤‘ì¹˜

    def __post_init__(self):
        if self.weights is None:
            # Phase 7.2: ì„ìƒì  í†µì°° ì¶”ê°€ (v2.9)
            self.weights = {
                "factual_accuracy": 0.35,       # 35%
                "logical_consistency": 0.20,    # 20%
                "guideline_compliance": 0.15,   # 15%
                "completeness": 0.15,           # 15%
                "clinical_insight": 0.15        # 15% (Phase 7.2 NEW)
            }


class AgentJudge:
    """
    Agent-as-a-Judge: ë‹µë³€ í’ˆì§ˆ í‰ê°€ì

    Usage:
        judge = AgentJudge()
        result = judge.evaluate(
            question="BI-RADS 4Aì˜ ì•…ì„± í™•ë¥ ì€?",
            answer="3-10%ì…ë‹ˆë‹¤.",
            reference_knowledge=knowledge
        )

        if result.verdict == JudgeVerdict.REJECTED:
            # ì¬ì¶”ë¡  í•„ìš”
            pass
    """

    def __init__(self, config: Optional[JudgeConfig] = None):
        self.config = config or JudgeConfig()

    def evaluate(
        self,
        question: str,
        answer: str,
        derivation: str = "",
        reference_knowledge: str = "",
        context: str = ""
    ) -> JudgeResult:
        """
        ë‹µë³€ í‰ê°€ ìˆ˜í–‰

        Args:
            question: ì›ë³¸ ì§ˆë¬¸
            answer: í‰ê°€í•  ë‹µë³€
            derivation: ì¦ëª… ê³¼ì • (ìˆëŠ” ê²½ìš°)
            reference_knowledge: ì°¸ì¡°ìš© í‘œì¤€ ì§€ì‹
            context: ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸

        Returns:
            JudgeResult: í‰ê°€ ê²°ê³¼
        """
        logger.info(f"Evaluating answer for: {question[:50]}...")

        system_prompt = """ë‹¹ì‹ ì€ ìœ ë°©ì˜ìƒì˜í•™ ë‹µë³€ í’ˆì§ˆ í‰ê°€ê´€ì…ë‹ˆë‹¤ (v2.9 - Phase 7.2).
ì£¼ì–´ì§„ ë‹µë³€ì„ í‘œì¤€ ì§€ì‹ ë° ê°€ì´ë“œë¼ì¸ê³¼ ëŒ€ì¡°í•˜ì—¬ ì ìˆ˜ë¥¼ ë§¤ê¸°ì„¸ìš”.

## í‰ê°€ ê¸°ì¤€ (ê° 0-100ì )

1. **Factual Accuracy (ì‚¬ì‹¤ ì •í™•ì„±)** [35%]:
   - 100: ëª¨ë“  ìˆ˜ì¹˜ì™€ ìš©ì–´ê°€ í‘œì¤€ê³¼ ì •í™•íˆ ì¼ì¹˜
   - 80-99: ëŒ€ë¶€ë¶„ ì •í™•, ì‚¬ì†Œí•œ ì˜¤ë¥˜
   - 60-79: í•µì‹¬ì€ ë§ì§€ë§Œ ì¼ë¶€ ë¶€ì •í™•
   - 40-59: ìƒë‹¹ ë¶€ë¶„ ë¶€ì •í™•
   - 0-39: ì‹¬ê°í•œ ì˜¤ë¥˜

2. **Logical Consistency (ë…¼ë¦¬ì  ì¼ê´€ì„±)** [20%]:
   - 100: ì¶”ë¡  ê³¼ì •ì´ ì™„ë²½í•˜ê²Œ ì—°ê²°
   - 80-99: ë…¼ë¦¬ì ì´ë‚˜ ì•½ê°„ì˜ ë¹„ì•½
   - 60-79: ëŒ€ì²´ë¡œ ë…¼ë¦¬ì ì´ë‚˜ ì¼ë¶€ í—ˆì 
   - 40-59: ë…¼ë¦¬ì  ë¹„ì•½ ë‹¤ìˆ˜
   - 0-39: ë…¼ë¦¬ êµ¬ì¡° ë¶€ì¬

3. **Guideline Compliance (ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜)** [15%]:
   - 100: BI-RADS/ACR ìš©ì–´ì™€ ê¸°ì¤€ ì™„ë²½ ì¤€ìˆ˜
   - 80-99: ëŒ€ë¶€ë¶„ ì¤€ìˆ˜, ê²½ë¯¸í•œ ë¶ˆì¼ì¹˜
   - 60-79: ê¸°ë³¸ ì¤€ìˆ˜í•˜ë‚˜ ì¼ë¶€ ìœ„ë°˜
   - 40-59: ìƒë‹¹ ë¶€ë¶„ ìœ„ë°˜
   - 0-39: ê°€ì´ë“œë¼ì¸ ë¬´ì‹œ

4. **Completeness (ì™„ì „ì„±)** [15%]:
   - 100: ì§ˆë¬¸ì˜ ëª¨ë“  ì¸¡ë©´ì— ë‹µë³€
   - 80-99: í•µì‹¬ ë‹µë³€ ì™„ë£Œ, ë¶€ê°€ ì„¤ëª… ë¶€ì¡±
   - 60-79: ì£¼ìš” ë¶€ë¶„ ë‹µë³€, ì¼ë¶€ ëˆ„ë½
   - 40-59: ë¶€ë¶„ì  ë‹µë³€
   - 0-39: ë¶ˆì™„ì „

5. **Clinical Insight (ì„ìƒì  í†µì°°)** [15%] â† Phase 7.2 NEW:
   - 100: FN/FP ë°œìƒ ê¸°ì „, ì„ìƒì  ì˜í–¥ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì™„ë²½ ê¸°ìˆ 
   - 80-99: ì„ìƒì  ë§¥ë½ ì œì‹œ, ê²½ë¯¸í•œ ëˆ„ë½
   - 60-79: ê¸°ë³¸ì  ì„ìƒ ì—°ê´€ì„± ì–¸ê¸‰
   - 40-59: ì„ìƒì  ë§¥ë½ ë¶€ì¡±, ë‹¨ìˆœ ìˆ˜ì¹˜ë§Œ ì œì‹œ
   - 0-39: ì„ìƒì  ì˜ë¯¸ ì „ë¬´

   â€» ì„ìƒì  í†µì°° í‰ê°€ ê¸°ì¤€:
   - ë…¸ì´ì¦ˆ ì¦ê°€ â†’ CNR ì €í•˜ â†’ ë¯¸ì„¸ì„íšŒí™” ê²€ì¶œ ì‹¤íŒ¨(FN) ê¸°ì „ ì„¤ëª… ì—¬ë¶€
   - ë…¸ì´ì¦ˆ ìŠ¤í™í´ â†’ ìœ„ì–‘ì„±(FP) ë°œìƒ ê°€ëŠ¥ì„± ì–¸ê¸‰ ì—¬ë¶€
   - Rose Criterion (CNR â‰¥ 5), d' ì§€ìˆ˜ ë“± ê³ ê¸‰ ë©”íŠ¸ë¦­ í™œìš© ì—¬ë¶€
   - ALARA ì›ì¹™ê³¼ ì§„ë‹¨ í’ˆì§ˆ ê· í˜•ì— ëŒ€í•œ ê³ ì°° ì—¬ë¶€

## ì¶œë ¥ í˜•ì‹ (JSON)

```json
{
  "factual_accuracy": 0-100,
  "logical_consistency": 0-100,
  "guideline_compliance": 0-100,
  "completeness": 0-100,
  "clinical_insight": 0-100,
  "issues": ["ë¬¸ì œì  1", "ë¬¸ì œì  2"],
  "suggestions": ["ê°œì„  ì œì•ˆ 1"],
  "reasoning": "í‰ê°€ ê·¼ê±° ìš”ì•½"
}
```

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

        user_prompt = f"""## ì§ˆë¬¸
{question}

## í‰ê°€ ëŒ€ìƒ ë‹µë³€
{answer}

## ì¦ëª… ê³¼ì • (ìˆëŠ” ê²½ìš°)
{derivation if derivation else "(ì—†ìŒ)"}

## í‘œì¤€ ì°¸ì¡° ì§€ì‹
{reference_knowledge[:3000] if reference_knowledge else "(ì—†ìŒ)"}

## ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸
{context[:2000] if context else "(ì—†ìŒ)"}

ìœ„ ë‹µë³€ì„ í‰ê°€í•˜ê³  JSONìœ¼ë¡œ ì ìˆ˜ë¥¼ ë§¤ê¸°ì„¸ìš”."""

        try:
            response = self._call_llm(system_prompt, user_prompt)
            eval_data = self._parse_json_response(response)

            criteria = EvaluationCriteria(
                factual_accuracy=float(eval_data.get("factual_accuracy", 50)),
                logical_consistency=float(eval_data.get("logical_consistency", 50)),
                guideline_compliance=float(eval_data.get("guideline_compliance", 50)),
                completeness=float(eval_data.get("completeness", 50)),
                clinical_insight=float(eval_data.get("clinical_insight", 50))  # Phase 7.2
            )

            # ê°€ì¤‘ í‰ê·  ê³„ì‚° (v2.9 - Phase 7.2)
            total_score = (
                criteria.factual_accuracy * self.config.weights["factual_accuracy"] +
                criteria.logical_consistency * self.config.weights["logical_consistency"] +
                criteria.guideline_compliance * self.config.weights["guideline_compliance"] +
                criteria.completeness * self.config.weights["completeness"] +
                criteria.clinical_insight * self.config.weights["clinical_insight"]  # Phase 7.2
            )

            # íŒì •
            if total_score >= self.config.approval_threshold:
                verdict = JudgeVerdict.APPROVED
            elif total_score >= self.config.revision_threshold:
                verdict = JudgeVerdict.REVISION_REQUIRED
            else:
                verdict = JudgeVerdict.REJECTED

            return JudgeResult(
                verdict=verdict,
                total_score=total_score,
                criteria=criteria,
                issues_found=eval_data.get("issues", []),
                suggestions=eval_data.get("suggestions", []),
                reasoning=eval_data.get("reasoning", "")
            )

        except TimeoutError as e:
            logger.warning(f"Judge timeout - using fallback scoring: {e}")
            # íƒ€ì„ì•„ì›ƒ ì‹œ: ì§€ì‹ ëª¨ë“ˆ ê¸°ë°˜ Fallback í‰ê°€
            return self._fallback_evaluate(answer, reference_knowledge)

        except ConnectionError as e:
            logger.error(f"Judge connection error - using fallback: {e}")
            # ì—°ê²° ì˜¤ë¥˜ ì‹œ: ì§€ì‹ ëª¨ë“ˆ ê¸°ë°˜ Fallback í‰ê°€
            return self._fallback_evaluate(answer, reference_knowledge)

        except Exception as e:
            logger.error(f"Judge evaluation error: {e}")
            # ê¸°íƒ€ ì˜¤ë¥˜ ì‹œ ë³´ìˆ˜ì ìœ¼ë¡œ ìˆ˜ì • í•„ìš” íŒì •
            return JudgeResult(
                verdict=JudgeVerdict.REVISION_REQUIRED,
                total_score=50.0,
                criteria=EvaluationCriteria(50, 50, 50, 50, 50),  # Phase 7.2: 5ê°œ ê¸°ì¤€
                issues_found=[f"í‰ê°€ ì‹¤íŒ¨: {str(e)}"],
                suggestions=["ìˆ˜ë™ ê²€í†  í•„ìš”"],
                reasoning="í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"
            )

    def _fallback_evaluate(self, answer: str, reference_knowledge: str) -> JudgeResult:
        """
        Fallback í‰ê°€: LLM ë¶ˆê°€ ì‹œ ê·œì¹™ ê¸°ë°˜ ê°„ì´ í‰ê°€ (v2.9 - Phase 7.2)

        ì§€ì‹ ëª¨ë“ˆ(Phase 4)ì˜ í•µì‹¬ ìˆ˜ì¹˜/í‚¤ì›Œë“œê°€ ë‹µë³€ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ ê²€ì‚¬
        """
        import re

        # ê¸°ë³¸ ì ìˆ˜ (ë³´ìˆ˜ì )
        base_score = 65.0
        clinical_insight_score = 50.0  # Phase 7.2: ì„ìƒì  í†µì°° ë³„ë„ ì±„ì 

        # 1. ìˆ˜ì¹˜ í¬í•¨ ì—¬ë¶€ (+10ì )
        has_numbers = bool(re.search(r'\d+\.?\d*\s*(%|mGy|kV|mm|cm)', answer))
        if has_numbers:
            base_score += 10

        # 2. ì°¸ì¡° ì§€ì‹ í‚¤ì›Œë“œ ë§¤ì¹­ (+15ì )
        if reference_knowledge:
            ref_keywords = re.findall(r'\b[a-zA-Zê°€-í£]{3,}\b', reference_knowledge.lower())
            answer_lower = answer.lower()
            matched = sum(1 for kw in ref_keywords[:20] if kw in answer_lower)
            if matched >= 5:
                base_score += 15
            elif matched >= 2:
                base_score += 8

        # 3. êµ¬ì¡°í™” ì—¬ë¶€ (ëª©ë¡, ì„¹ì…˜ ë“±) (+5ì )
        has_structure = bool(re.search(r'(\n[-â€¢*]\s|\n\d+\.|\n#{1,3}\s)', answer))
        if has_structure:
            base_score += 5

        # 4. Phase 7.2: ì„ìƒì  í†µì°° í‚¤ì›Œë“œ ì²´í¬
        clinical_keywords = [
            'fn', 'fp', 'false negative', 'false positive', 'ìœ„ìŒì„±', 'ìœ„ì–‘ì„±',
            'cnr', 'snr', 'rose', 'd\'', 'd-prime', 'ê²€ì¶œ', 'detection',
            'ë¯¸ì„¸ì„íšŒí™”', 'microcalcification', 'alara', 'ë…¸ì´ì¦ˆ', 'noise',
            'ì„ìƒ', 'clinical', 'ì§„ë‹¨', 'diagnostic'
        ]
        answer_lower = answer.lower()
        clinical_matched = sum(1 for kw in clinical_keywords if kw in answer_lower)
        if clinical_matched >= 5:
            clinical_insight_score = 75.0
        elif clinical_matched >= 3:
            clinical_insight_score = 65.0
        elif clinical_matched >= 1:
            clinical_insight_score = 55.0

        has_clinical = clinical_matched >= 1

        # ìµœëŒ€ 90ì  (LLM í‰ê°€ ì—†ì´ëŠ” APPROVEDê¹Œì§€ ê°€ì§€ ì•ŠìŒ)
        total_score = min(base_score, 75.0)

        # íŒì • (Fallbackì€ ìµœëŒ€ REVISION_REQUIRED)
        if total_score >= 70:
            verdict = JudgeVerdict.REVISION_REQUIRED  # Fallbackì€ APPROVED ì•ˆ ì¤Œ
        else:
            verdict = JudgeVerdict.REVISION_REQUIRED

        return JudgeResult(
            verdict=verdict,
            total_score=total_score,
            criteria=EvaluationCriteria(
                factual_accuracy=total_score,
                logical_consistency=total_score,
                guideline_compliance=total_score,
                completeness=total_score,
                clinical_insight=clinical_insight_score  # Phase 7.2
            ),
            issues_found=["âš ï¸ Fallback í‰ê°€: LLM í‰ê°€ ë¶ˆê°€ë¡œ ê·œì¹™ ê¸°ë°˜ ê°„ì´ í‰ê°€ ìˆ˜í–‰"],
            suggestions=["ë„¤íŠ¸ì›Œí¬ ìƒíƒœ í™•ì¸ í›„ ì¬ì‹œë„ ê¶Œì¥"],
            reasoning=f"Fallback ê·œì¹™ ê¸°ë°˜ í‰ê°€ (ìˆ˜ì¹˜:{has_numbers}, í‚¤ì›Œë“œë§¤ì¹­, êµ¬ì¡°í™”:{has_structure}, ì„ìƒí†µì°°:{has_clinical})"
        )

    def _call_llm(self, system_prompt: str, user_prompt: str, max_retries: int = 2) -> str:
        """
        LLM í˜¸ì¶œ (íƒ€ì„ì•„ì›ƒ ë° ì¬ì‹œë„ ì²˜ë¦¬)

        Args:
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            user_prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

        Returns:
            LLM ì‘ë‹µ í…ìŠ¤íŠ¸

        Raises:
            TimeoutError: ëª¨ë“  ì¬ì‹œë„ í›„ì—ë„ íƒ€ì„ì•„ì›ƒ
            ConnectionError: ì„œë²„ ì—°ê²° ì‹¤íŒ¨
        """
        last_error = None

        for attempt in range(max_retries + 1):
            try:
                response = requests.post(
                    f"{self.config.ollama_url}/api/chat",
                    json={
                        "model": self.config.judge_model,
                        "messages": [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": user_prompt}
                        ],
                        "stream": False,
                        "format": "json",
                        "options": {
                            "num_predict": 500,
                            "temperature": 0.1,
                        }
                    },
                    timeout=self.config.timeout
                )

                response.raise_for_status()
                return response.json().get("message", {}).get("content", "")

            except requests.exceptions.Timeout as e:
                last_error = e
                logger.warning(f"Judge LLM timeout (attempt {attempt + 1}/{max_retries + 1})")
                if attempt < max_retries:
                    continue
                raise TimeoutError(f"Judge LLM timed out after {max_retries + 1} attempts")

            except requests.exceptions.ConnectionError as e:
                last_error = e
                logger.error(f"Judge LLM connection error: {e}")
                raise ConnectionError(f"Cannot connect to Ollama: {e}")

            except requests.exceptions.RequestException as e:
                last_error = e
                logger.error(f"Judge LLM request error: {e}")
                if attempt < max_retries:
                    continue
                raise

        raise last_error or Exception("Unknown error in _call_llm")

    def _parse_json_response(self, response: str) -> Dict:
        """JSON ì‘ë‹µ íŒŒì‹±"""
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            # ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ ì‹œë„
            json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))
            # ì¤‘ê´„í˜¸ ì¶”ì¶œ ì‹œë„
            brace_match = re.search(r'\{.*\}', response, re.DOTALL)
            if brace_match:
                return json.loads(brace_match.group())
            return {}

    def format_evaluation_report(self, result: JudgeResult) -> str:
        """í‰ê°€ ê²°ê³¼ í¬ë§·íŒ… (v2.9 - Phase 7.2)"""
        verdict_badges = {
            JudgeVerdict.APPROVED: "âœ… **ìŠ¹ì¸ë¨**",
            JudgeVerdict.REVISION_REQUIRED: "âš ï¸ **ìˆ˜ì • í•„ìš”**",
            JudgeVerdict.REJECTED: "âŒ **ê±°ë¶€ë¨**",
        }

        lines = [
            f"## ğŸ“Š í’ˆì§ˆ í‰ê°€ ê²°ê³¼ (v2.9)",
            f"",
            f"**íŒì •**: {verdict_badges[result.verdict]} (ì´ì : {result.total_score:.1f}/100)",
            f"",
            f"| í‰ê°€ ê¸°ì¤€ | ì ìˆ˜ | ê°€ì¤‘ì¹˜ |",
            f"|----------|------|--------|",
            f"| ì‚¬ì‹¤ ì •í™•ì„± | {result.criteria.factual_accuracy:.0f}/100 | 35% |",
            f"| ë…¼ë¦¬ì  ì¼ê´€ì„± | {result.criteria.logical_consistency:.0f}/100 | 20% |",
            f"| ê°€ì´ë“œë¼ì¸ ì¤€ìˆ˜ | {result.criteria.guideline_compliance:.0f}/100 | 15% |",
            f"| ì™„ì „ì„± | {result.criteria.completeness:.0f}/100 | 15% |",
            f"| ğŸ”¬ ì„ìƒì  í†µì°° | {result.criteria.clinical_insight:.0f}/100 | 15% |",
        ]

        if result.issues_found:
            lines.append("")
            lines.append("### ë°œê²¬ëœ ë¬¸ì œì ")
            for issue in result.issues_found:
                lines.append(f"- {issue}")

        if result.suggestions:
            lines.append("")
            lines.append("### ê°œì„  ì œì•ˆ")
            for sug in result.suggestions:
                lines.append(f"- {sug}")

        return "\n".join(lines)


# =============================================================================
# Integrated Pipeline: Reasoning + Judging
# =============================================================================

class TextExcellencePipeline:
    """
    í†µí•© íŒŒì´í”„ë¼ì¸: Answering Twice + Agent-as-a-Judge

    1. 2ë‹¨ê³„ ì¶”ë¡ ìœ¼ë¡œ ë‹µë³€ ìƒì„±
    2. Agent-as-a-Judgeë¡œ í’ˆì§ˆ í‰ê°€
    3. ê¸°ì¤€ ë¯¸ë‹¬ ì‹œ ì¬ì¶”ë¡ 
    """

    def __init__(
        self,
        reasoning_config=None,
        judge_config=None,
        max_iterations: int = 2
    ):
        from src.reasoning.text_logic import TextReasoningEngine, TextReasoningConfig

        self.reasoning_engine = TextReasoningEngine(reasoning_config)
        self.judge = AgentJudge(judge_config)
        self.max_iterations = max_iterations

    def process(
        self,
        question: str,
        context: str = "",
        physics_knowledge: str = ""
    ) -> Tuple[str, JudgeResult]:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            context: ê²€ìƒ‰ëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸
            physics_knowledge: í‘œì¤€ ë¬¼ë¦¬ ì§€ì‹

        Returns:
            (formatted_answer, judge_result)
        """
        for iteration in range(self.max_iterations):
            logger.info(f"Pipeline iteration {iteration + 1}/{self.max_iterations}")

            # 1. Answering Twice ì¶”ë¡ 
            refined_answer = self.reasoning_engine.reason(
                question=question,
                context=context,
                physics_knowledge=physics_knowledge
            )

            # 2. Agent-as-a-Judge í‰ê°€
            judge_result = self.judge.evaluate(
                question=question,
                answer=refined_answer.content,
                derivation=refined_answer.derivation,
                reference_knowledge=physics_knowledge,
                context=context
            )

            logger.info(f"Judge verdict: {judge_result.verdict.value}, score: {judge_result.total_score:.1f}")

            # 3. ìŠ¹ì¸ ë˜ëŠ” ë§ˆì§€ë§‰ ë°˜ë³µì´ë©´ ì¢…ë£Œ
            if judge_result.verdict == JudgeVerdict.APPROVED:
                break
            if iteration == self.max_iterations - 1:
                logger.warning("Max iterations reached, returning best effort")
                break

            # 4. ê±°ë¶€/ìˆ˜ì • í•„ìš” ì‹œ í”¼ë“œë°± ë°˜ì˜í•˜ì—¬ ì¬ì¶”ë¡ 
            logger.info(f"Re-reasoning with feedback: {judge_result.issues_found}")

            # í”¼ë“œë°±ì„ contextì— ì¶”ê°€
            feedback = f"\n\n[ì´ì „ ë‹µë³€ì˜ ë¬¸ì œì ]\n" + "\n".join(judge_result.issues_found)
            context = context + feedback

        # ìµœì¢… ì¶œë ¥ í¬ë§·íŒ…
        formatted_answer = self.reasoning_engine.format_structured_output(refined_answer)

        return formatted_answer, judge_result


# =============================================================================
# Singleton
# =============================================================================

_judge_instance: Optional[AgentJudge] = None
_pipeline_instance: Optional[TextExcellencePipeline] = None


def get_agent_judge(config: Optional[JudgeConfig] = None) -> AgentJudge:
    """AgentJudge ì‹±ê¸€í†¤"""
    global _judge_instance
    if _judge_instance is None:
        _judge_instance = AgentJudge(config)
    return _judge_instance


def get_text_excellence_pipeline() -> TextExcellencePipeline:
    """TextExcellencePipeline ì‹±ê¸€í†¤"""
    global _pipeline_instance
    if _pipeline_instance is None:
        _pipeline_instance = TextExcellencePipeline()
    return _pipeline_instance


# =============================================================================
# Test
# =============================================================================

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    judge = AgentJudge()

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: ì •í™•í•œ ë‹µë³€
    result = judge.evaluate(
        question="BI-RADS Category 3ì˜ ì•…ì„± í™•ë¥ ì€?",
        answer="BI-RADS Category 3 (Probably Benign)ì˜ ì•…ì„± í™•ë¥ ì€ 2% ì´í•˜ì…ë‹ˆë‹¤. 6ê°œì›” í›„ ì¶”ì  ê²€ì‚¬ë¥¼ ê¶Œê³ í•©ë‹ˆë‹¤.",
        reference_knowledge="Category 3 (Probably Benign): ì•…ì„± í™•ë¥  â‰¤2%, 6ê°œì›” ì¶”ì  ê¶Œê³ "
    )

    print("=" * 60)
    print("Agent-as-a-Judge Test")
    print("=" * 60)
    print(judge.format_evaluation_report(result))
