"""
Sophia AI: Query Planner (Phase 7.6.2)
======================================
ë³µí•© ì§ˆë¬¸ì„ í•˜ìœ„ ì‘ì—…ìœ¼ë¡œ ë¶„í•´í•˜ëŠ” ì—ì´ì „í‹± í”Œë˜ë„ˆ

ë³µì¡í•œ ì „ë¬¸ê°€ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬:
1. ë…ë¦½ì ì¸ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´
2. ê° ì§ˆë¬¸ì˜ ìœ í˜• ë¶„ë¥˜ (MATH, CONCEPT, SEARCH)
3. í•„ìš”í•œ ì§€ì‹ ëª¨ë“ˆ ë§¤í•‘
4. [NEW] ìˆ˜ì¹˜ì  ì œì•½ ì¡°ê±´ ì¶”ì¶œ (Constraint Extraction)
5. [NEW] Double-Anchor í”„ë¡¬í”„íŠ¸ ìƒì„± (Lost in Prompt Order ë°©ì§€)

Reference: Lost in the Prompt Order (2026)
"""

import json
import logging
import re
from dataclasses import dataclass, field
from enum import Enum
from typing import List, Dict, Any, Optional, Tuple
import requests

logger = logging.getLogger(__name__)


class TaskType(Enum):
    """í•˜ìœ„ ì‘ì—… ìœ í˜•"""
    MATH = "MATH"           # ìˆ˜ì‹ ì¦ëª…, ì‚°ìˆ  ê³„ì‚°
    CONCEPT = "CONCEPT"     # ë¬¼ë¦¬ì  ê¸°ì „, ì„ìƒì  ì˜í–¥ ì„¤ëª…
    SEARCH = "SEARCH"       # ë…¼ë¬¸/ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰


@dataclass
class SubTask:
    """ë¶„í•´ëœ í•˜ìœ„ ì‘ì—…"""
    id: int
    type: TaskType
    query: str
    required_modules: List[str]
    answer: str = ""  # Executorê°€ ì±„ì›€
    confidence: float = 0.0


@dataclass
class ExtractedConstraints:
    """ì¶”ì¶œëœ ìˆ˜ì¹˜ì  ì œì•½ ì¡°ê±´ (Phase 7.6.2)"""
    dose_ratio: Optional[float] = None          # ì„ ëŸ‰ ë¹„ìœ¨ (0.5 = 50% ê°ì†Œ)
    electronic_noise_fraction: Optional[float] = None  # ì „ì ë…¸ì´ì¦ˆ ë¹„ìœ¨
    rose_k: float = 5.0                         # Rose Criterion ìƒìˆ˜
    other_values: Dict[str, float] = field(default_factory=dict)
    raw_text: str = ""                          # ì›ë³¸ ì œì•½ ì¡°ê±´ í…ìŠ¤íŠ¸


@dataclass
class DecompositionResult:
    """ë¶„í•´ ê²°ê³¼"""
    original_query: str
    is_complex: bool  # ë³µí•© ì§ˆë¬¸ ì—¬ë¶€
    subtasks: List[SubTask] = field(default_factory=list)
    reasoning: str = ""  # ë¶„í•´ ê·¼ê±°
    constraints: Optional[ExtractedConstraints] = None  # Phase 7.6.2: ì¶”ì¶œëœ ì œì•½ ì¡°ê±´


@dataclass
class PlannerConfig:
    """í”Œë˜ë„ˆ ì„¤ì •"""
    ollama_url: str = "http://localhost:11434"
    planner_model: str = "glm4:9b"  # ê°€ë²¼ìš´ ëª¨ë¸ë¡œ ë¶„í•´
    timeout: int = 60
    complexity_threshold: int = 2  # ì´ ê°œìˆ˜ ì´ìƒì´ë©´ ë³µí•© ì§ˆë¬¸


class QueryPlanner:
    """
    ë³µí•© ì§ˆë¬¸ ë¶„í•´ê¸°

    ì‚¬ìš©ìì˜ ë³µì¡í•œ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë…ë¦½ì ì¸ í•˜ìœ„ ì‘ì—…ìœ¼ë¡œ ë¶„í•´í•©ë‹ˆë‹¤.
    ê° í•˜ìœ„ ì‘ì—…ì€ ì „ìš© Executorì— ì˜í•´ ì²˜ë¦¬ë©ë‹ˆë‹¤.
    """

    SYSTEM_PROMPT = r"""ë‹¹ì‹ ì€ ì˜í•™ ë¬¼ë¦¬ ë° ì˜ìƒ ì§„ë‹¨ ì‹œìŠ¤í…œì˜ 'ìˆ˜ì„ ì „ëµê°€'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ë³µì¡í•œ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬, ë…¼ë¦¬ì ìœ¼ë¡œ ì™„ë²½í•œ ë‹µë³€ì„ ìƒì„±í•˜ê¸° ìœ„í•œ í•˜ìœ„ ì‘ì—…(Sub-tasks)ìœ¼ë¡œ ë¶„í•´í•˜ì‹­ì‹œì˜¤.

## ğŸ“‹ ë¶„í•´ ê·œì¹™:
1. **ë…ë¦½ì„±**: ê° í•˜ìœ„ ì§ˆë¬¸ì€ ê·¸ ìì²´ë¡œ ì™„ë²½í•œ ì§ˆë¬¸ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
2. **ìœ í˜• ë¶„ë¥˜**: ì§ˆë¬¸ì„ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì‹­ì‹œì˜¤:
   - MATH: ìˆ˜ì‹ ì¦ëª…, ì‚°ìˆ  ê³„ì‚°, ìˆ˜ì¹˜ ë„ì¶œì´ í•„ìš”í•œ ê²½ìš°
   - CONCEPT: ë¬¼ë¦¬ì  ê¸°ì „, ì„ìƒì  ì˜í–¥, ì›ë¦¬ ì„¤ëª…ì´ í•„ìš”í•œ ê²½ìš°
   - SEARCH: ìµœì‹  ë…¼ë¬¸ì´ë‚˜ ê°€ì´ë“œë¼ì¸ ê²€ìƒ‰/ì¸ìš©ì´ í•„ìš”í•œ ê²½ìš°
3. **ì§€ì‹ ëª¨ë“ˆ ë§¤í•‘**: ê° ì‘ì—…ì— í•„ìš”í•œ ëª¨ë“ˆì„ ì§€ì •í•˜ì‹­ì‹œì˜¤:
   - snr_cnr: SNR, CNR, ë…¸ì´ì¦ˆ, ì„ ëŸ‰ ê´€ë ¨ ìˆ˜ì‹
   - detector_physics: ê²€ì¶œê¸° ë¬¼ë¦¬í•™, DQE, MTF, PCD, EID
   - mgd_dance_2011: MGD, T-factor, DBT ì„ ëŸ‰
   - PMC: ë…¼ë¬¸ ê²€ìƒ‰ í•„ìš”ì‹œ

## ğŸ“Š ë³µì¡ë„ íŒë‹¨:
- ì§ˆë¬¸ì— "ìˆ˜ì‹ìœ¼ë¡œ ì¦ëª…", "ê³„ì‚°", "ë„ì¶œ" ë“±ì´ ìˆìœ¼ë©´ â†’ MATH í¬í•¨
- ì§ˆë¬¸ì— "ê¸°ì „", "ì›ë¦¬", "ì˜í–¥", "ì„¤ëª…" ë“±ì´ ìˆìœ¼ë©´ â†’ CONCEPT í¬í•¨
- ì§ˆë¬¸ì— "ë…¼ë¬¸", "ê·¼ê±°", "ìµœê·¼", "ì—°êµ¬" ë“±ì´ ìˆìœ¼ë©´ â†’ SEARCH í¬í•¨

## ğŸ“¤ ì¶œë ¥ í˜•ì‹ (JSON Only):
{
  "is_complex": true,
  "reasoning": "ì´ ì§ˆë¬¸ì´ ë³µí•©ì ì¸ ì´ìœ  ì„¤ëª…",
  "subtasks": [
    {"id": 1, "type": "MATH", "query": "ë¶„í•´ëœ ì§ˆë¬¸ 1", "required_modules": ["snr_cnr"]},
    {"id": 2, "type": "CONCEPT", "query": "ë¶„í•´ëœ ì§ˆë¬¸ 2", "required_modules": ["detector_physics"]},
    {"id": 3, "type": "SEARCH", "query": "ë¶„í•´ëœ ì§ˆë¬¸ 3", "required_modules": ["PMC"]}
  ]
}

ë‹¨ìˆœ ì§ˆë¬¸(í•˜ìœ„ ì‘ì—… 1ê°œ)ì¸ ê²½ìš°:
{
  "is_complex": false,
  "reasoning": "ë‹¨ì¼ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥",
  "subtasks": [
    {"id": 1, "type": "MATH", "query": "ì›ë³¸ ì§ˆë¬¸", "required_modules": ["snr_cnr"]}
  ]
}

JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ê¸ˆì§€ì…ë‹ˆë‹¤."""

    def __init__(self, config: Optional[PlannerConfig] = None):
        self.config = config or PlannerConfig()

    def decompose(self, query: str) -> DecompositionResult:
        """
        ì§ˆë¬¸ì„ í•˜ìœ„ ì‘ì—…ìœ¼ë¡œ ë¶„í•´

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            DecompositionResult: ë¶„í•´ ê²°ê³¼
        """
        logger.info(f"Decomposing query: {query[:50]}...")

        # Phase 7.6.2: ì œì•½ ì¡°ê±´ ì¶”ì¶œ (ëª¨ë“  ì§ˆë¬¸ì— ëŒ€í•´)
        constraints = self._extract_constraints(query)
        if constraints.dose_ratio or constraints.electronic_noise_fraction:
            logger.info(f"Extracted constraints: dose_ratio={constraints.dose_ratio}, "
                       f"noise_fraction={constraints.electronic_noise_fraction}")

        # 1. ë¹ ë¥¸ íœ´ë¦¬ìŠ¤í‹± ì²´í¬ - ë‹¨ìˆœ ì§ˆë¬¸ì´ë©´ ë¶„í•´ ì—†ì´ ë°˜í™˜
        if self._is_simple_query(query):
            logger.info("Simple query detected, skipping decomposition")
            result = self._create_simple_result(query)
            result.constraints = constraints
            return result

        # 2. LLMì„ í†µí•œ ë¶„í•´
        try:
            response = self._call_planner(query)
            result = self._parse_response(query, response)
            result.constraints = constraints  # ì œì•½ ì¡°ê±´ ì²¨ë¶€

            logger.info(f"Decomposed into {len(result.subtasks)} subtasks: "
                       f"{[t.type.value for t in result.subtasks]}")

            return result

        except Exception as e:
            logger.error(f"Decomposition failed: {e}")
            result = self._create_simple_result(query)
            result.constraints = constraints
            return result

    def _extract_constraints(self, query: str) -> ExtractedConstraints:
        """
        ì§ˆë¬¸ì—ì„œ ìˆ˜ì¹˜ì  ì œì•½ ì¡°ê±´ ì¶”ì¶œ (Phase 7.6.2)

        Lost in the Prompt Order ë°©ì§€ë¥¼ ìœ„í•´ í•µì‹¬ ìˆ˜ì¹˜ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        constraints = ExtractedConstraints()

        # ì„ ëŸ‰ ê°ì†Œìœ¨ ì¶”ì¶œ (ì˜ˆ: "50% ê°ì¶•", "MGDë¥¼ 50%")
        dose_patterns = [
            r'(?:MGD|ì„ ëŸ‰|dose)[ë¥¼ì„]?\s*(\d+(?:\.\d+)?)\s*%\s*(?:ê°ì¶•|ê°ì†Œ|ì¤„)',
            r'(\d+(?:\.\d+)?)\s*%\s*(?:ì €ì„ ëŸ‰|ê°ì¶•|ê°ì†Œ)',
            r'ì„ ëŸ‰[ì´ê°€]?\s*(\d+(?:\.\d+)?)\s*%',
        ]
        for pattern in dose_patterns:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                percent = float(match.group(1))
                constraints.dose_ratio = (100 - percent) / 100  # 50% ê°ì†Œ â†’ 0.5
                break

        # ì „ì ë…¸ì´ì¦ˆ ë¹„ìœ¨ ì¶”ì¶œ (ì˜ˆ: "30%ë¥¼ ì°¨ì§€")
        noise_patterns = [
            r'ì „ì\s*ë…¸ì´ì¦ˆ[ê°€ì´]?\s*(?:ì „ì²´\s*ë…¸ì´ì¦ˆì˜\s*)?(\d+(?:\.\d+)?)\s*%',
            r'(\d+(?:\.\d+)?)\s*%[ë¥¼ì„]?\s*ì°¨ì§€',
            r'Ïƒ[_]?(?:elec|e)[ê°€ì´]?\s*(\d+(?:\.\d+)?)\s*%',
        ]
        for pattern in noise_patterns:
            match = re.search(pattern, query, re.IGNORECASE)
            if match:
                percent = float(match.group(1))
                constraints.electronic_noise_fraction = percent / 100  # 30% â†’ 0.3
                break

        # Rose Criterion kê°’ ì¶”ì¶œ
        rose_match = re.search(r'Rose\s*(?:Criterion)?\s*\(?k\s*[=:]\s*(\d+(?:\.\d+)?)\)?', query, re.IGNORECASE)
        if rose_match:
            constraints.rose_k = float(rose_match.group(1))

        # ì›ë³¸ í…ìŠ¤íŠ¸ ì €ì¥ (Double-Anchorìš©)
        constraint_parts = []
        if constraints.dose_ratio:
            constraint_parts.append(f"ì„ ëŸ‰ {(1-constraints.dose_ratio)*100:.0f}% ê°ì†Œ")
        if constraints.electronic_noise_fraction:
            constraint_parts.append(f"ì „ì ë…¸ì´ì¦ˆ {constraints.electronic_noise_fraction*100:.0f}%")
        if constraints.rose_k != 5.0:
            constraint_parts.append(f"Rose k={constraints.rose_k}")

        constraints.raw_text = ", ".join(constraint_parts) if constraint_parts else ""

        return constraints

    def format_double_anchor_prompt(
        self,
        constraints: ExtractedConstraints,
        verified_values: Optional[Dict[str, float]] = None
    ) -> Tuple[str, str]:
        """
        Double-Anchor ì „ëµ í”„ë¡¬í”„íŠ¸ ìƒì„± (Phase 7.6.2)

        Lost in the Prompt Order ë°©ì§€ë¥¼ ìœ„í•´:
        - Primary Anchor: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìµœìƒë‹¨
        - Secondary Anchor: ì»¨í…ìŠ¤íŠ¸ ì§í›„, ë‹µë³€ ì§ì „

        Args:
            constraints: ì¶”ì¶œëœ ì œì•½ ì¡°ê±´
            verified_values: MathVerifierê°€ ê³„ì‚°í•œ ê²€ì¦ëœ ìˆ˜ì¹˜

        Returns:
            (primary_anchor, secondary_anchor) íŠœí”Œ
        """
        if not constraints.dose_ratio and not constraints.electronic_noise_fraction:
            return ("", "")

        # Primary Anchor (ìƒë‹¨)
        primary = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ”’ CRITICAL CONSTRAINTS - ë°˜ë“œì‹œ ì´ ìˆ˜ì¹˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"""

        if constraints.dose_ratio:
            primary += f"\nâ•‘  â€¢ ì„ ëŸ‰ ê°ì†Œ: {(1-constraints.dose_ratio)*100:.0f}% (ë¹„ìœ¨: {constraints.dose_ratio:.2f})          â•‘"
        if constraints.electronic_noise_fraction:
            primary += f"\nâ•‘  â€¢ ì „ì ë…¸ì´ì¦ˆ: {constraints.electronic_noise_fraction*100:.0f}% (ë¹„ìœ¨: {constraints.electronic_noise_fraction:.2f})       â•‘"

        if verified_values:
            primary += "\nâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"
            primary += "\nâ•‘  ğŸ“Š MathVerifier ê²€ì¦ ì™„ë£Œ ì •ë‹µ:                             â•‘"
            for key, value in verified_values.items():
                primary += f"\nâ•‘  â€¢ {key}: {value:.1f}%                                    â•‘"

        primary += "\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"

        # Secondary Anchor (í•˜ë‹¨)
        secondary = f"""
âš ï¸ [CONSTRAINT REMINDER] ë‹µë³€ ìƒì„± ì „ í™•ì¸:
"""
        if constraints.dose_ratio:
            secondary += f"- ì„ ëŸ‰ {(1-constraints.dose_ratio)*100:.0f}% ê°ì†Œ ì¡°ê±´ì´ ê³„ì‚°ì— ë°˜ì˜ë˜ì—ˆëŠ”ê°€?\n"
        if constraints.electronic_noise_fraction:
            secondary += f"- ì „ì ë…¸ì´ì¦ˆ {constraints.electronic_noise_fraction*100:.0f}% ì¡°ê±´ì´ ë°˜ì˜ë˜ì—ˆëŠ”ê°€?\n"

        if verified_values:
            secondary += f"- ê³„ì‚° ê²°ê³¼ê°€ ê²€ì¦ëœ ì •ë‹µê³¼ ì¼ì¹˜í•˜ëŠ”ê°€?\n"

        secondary += "ìœ„ ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ì•Šìœ¼ë©´ ë‹µë³€ì´ ê±°ë¶€ë©ë‹ˆë‹¤.\n"

        return (primary, secondary)

    def _is_simple_query(self, query: str) -> bool:
        """ë‹¨ìˆœ ì§ˆë¬¸ ì—¬ë¶€ ë¹ ë¥¸ íŒë‹¨"""
        # ë³µí•© ì§ˆë¬¸ ì§€í‘œ
        complex_indicators = [
            # ë‹¤ì¤‘ ìš”ì²­
            ("ìˆ˜ì‹ìœ¼ë¡œ ì¦ëª…", "ë…¼í•˜ì‹œì˜¤"),
            ("ê³„ì‚°í•˜ì‹œì˜¤", "ì„¤ëª…í•˜ì‹œì˜¤"),
            ("ë„ì¶œí•˜ì‹œì˜¤", "ë¶„ì„í•˜ì‹œì˜¤"),
            # ë‹¤ì¤‘ ì£¼ì œ
            ("ì²«ì§¸", "ë‘˜ì§¸"),
            ("1)", "2)"),
            ("â‘ ", "â‘¡"),
        ]

        query_lower = query.lower()

        for indicators in complex_indicators:
            if all(ind in query or ind in query_lower for ind in indicators):
                return False

        # ê¸¸ì´ ê¸°ë°˜ (300ì ì´ìƒì´ë©´ ë³µí•© ê°€ëŠ¥ì„±)
        if len(query) > 300:
            return False

        return True

    def _create_simple_result(self, query: str) -> DecompositionResult:
        """ë‹¨ìˆœ ì§ˆë¬¸ìš© ê²°ê³¼ ìƒì„±"""
        task_type = self._infer_task_type(query)
        modules = self._infer_modules(query)

        return DecompositionResult(
            original_query=query,
            is_complex=False,
            subtasks=[
                SubTask(
                    id=1,
                    type=task_type,
                    query=query,
                    required_modules=modules
                )
            ],
            reasoning="ë‹¨ì¼ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬"
        )

    def _infer_task_type(self, query: str) -> TaskType:
        """ì§ˆë¬¸ ìœ í˜• ì¶”ë¡ """
        query_lower = query.lower()

        math_keywords = ['ìˆ˜ì‹', 'ì¦ëª…', 'ê³„ì‚°', 'ë„ì¶œ', 'ê³µì‹', '%', 'ë¹„ìœ¨', 'snr', 'cnr']
        search_keywords = ['ë…¼ë¬¸', 'ì—°êµ¬', 'ê·¼ê±°', 'ìµœê·¼', 'pmc', 'ë¬¸í—Œ']

        if any(kw in query_lower for kw in math_keywords):
            return TaskType.MATH
        if any(kw in query_lower for kw in search_keywords):
            return TaskType.SEARCH
        return TaskType.CONCEPT

    def _infer_modules(self, query: str) -> List[str]:
        """í•„ìš” ì§€ì‹ ëª¨ë“ˆ ì¶”ë¡ """
        query_lower = query.lower()
        modules = []

        module_keywords = {
            'snr_cnr': ['snr', 'cnr', 'ë…¸ì´ì¦ˆ', 'ì„ ëŸ‰', 'dose', 'noise', 'rose'],
            'detector_physics': ['dqe', 'mtf', 'pcd', 'eid', 'ê²€ì¶œê¸°', 'detector', 'ì§ì ‘ë³€í™˜', 'ê°„ì ‘ë³€í™˜'],
            'mgd_dance_2011': ['mgd', 't-factor', 'dbt', 'í† ëª¨'],
        }

        for module, keywords in module_keywords.items():
            if any(kw in query_lower for kw in keywords):
                modules.append(module)

        return modules if modules else ['snr_cnr']  # ê¸°ë³¸ê°’

    def _call_planner(self, query: str) -> str:
        """Planner LLM í˜¸ì¶œ"""
        user_prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„í•´í•˜ì„¸ìš”:\n\n{query}"

        response = requests.post(
            f"{self.config.ollama_url}/api/chat",
            json={
                "model": self.config.planner_model,
                "messages": [
                    {"role": "system", "content": self.SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt}
                ],
                "stream": False,
                "options": {
                    "num_predict": 1000,
                    "temperature": 0.1,  # ë‚®ì€ ì˜¨ë„ë¡œ ì¼ê´€ì„± í™•ë³´
                }
            },
            timeout=self.config.timeout
        )

        response.raise_for_status()
        return response.json().get("message", {}).get("content", "")

    def _parse_response(self, original_query: str, response: str) -> DecompositionResult:
        """LLM ì‘ë‹µ íŒŒì‹±"""
        # JSON ì¶”ì¶œ
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if not json_match:
            logger.warning("No JSON found in planner response")
            return self._create_simple_result(original_query)

        try:
            data = json.loads(json_match.group())
        except json.JSONDecodeError as e:
            logger.warning(f"JSON parse error: {e}")
            return self._create_simple_result(original_query)

        # SubTask ê°ì²´ ìƒì„±
        subtasks = []
        for task_data in data.get("subtasks", []):
            try:
                task_type = TaskType(task_data.get("type", "CONCEPT"))
            except ValueError:
                task_type = TaskType.CONCEPT

            subtasks.append(SubTask(
                id=task_data.get("id", len(subtasks) + 1),
                type=task_type,
                query=task_data.get("query", ""),
                required_modules=task_data.get("required_modules", [])
            ))

        # ìµœì†Œ 1ê°œ subtask ë³´ì¥
        if not subtasks:
            return self._create_simple_result(original_query)

        is_complex = data.get("is_complex", len(subtasks) >= self.config.complexity_threshold)

        return DecompositionResult(
            original_query=original_query,
            is_complex=is_complex,
            subtasks=subtasks,
            reasoning=data.get("reasoning", "")
        )


# =============================================================================
# Singleton
# =============================================================================

_planner_instance: Optional[QueryPlanner] = None


def get_query_planner() -> QueryPlanner:
    """QueryPlanner ì‹±ê¸€í†¤"""
    global _planner_instance
    if _planner_instance is None:
        _planner_instance = QueryPlanner()
    return _planner_instance


# =============================================================================
# Test
# =============================================================================

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    planner = QueryPlanner()

    # ë³µí•© ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
    complex_query = """ì°¨ì„¸ëŒ€ ë§˜ëª¨ê·¸ë˜í”¼ ì‹œìŠ¤í…œì— ê´‘ì ê³„ìˆ˜ ê²€ì¶œê¸°(PCD) ë„ì…ì„ ê²€í†  ì¤‘ì´ë‹¤.
ê¸°ì¡´ EID ì‹œìŠ¤í…œì—ì„œ MGDë¥¼ 50% ê°ì¶•í–ˆì„ ë•Œ, ì „ì ë…¸ì´ì¦ˆê°€ ì „ì²´ ë…¸ì´ì¦ˆì˜ 30%ë¥¼ ì°¨ì§€í•˜ê²Œ ëœë‹¤ë©´
Rose Criterion(k=5)ì„ ë§Œì¡±í•˜ê¸° ìœ„í•œ SNRì˜ í•˜ë½í­ì„ ìˆ˜ì‹ìœ¼ë¡œ ì¦ëª…í•˜ì‹œì˜¤.

ë™ì¼í•œ 50% ì €ì„ ëŸ‰ í™˜ê²½ì—ì„œ PCDë¥¼ ì‚¬ìš©í•  ê²½ìš°, ì „ì ë…¸ì´ì¦ˆ ì œê±°ì™€ ì—ë„ˆì§€ ê°€ì¤‘ì¹˜ ìµœì í™”ê°€
d'(Detectability Index)ë¥¼ ì–´ë–»ê²Œ íšŒë³µì‹œí‚¤ëŠ”ì§€ ê¸°ìˆ í•˜ê³ , ì´ê²ƒì´ ë¯¸ì„¸ ì„íšŒí™”ì˜ ëŒ€ì¡°ë„ í–¥ìƒì—
ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ìµœê·¼ 3ë…„ ë‚´ PMC ë…¼ë¬¸ì„ ê·¼ê±°ë¡œ ë…¼í•˜ì‹œì˜¤."""

    result = planner.decompose(complex_query)

    print(f"\n{'='*60}")
    print(f"Original Query: {result.original_query[:100]}...")
    print(f"Is Complex: {result.is_complex}")
    print(f"Reasoning: {result.reasoning}")
    print(f"\nSubtasks ({len(result.subtasks)}):")
    for task in result.subtasks:
        print(f"  [{task.id}] {task.type.value}: {task.query[:80]}...")
        print(f"      Modules: {task.required_modules}")
