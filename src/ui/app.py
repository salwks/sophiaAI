"""
Sophia AI Alpha: RAG-based Medical AI Assistant
===============================================
ë…¼ë¬¸ ê²€ìƒ‰ ê¸°ë°˜ ì˜ë£Œ AI ì–´ì‹œìŠ¤í„´íŠ¸ (í• ë£¨ì‹œë„¤ì´ì…˜ ìµœì†Œí™”)
"""

import os
import sys
from pathlib import Path
import streamlit as st
import requests
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.search.engine import SearchEngine

# =============================================================================
# í˜ì´ì§€ ì„¤ì •
# =============================================================================

st.set_page_config(
    page_title="Sophia AI Alpha",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded",
)

# =============================================================================
# ê²€ìƒ‰ ì—”ì§„ & LLM ì´ˆê¸°í™”
# =============================================================================

@st.cache_resource(ttl=3600)  # 1ì‹œê°„ë§ˆë‹¤ ìºì‹œ ê°±ì‹ 
def get_search_engine():
    """ê²€ìƒ‰ ì—”ì§„ ì‹±ê¸€í†¤ (BI-RADS í¬í•¨)"""
    return SearchEngine(
        db_path=Path("data/index"),
        parser_mode="smart",
        ollama_url="http://localhost:11434",
        llm_model="qwen2.5:14b",
        use_reranker=True,
    )

def call_llm_with_context(question: str, context: str, model="qwen2.5:14b", temperature=0.7):
    """
    RAG: ê²€ìƒ‰ëœ ë…¼ë¬¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLM ë‹µë³€ ìƒì„±

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        context: ê²€ìƒ‰ëœ ë…¼ë¬¸ ë‚´ìš©
        model: LLM ëª¨ë¸ëª…
        temperature: ì˜¨ë„ ì„¤ì •

    Returns:
        LLM ë‹µë³€ (generator)
    """
    url = "http://localhost:11434/api/chat"

    system_message = """ë‹¹ì‹ ì€ ìœ ë°©ì˜ìƒì˜í•™ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ì¤‘ìš”í•œ ê·œì¹™:**
1. ì œê³µëœ ë…¼ë¬¸ê³¼ BI-RADS ê°€ì´ë“œë¼ì¸ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ì¶”ì¸¡í•˜ì§€ ë§ê³ , ì œê³µëœ ì •ë³´ì— ì—†ìœ¼ë©´ "ì œê³µëœ ìë£Œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
3. ë‹µë³€ ì‹œ ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš” (ì˜ˆ: [1], [2] í˜•ì‹)
4. ì˜í•™ì  ì¡°ì–¸ì´ ì•„ë‹Œ ì—°êµ¬ ì •ë³´ ì œê³µì„ì„ ëª…í™•íˆ í•˜ì„¸ìš”
5. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”"""

    user_message = f"""ë‹¤ìŒ ë…¼ë¬¸ë“¤ê³¼ ê°€ì´ë“œë¼ì¸ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

**ì°¸ê³  ìë£Œ:**
{context}

**ì§ˆë¬¸:** {question}

**ë‹µë³€ í˜•ì‹:**
- ì œê³µëœ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
- ì¶œì²˜ ë²ˆí˜¸ [1], [2] ë“±ìœ¼ë¡œ ëª…ì‹œ
- ì˜í•™ì  ì¡°ì–¸ì´ ì•„ë‹Œ ì—°êµ¬ ì •ë³´ì„ì„ ëª…í™•íˆ í‘œì‹œ"""

    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_message}
    ]

    payload = {
        "model": model,
        "messages": messages,
        "stream": True,
        "options": {
            "temperature": temperature,
        }
    }

    try:
        response = requests.post(url, json=payload, stream=True, timeout=180)
        response.raise_for_status()

        for line in response.iter_lines():
            if line:
                chunk = json.loads(line)
                if "message" in chunk and "content" in chunk["message"]:
                    yield chunk["message"]["content"]
    except requests.exceptions.RequestException as e:
        yield f"âš ï¸ LLM ì—°ê²° ì˜¤ë¥˜: {str(e)}"

# =============================================================================
# ì‚¬ì´ë“œë°”
# =============================================================================

def render_sidebar():
    """ì‚¬ì´ë“œë°” ë Œë”ë§"""
    with st.sidebar:
        st.markdown("### âš™ï¸ Settings")

        st.markdown("#### Model")
        model = st.selectbox(
            "LLM Model",
            options=["qwen2.5:14b"],
            index=0,
        )

        temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=1.0,
            value=0.3,
            step=0.1,
            help="ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì , ë†’ì„ìˆ˜ë¡ ì°½ì˜ì "
        )

        st.markdown("#### Search")
        top_k = st.slider(
            "ì°¸ê³  ë…¼ë¬¸ ìˆ˜",
            min_value=3,
            max_value=10,
            value=5,
            step=1,
            help="ë‹µë³€ ìƒì„± ì‹œ ì°¸ê³ í•  ë…¼ë¬¸ ìˆ˜"
        )

        st.markdown("---")

        if st.button("ğŸ—‘ï¸ Clear Chat", use_container_width=True):
            st.session_state.messages = []
            st.rerun()

        st.markdown("---")
        st.markdown("#### About")
        st.markdown("""
        <div style="font-size: 0.85rem; line-height: 1.6;">
        <b>Sophia AI Alpha</b><br>
        ìœ ë°©ì˜ìƒì˜í•™ ë…¼ë¬¸ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ<br><br>

        âœ… í• ë£¨ì‹œë„¤ì´ì…˜ ìµœì†Œí™”<br>
        âœ… ë…¼ë¬¸ ì¶œì²˜ ëª…ì‹œ<br>
        âœ… BI-RADS ê°€ì´ë“œë¼ì¸ ì°¸ì¡°
        </div>
        """, unsafe_allow_html=True)

        return {
            "model": model,
            "temperature": temperature,
            "top_k": top_k,
        }

# =============================================================================
# ë©”ì¸ UI
# =============================================================================

def main():
    """ë©”ì¸ RAG ì±—ë´‡"""

    # ì‚¬ì´ë“œë°”
    options = render_sidebar()

    # í—¤ë”
    st.markdown("""
    <h1 style='font-size: 2.5rem; margin-bottom: 0;'>
        ğŸ’¬ Sophia AI
        <span style='font-size: 0.9rem; color: #888888; font-weight: normal; vertical-align: super;'>Alpha</span>
    </h1>
    """, unsafe_allow_html=True)
    st.caption("ğŸš€ ìœ ë°©ì˜ìƒì˜í•™ ë…¼ë¬¸ ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ (RAG)")

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state["messages"] = [{
            "role": "assistant",
            "content": "ì•ˆë…•í•˜ì„¸ìš”! ìœ ë°©ì˜ìƒì˜í•™ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´, ê´€ë ¨ ë…¼ë¬¸ê³¼ BI-RADS ê°€ì´ë“œë¼ì¸ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\nì˜ˆì‹œ:\n- Mammographyì— ëŒ€í•œ ê¸°ë³¸ ê°œë… ì„¤ëª…\n- DBTì™€ FFDMì˜ ì°¨ì´ì \n- BI-RADS ì¹´í…Œê³ ë¦¬ ì„¤ëª…"
        }]

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

            # ì¶œì²˜ í‘œì‹œ
            if "sources" in msg:
                with st.expander("ğŸ“š ì°¸ê³  ìë£Œ", expanded=False):
                    for i, source in enumerate(msg["sources"], 1):
                        icon = "ğŸ“˜" if source.get("is_birads", False) else "ğŸ“„"

                        if source.get("is_birads", False):
                            # BI-RADS ë¬¸ì„œëŠ” ë‚´ìš©ì„ ì§ì ‘ í‘œì‹œ
                            st.markdown(f"**{icon} [{i}] {source['title']}**")
                            st.markdown(f"{source['authors']} - {source['journal']} ({source['year']})")

                            # ì „ì²´ ë‚´ìš©ì„ expanderë¡œ í‘œì‹œ
                            if source.get("full_content"):
                                with st.expander("ğŸ“– ì „ì²´ ë‚´ìš© ë³´ê¸°", expanded=False):
                                    st.markdown(source["full_content"])
                            else:
                                st.caption("_ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤_")
                        else:
                            # ì¼ë°˜ ë…¼ë¬¸ì€ PubMed ë§í¬
                            st.markdown(f"""
                            **{icon} [{i}] {source['title']}**
                            {source['authors']} - {source['journal']} ({source['year']})
                            [PubMed ë³´ê¸°]({source['url']})
                            """)

    # ì±„íŒ… ì…ë ¥
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        # ê²€ìƒ‰ ì—”ì§„ìœ¼ë¡œ ê´€ë ¨ ë…¼ë¬¸ ê²€ìƒ‰
        with st.spinner("ğŸ“š ê´€ë ¨ ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘..."):
            try:
                engine = get_search_engine()

                # ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”
                search_query = prompt
                prompt_lower = prompt.lower()

                # 1. BI-RADS ì¹´í…Œê³ ë¦¬ ì§ˆë¬¸ ì²˜ë¦¬
                is_birads_concept = (
                    'bi-rads' in prompt_lower or 'birads' in prompt_lower or 'ì¹´í…Œê³ ë¦¬' in prompt_lower
                ) and any(keyword in prompt_lower for keyword in ['ê¸°ë³¸', 'ê°œë…', 'ì •ì˜', 'ì„¤ëª…', 'basic', 'concept', 'definition', 'ë¬´ì—‡'])

                if is_birads_concept and 'bi-rads' not in prompt_lower:
                    search_query = f"BI-RADS {prompt}"

                # 2. ê¸°ìˆ  ìš©ì–´ ì§ˆë¬¸ ì²˜ë¦¬ (í•œì˜ í˜¼ìš© â†’ ì˜ë¬¸ í‚¤ì›Œë“œ ê°•í™”)
                technical_terms = {
                    'exposure': ['kVp', 'mAs', 'radiation dose', 'technique'],
                    'positioning': ['positioning', 'technique', 'procedure'],
                    'protocol': ['protocol', 'procedure', 'technique'],
                    'ë…¸ì¶œ': ['exposure', 'kVp', 'mAs', 'radiation dose'],
                    'ì´¬ì˜': ['acquisition', 'technique', 'imaging'],
                    'í¬ì§€ì…”ë‹': ['positioning', 'technique'],
                    'í”„ë¡œí† ì½œ': ['protocol', 'procedure'],
                }

                for term, keywords in technical_terms.items():
                    if term in prompt_lower:
                        # ê¸°ìˆ  ìš©ì–´ê°€ ìˆìœ¼ë©´ ì˜ë¬¸ í‚¤ì›Œë“œ ì¶”ê°€
                        search_query = f"{prompt} {' '.join(keywords)}"
                        break

                search_response = engine.search(search_query, top_k=options["top_k"], use_rerank=True)

                if not search_response.results:
                    with st.chat_message("assistant"):
                        error_msg = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."
                        st.markdown(error_msg)
                        st.session_state.messages.append({"role": "assistant", "content": error_msg})
                    st.stop()

                # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ê²€ìƒ‰ëœ ë…¼ë¬¸)
                context_parts = []
                sources = []

                for i, result in enumerate(search_response.results, 1):
                    paper = result.paper

                    # BI-RADS ë¬¸ì„œ ì—¬ë¶€
                    is_birads = paper.pmid.startswith("BIRADS_")
                    doc_type = "ğŸ“˜ BI-RADS ê°€ì´ë“œë¼ì¸" if is_birads else "ğŸ“„ ë…¼ë¬¸"

                    # ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                    # BI-RADSëŠ” full_content ì‚¬ìš©, ì¼ë°˜ ë…¼ë¬¸ì€ abstract ì‚¬ìš©
                    if is_birads:
                        content_text = getattr(paper, 'full_content', paper.abstract or 'ë‚´ìš© ì—†ìŒ')
                    else:
                        content_text = (paper.abstract[:500] + '...' if paper.abstract and len(paper.abstract) > 500 else paper.abstract or 'ì´ˆë¡ ì—†ìŒ')

                    context_parts.append(f"""
[{i}] {doc_type}
ì œëª©: {paper.title}
{'ì €ì: ' + paper.author_string if paper.author_string else ''}
{'ì €ë„: ' + paper.journal + ' (' + str(paper.year) + ')' if paper.journal else ''}
ë‚´ìš©: {content_text}
""")

                    # ì¶œì²˜ ì •ë³´ ì €ì¥
                    sources.append({
                        "title": paper.title,
                        "authors": paper.author_string or "ì €ì ì •ë³´ ì—†ìŒ",
                        "journal": paper.journal or "ì €ë„ ì •ë³´ ì—†ìŒ",
                        "year": paper.year or "ì—°ë„ ì •ë³´ ì—†ìŒ",
                        "url": paper.pubmed_url if not is_birads else None,
                        "is_birads": is_birads,
                        "full_content": getattr(paper, 'full_content', None) if is_birads else None
                    })

                context = "\n".join(context_parts)

            except Exception as e:
                with st.chat_message("assistant"):
                    error_msg = f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                    st.markdown(error_msg)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})
                st.stop()

        # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (RAG)
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            # ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€
            for chunk in call_llm_with_context(
                question=prompt,
                context=context,
                model=options["model"],
                temperature=options["temperature"]
            ):
                full_response += chunk
                message_placeholder.markdown(full_response + "â–Œ")

            message_placeholder.markdown(full_response)

            # ì¶œì²˜ í‘œì‹œ
            with st.expander("ğŸ“š ì°¸ê³  ìë£Œ", expanded=False):
                for i, source in enumerate(sources, 1):
                    icon = "ğŸ“˜" if source["is_birads"] else "ğŸ“„"

                    if source["is_birads"]:
                        # BI-RADS ë¬¸ì„œëŠ” ë‚´ìš©ì„ ì§ì ‘ í‘œì‹œ
                        st.markdown(f"**{icon} [{i}] {source['title']}**")
                        st.markdown(f"{source['authors']} - {source['journal']} ({source['year']})")

                        # ì „ì²´ ë‚´ìš©ì„ expanderë¡œ í‘œì‹œ
                        if source.get("full_content"):
                            with st.expander("ğŸ“– ì „ì²´ ë‚´ìš© ë³´ê¸°", expanded=False):
                                st.markdown(source["full_content"])
                        else:
                            st.caption("_ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤_")
                    else:
                        # ì¼ë°˜ ë…¼ë¬¸ì€ PubMed ë§í¬
                        st.markdown(f"""
                        **{icon} [{i}] {source['title']}**
                        {source['authors']} - {source['journal']} ({source['year']})
                        [PubMed ë³´ê¸°]({source['url']})
                        """)

        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥ (ì¶œì²˜ í¬í•¨)
        st.session_state.messages.append({
            "role": "assistant",
            "content": full_response,
            "sources": sources
        })


if __name__ == "__main__":
    main()
