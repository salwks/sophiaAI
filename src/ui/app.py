"""
Sophia AI Alpha: RAG-based Medical AI Assistant
===============================================
ë…¼ë¬¸ ê²€ìƒ‰ ê¸°ë°˜ ì˜ë£Œ AI ì–´ì‹œìŠ¤í„´íŠ¸ (í• ë£¨ì‹œë„¤ì´ì…˜ ìµœì†Œí™”)
"""

import os
import sys
import re
import copy
from pathlib import Path
import streamlit as st
import requests
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.search.engine import SearchEngine
from src.search.query_translator import get_translator
from src.search.relay_router import RelayRouter, get_relay_router, QueryIntent
from src.evaluation.agent_judge import (
    TextExcellencePipeline, get_text_excellence_pipeline, JudgeVerdict,
    AgentJudge, get_agent_judge  # Phase 7.6: Agent-as-a-Judge í†µí•©
)
from src.retrieval.dynamic_evidence import DynamicEvidencePipeline, get_dynamic_evidence_pipeline


def convert_latex_for_streamlit(text: str) -> str:
    r"""
    LLM ì‘ë‹µì˜ ìˆ˜ì‹ì„ Streamlitì´ ë Œë”ë§í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜

    íŒ¨í„´:
    - (ìˆ˜ì‹) â†’ $ìˆ˜ì‹$ (LaTeX ë¬¸ë²•ì´ í¬í•¨ëœ ê²½ìš°ë§Œ)
    - \(...\) â†’ $...$
    - \[...\] â†’ $$...$$
    """
    if not text:
        return text

    # 1. \(...\) ì¸ë¼ì¸ LaTeX â†’ $...$
    text = re.sub(r'\\\((.+?)\\\)', r'$\1$', text)

    # 2. \[...\] ë””ìŠ¤í”Œë ˆì´ LaTeX â†’ $$...$$
    text = re.sub(r'\\\[(.+?)\\\]', r'$$\1$$', text, flags=re.DOTALL)

    # 3. (ìˆ˜ì‹) íŒ¨í„´ ë³€í™˜ - LaTeX ë¬¸ë²•ì´ í¬í•¨ëœ ê²½ìš°ë§Œ
    # LaTeX ë¬¸ë²• íŒ¨í„´: \frac, \times, \sin, \cos, \pi, \sqrt, _{, ^{, Ã—, âˆ, â‰ˆ
    latex_indicators = r'\\(?:frac|times|sin|cos|pi|sqrt|alpha|beta|sigma|delta|Delta|mu)|_\{|\^\{|[Ã—âˆâ‰ˆâˆ¼â†’â†â†‘â†“]'

    def replace_latex_parens(match):
        content = match.group(1)
        # LaTeX ë¬¸ë²•ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ $...$ë¡œ ë³€í™˜
        if re.search(latex_indicators, content):
            return f'${content}$'
        # ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
        return match.group(0)

    # ê´„í˜¸ ì•ˆì— = ë˜ëŠ” LaTeX ë¬¸ë²•ì´ ìˆëŠ” ê²½ìš°ë§Œ ë³€í™˜
    # ì˜ˆ: (w = f \times (M-1)) â†’ $w = f \times (M-1)$
    text = re.sub(r'\(([^()]*(?:=|' + latex_indicators + r')[^()]*)\)', replace_latex_parens, text)

    # 4. <br> íƒœê·¸ ì „í›„ì˜ ìˆ˜ì‹ ì •ë¦¬
    text = re.sub(r'\$\s*<br>\s*', '<br>\n$', text)
    text = re.sub(r'\s*<br>\s*\$', '$<br>\n', text)

    return text

# =============================================================================
# í˜ì´ì§€ ì„¤ì •
# =============================================================================

st.set_page_config(
    page_title="Sophia AI Alpha",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded",
)

# =============================================================================
# ê²€ìƒ‰ ì—”ì§„ & LLM ì´ˆê¸°í™”
# =============================================================================

@st.cache_resource(ttl=3600)  # 1ì‹œê°„ë§ˆë‹¤ ìºì‹œ ê°±ì‹ 
def get_search_engine():
    """ê²€ìƒ‰ ì—”ì§„ ì‹±ê¸€í†¤ (BI-RADS í¬í•¨)"""
    return SearchEngine(
        db_path=Path("data/index"),
        parser_mode="smart",
        ollama_url="http://localhost:11434",
        llm_model="gpt-oss:20b",
        use_reranker=True,
    )

@st.cache_resource(ttl=3600)
def get_query_translator():
    """ì¿¼ë¦¬ ë²ˆì—­ê¸° ì‹±ê¸€í†¤ (ë ˆê±°ì‹œ, RelayRouterë¡œ ëŒ€ì²´ ì˜ˆì •)"""
    return get_translator(
        ollama_url="http://localhost:11434",
        model="gpt-oss:20b"
    )

@st.cache_resource(ttl=3600)
def get_cached_relay_router():
    """RelayRouter ì‹±ê¸€í†¤ (SLM-LLM í˜‘ì—…)"""
    return get_relay_router()

@st.cache_resource(ttl=3600)
def get_cached_text_pipeline():
    """TextExcellencePipeline ì‹±ê¸€í†¤ (Answering Twice + Agent Judge)"""
    return get_text_excellence_pipeline()

@st.cache_resource(ttl=3600)
def get_cached_dynamic_pipeline(_version: str = "7.17"):
    """DynamicEvidencePipeline ì‹±ê¸€í†¤ (Phase 7.17: ë‹¨ìˆœí™”ëœ íŒŒì´í”„ë¼ì¸)"""
    from src.retrieval.dynamic_evidence import DynamicEvidencePipeline
    # Phase 7.17: decomposition ë¹„í™œì„±í™” (ë³µì¡í•œ ë ˆì´ì–´ ì œê±°)
    return DynamicEvidencePipeline(use_summarizer=False, enable_decomposition=False)

def get_guideline_type(pmid: str) -> str:
    """PMIDë¡œë¶€í„° ê°€ì´ë“œë¼ì¸ íƒ€ì… ë°˜í™˜"""
    if not pmid:
        return "unknown"
    if pmid.startswith("BIRADS_"):
        return "birads"
    elif pmid.startswith("ACR_"):
        return "acr"
    elif pmid.startswith("PHYSICS_"):
        return "physics"
    elif pmid.startswith("CLINICAL_"):
        return "clinical"
    elif pmid.startswith("DANCE_"):
        return "dance"  # Dance et al. ë¬¼ë¦¬í•™ ì°¸ì¡° ë…¼ë¬¸
    return "paper"


def get_guideline_label(pmid: str) -> str:
    """PMIDë¡œë¶€í„° ê°€ì´ë“œë¼ì¸ ë¼ë²¨ ë°˜í™˜"""
    guideline_type = get_guideline_type(pmid)
    labels = {
        "birads": "ğŸ“˜ BI-RADS ê°€ì´ë“œë¼ì¸",
        "acr": "ğŸ“— ACR Practice Parameter",
        "physics": "ğŸ“™ ë¬¼ë¦¬í•™ ê°€ì´ë“œë¼ì¸",
        "clinical": "ğŸ“• ì„ìƒ ê°€ì´ë“œë¼ì¸",
        "dance": "ğŸ“™ Dance 2011 ë¬¼ë¦¬ ì°¸ì¡°",  # Dance et al. MGD ë…¼ë¬¸
        "paper": "ğŸ“„ ì—°êµ¬ë…¼ë¬¸",
        "unknown": "ğŸ“„ ë¬¸ì„œ"
    }
    return labels.get(guideline_type, "ğŸ“„ ë¬¸ì„œ")


def get_birads_nav_params(pmid: str) -> dict:
    """
    PMIDë¡œë¶€í„° ê°€ì´ë“œë¼ì¸ í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜ íŒŒë¼ë¯¸í„° ìƒì„±

    ì˜ˆ: BIRADS_2025_SECTION_IV_A_CHUNK_MARGIN
    â†’ {"modality": "mammography", "section": "BIRADS_2025_SECTION_IV", "sub": "BIRADS_2025_SECTION_IV_A", "chunk": "MARGIN"}
    """
    GUIDELINE_PREFIXES = ("BIRADS_", "ACR_", "PHYSICS_", "CLINICAL_")
    if not pmid or not pmid.startswith(GUIDELINE_PREFIXES):
        return {"modality": "mammography"}

    params = {"modality": "mammography"}

    if "_CHUNK_" in pmid:
        parts = pmid.split("_CHUNK_")
        parent = parts[0]
        chunk_name = parts[1]

        if "SECTION_IV" in parent:
            params["section"] = "BIRADS_2025_SECTION_IV"
            params["sub"] = parent
            params["chunk"] = chunk_name
        elif "SECTION_V" in parent:
            params["section"] = "BIRADS_2025_SECTION_V"
            params["sub"] = parent
            params["chunk"] = chunk_name
    else:
        if "SECTION_IV" in pmid and len(pmid) > len("BIRADS_2025_SECTION_IV"):
            params["section"] = "BIRADS_2025_SECTION_IV"
            params["sub"] = pmid
        elif "SECTION_V" in pmid and len(pmid) > len("BIRADS_2025_SECTION_V"):
            params["section"] = "BIRADS_2025_SECTION_V"
            params["sub"] = pmid
        else:
            params["section"] = pmid
            params["view"] = "content"

    return params


def get_acr_nav_params(pmid: str) -> dict:
    """
    ACR PMIDë¡œë¶€í„° ACR í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜ íŒŒë¼ë¯¸í„° ìƒì„±

    ì˜ˆ: ACR_CEM_INDICATIONS â†’ {"category": "cem", "doc": "ACR_CEM_INDICATIONS"}
    ì˜ˆ: ACR_MAMMO_SECTION_I â†’ {"category": "mammo", "doc": "ACR_MAMMO_SECTION_I"}
    """
    if not pmid or not pmid.startswith("ACR_"):
        return {}

    params = {}

    # ì¹´í…Œê³ ë¦¬ ê²°ì •
    if pmid.startswith("ACR_CEM_"):
        params["category"] = "cem"
    elif pmid.startswith("ACR_MAMMO_"):
        params["category"] = "mammo"
    elif pmid.startswith("ACR_IQ_"):
        params["category"] = "iq"
    else:
        params["category"] = "mammo"  # ê¸°ë³¸ê°’

    params["doc"] = pmid
    return params


def get_guideline_page_url(pmid: str) -> str:
    """
    ê°€ì´ë“œë¼ì¸ PMIDì— ë”°ë¼ ì ì ˆí•œ í˜ì´ì§€ URL ìƒì„±
    """
    guideline_type = get_guideline_type(pmid)

    if guideline_type == "acr":
        nav_params = get_acr_nav_params(pmid)
        param_str = "&".join([f"{k}={v}" for k, v in nav_params.items()])
        return f"/ACR_Practice_Parameters?{param_str}"
    elif guideline_type in ("birads", "physics", "clinical"):
        nav_params = get_birads_nav_params(pmid)
        param_str = "&".join([f"{k}={v}" for k, v in nav_params.items()])
        return f"/BI-RADS_Guidelines?{param_str}"
    else:
        return "#"


def fetch_pmc_fulltext(pmc_url: str, max_chars: int = 8000) -> str:
    """
    PMC ë…¼ë¬¸ ì „ë¬¸ì„ ê°€ì ¸ì™€ì„œ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜

    Args:
        pmc_url: PMC ë…¼ë¬¸ URL (ì˜ˆ: https://pmc.ncbi.nlm.nih.gov/articles/PMC7533093/)
        max_chars: ìµœëŒ€ ë¬¸ì ìˆ˜ (ê¸°ë³¸ 8000ì)

    Returns:
        ë…¼ë¬¸ ì „ë¬¸ í…ìŠ¤íŠ¸ (HTML íƒœê·¸ ì œê±°ë¨)
    """
    try:
        # URL ì •ê·œí™”
        if "www.ncbi.nlm.nih.gov" in pmc_url:
            pmc_url = pmc_url.replace("www.ncbi.nlm.nih.gov/pmc", "pmc.ncbi.nlm.nih.gov")

        response = requests.get(pmc_url, timeout=15, headers={
            "User-Agent": "Mozilla/5.0 (compatible; SophiaAI/1.0; Medical Research Assistant)"
        })
        response.raise_for_status()

        html = response.text

        # HTMLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
        import re

        # script, style íƒœê·¸ ì œê±°
        html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
        html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL | re.IGNORECASE)

        # ì£¼ìš” ì„¹ì…˜ ì¶”ì¶œ ì‹œë„ (article-body, main-content ë“±)
        article_match = re.search(r'<article[^>]*>(.*?)</article>', html, flags=re.DOTALL | re.IGNORECASE)
        if article_match:
            html = article_match.group(1)

        # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', ' ', html)

        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        text = re.sub(r'\s+', ' ', text)

        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        text = text.replace('&nbsp;', ' ')
        text = text.replace('&amp;', '&')
        text = text.replace('&lt;', '<')
        text = text.replace('&gt;', '>')

        text = text.strip()

        # ìµœëŒ€ ê¸¸ì´ ì œí•œ
        if len(text) > max_chars:
            text = text[:max_chars] + "... [ì „ë¬¸ ì¼ë¶€ ìƒëµ]"

        return text

    except Exception as e:
        return f"[PMC ì „ë¬¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {str(e)}]"


def is_korean(text: str) -> bool:
    """í…ìŠ¤íŠ¸ì— í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    import re
    return bool(re.search(r'[ê°€-í£]', text))


def get_messages(is_ko: bool) -> dict:
    """ì–¸ì–´ë³„ ë©”ì‹œì§€ ë°˜í™˜"""
    if is_ko:
        return {
            "found_high": "ğŸ“š **ê°€ì´ë“œë¼ì¸ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.**\n\nì•„ë˜ ì›ë¬¸ì„ í™•ì¸í•´ì£¼ì„¸ìš”.",
            "found_medium": "ğŸ“‹ **ê°€ì´ë“œë¼ì¸ì—ì„œ ê´€ë ¨ë  ìˆ˜ ìˆëŠ” ë‚´ìš©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.**\n\nâš ï¸ _{reason}_\n\nì•„ë˜ ì›ë¬¸ì„ í™•ì¸í•´ì£¼ì„¸ìš”.",
            "not_found": "ğŸ“­ **ê°€ì´ë“œë¼ì¸ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.**\n\n_{reason}_",
            "no_results": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ ì£¼ì„¸ìš”.",
            "view_source": "ğŸ“š ì›ë¬¸ í™•ì¸í•˜ê¸°",
            "papers_high": "ğŸ“„ ê´€ë ¨ ì—°êµ¬ ë…¼ë¬¸",
            "papers_medium": "ğŸ“„ ê´€ë ¨ë  ìˆ˜ ìˆëŠ” ì—°êµ¬ ë…¼ë¬¸ âš ï¸",
            "verifying": "ğŸ” ë¬¸ì„œ ê´€ë ¨ì„± ê²€ì¦ ì¤‘...",
            "searching": "ğŸ“š ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."
        }
    else:
        return {
            "found_high": "ğŸ“š **Found relevant content in Guidelines.**\n\nPlease check the original text below.",
            "found_medium": "ğŸ“‹ **Found possibly relevant content in Guidelines.**\n\nâš ï¸ _{reason}_\n\nPlease check the original text below.",
            "not_found": "ğŸ“­ **No relevant content found in Guidelines.**\n\n_{reason}_",
            "no_results": "No search results. Please try different keywords.",
            "view_source": "ğŸ“š View Original",
            "papers_high": "ğŸ“„ Related Research Papers",
            "papers_medium": "ğŸ“„ Possibly Related Research Papers âš ï¸",
            "verifying": "ğŸ” Verifying document relevance...",
            "searching": "ğŸ“š Searching for related papers..."
        }


def enhance_query_with_context(current_question: str, chat_history: list, model="gpt-oss:20b") -> str:
    """
    ì´ì „ ëŒ€í™” ë§¥ë½ì„ ì°¸ê³ í•´ì„œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë³´ê°•

    Args:
        current_question: í˜„ì¬ ì§ˆë¬¸
        chat_history: ì´ì „ ëŒ€í™” ê¸°ë¡
        model: LLM ëª¨ë¸ëª…

    Returns:
        ë³´ê°•ëœ ê²€ìƒ‰ ì¿¼ë¦¬
    """
    # ì´ì „ ëŒ€í™”ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì§ˆë¬¸ ë°˜í™˜
    if not chat_history:
        return current_question

    # ì§ˆë¬¸ì´ ì¶©ë¶„íˆ ê¸¸ê³  ì°¸ì¡° ë‹¨ì–´ê°€ ì—†ìœ¼ë©´ ìƒˆ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼ (ëŒ€í™” ë³´ê°• ê±´ë„ˆë›°ê¸°)
    reference_words = ["ë”", "ê·¸ê±°", "ê·¸ê²ƒ", "ìœ„ì—ì„œ", "ì•„ê¹Œ", "ë°©ê¸ˆ", "ì´ì „", "ê·¸ëŸ¬ë©´", "ê·¸ëŸ¼"]
    has_reference = any(word in current_question for word in reference_words)
    if len(current_question) > 20 and not has_reference:
        return current_question

    # ìµœê·¼ 4ê°œ ë©”ì‹œì§€ë§Œ ì‚¬ìš© (í† í° ì ˆì•½)
    recent_history = chat_history[-4:]

    # ëŒ€í™” ê¸°ë¡ í¬ë§·íŒ…
    history_text = ""
    for msg in recent_history:
        role = "ì‚¬ìš©ì" if msg["role"] == "user" else "ì–´ì‹œìŠ¤í„´íŠ¸"
        content = msg["content"][:200]  # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
        history_text += f"{role}: {content}\n"

    url = "http://localhost:11434/api/chat"

    system_message = """ë‹¹ì‹ ì€ ê²€ìƒ‰ ì¿¼ë¦¬ ë³´ê°• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ë¥¼ ì°¸ì¡°í•˜ëŠ”ì§€ íŒë‹¨í•˜ê³ , ê²€ìƒ‰ì— ì í•©í•œ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.

ê·œì¹™:
1. í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ë§¥ë½ì„ ì°¸ì¡°í•˜ë©´ â†’ ë§¥ë½ì„ í¬í•¨í•œ ì™„ì „í•œ ì¿¼ë¦¬ ìƒì„±
2. í˜„ì¬ ì§ˆë¬¸ì´ ìƒˆë¡œìš´ ì£¼ì œë©´ â†’ ì›ë³¸ ì§ˆë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
3. ê²€ìƒ‰ ì¿¼ë¦¬ë§Œ ì¶œë ¥ (ì„¤ëª… ì—†ì´)
4. **ì¤‘ìš”**: ì…ë ¥ ì–¸ì–´(í•œêµ­ì–´)ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€. ì ˆëŒ€ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ì§€ ë§ˆì„¸ìš”!

ì˜ˆì‹œ:
- ì´ì „: "massì˜ margin ë¶„ë¥˜ëŠ”?" / í˜„ì¬: "ë” ìì„¸íˆ" â†’ "mass margin ë¶„ë¥˜ ìƒì„¸ ì„¤ëª…"
- ì´ì „: "calcification ì¢…ë¥˜" / í˜„ì¬: "MRI ì›ë¦¬ëŠ”?" â†’ "MRI ì›ë¦¬" (ìƒˆ ì£¼ì œ)
- ì´ì „: "BI-RADS ì¹´í…Œê³ ë¦¬" / í˜„ì¬: "3ì€ ë­ì•¼?" â†’ "BI-RADS ì¹´í…Œê³ ë¦¬ 3 ì˜ë¯¸"
"""

    user_message = f"""ì´ì „ ëŒ€í™”:
{history_text}

í˜„ì¬ ì§ˆë¬¸: {current_question}

ê²€ìƒ‰ ì¿¼ë¦¬:"""

    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_message}
    ]

    payload = {
        "model": model,
        "messages": messages,
        "stream": False,
        "options": {"temperature": 0.1}
    }

    try:
        response = requests.post(url, json=payload, timeout=45)
        response.raise_for_status()
        result = response.json()
        enhanced = result.get("message", {}).get("content", "").strip()

        # ë¹ˆ ê²°ê³¼ë©´ ì›ë³¸ ë°˜í™˜
        if not enhanced:
            return current_question

        return enhanced
    except Exception:
        # ì˜¤ë¥˜ì‹œ ì›ë³¸ ì§ˆë¬¸ ë°˜í™˜
        return current_question


def call_llm_with_context(
    question: str,
    context: str,
    model="gpt-oss:20b",
    temperature=0.7,
    has_guidelines: bool = True,
    physics_knowledge: str = ""  # Phase 7.18: ë™ì  ë¬¼ë¦¬ ì§€ì‹ ë³„ë„ ì „ë‹¬
):
    """
    RAG: ê²€ìƒ‰ëœ ë…¼ë¬¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLM ë‹µë³€ ìƒì„±

    Phase 7.19: UnifiedPromptBuilder ì‚¬ìš©ìœ¼ë¡œ ì§€ì‹ ì „ë‹¬ ê²½ë¡œ í†µí•©

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        context: ê²€ìƒ‰ëœ ë…¼ë¬¸ ë‚´ìš©
        model: LLM ëª¨ë¸ëª…
        temperature: ì˜¨ë„ ì„¤ì •
        has_guidelines: ê°€ì´ë“œë¼ì¸ ë¬¸ì„œ í¬í•¨ ì—¬ë¶€
        physics_knowledge: KnowledgeManagerì—ì„œ ë¡œë“œí•œ ê²€ì¦ëœ ë¬¼ë¦¬ ì§€ì‹ (Phase 7.18)

    Returns:
        LLM ë‹µë³€ (generator)
    """
    url = "http://localhost:11434/api/chat"

    # Phase 7.19: UnifiedPromptBuilder ì‚¬ìš© (í†µí•© ì§€ì‹ ì „ë‹¬)
    # Phase 7.20: Query Decomposition + Grounded Values
    grounded_values_context = ""
    try:
        from src.prompts.unified_builder import UnifiedPromptBuilder, PromptLimits
        from src.knowledge.manager import get_knowledge_manager

        km = get_knowledge_manager()
        builder = UnifiedPromptBuilder(km)

        # ë™ì  ì§€ì‹ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (physics_knowledgeê°€ ì—†ìœ¼ë©´ ì¿¼ë¦¬ ê¸°ë°˜ ìë™ ê²€ìƒ‰)
        if not physics_knowledge:
            physics_knowledge = builder.build_knowledge_context(question)

        # í†µí•© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì—ì„œ core_physics ì¶”ì¶œ
        core_physics = builder.get_axioms()

        # Phase 7.20: Grounded Values (ê²€ì¦ëœ ê°’ í…Œì´ë¸”)
        grounded_values_context = builder._get_grounded_values(question)
    except ImportError:
        # Fallback: ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
        try:
            from src.knowledge.core_physics import get_core_physics_prompt
            core_physics = get_core_physics_prompt()
        except ImportError:
            core_physics = ""

    # Phase 7.18/7.19: ë™ì  ë¬¼ë¦¬ ì§€ì‹ì„ core_physics ì•ì— ë°°ì¹˜ (ìš°ì„ ìˆœìœ„)
    if physics_knowledge:
        core_physics = f"{physics_knowledge}\n\n{core_physics}"

    if has_guidelines:
        # ë°ì´í„° ë¬´ê²°ì„± ê°•í™” í”„ë¡¬í”„íŠ¸ (Integrity-First Prompt)
        system_message = f"""# Role
ë„ˆëŠ” ì˜í•™ ë¬¼ë¦¬ ë°ì´í„°ì˜ 'ë¬´ê²°ì„±(Integrity)'ì„ ê²€ì¦í•˜ëŠ” ì „ë¬¸ ê°ì‚¬ê´€(Auditor)ì´ë‹¤.
ë‹¨ìˆœí•œ ì •ë³´ ìš”ì•½ìê°€ ì•„ë‹ˆë¼, ì œê³µëœ ìë£Œì˜ ìˆ˜ì¹˜ì™€ ë¬¼ë¦¬ ë²•ì¹™ì´ ë‹µë³€ì— 'ì™œê³¡ ì—†ì´' ë°˜ì˜ë˜ì—ˆëŠ”ì§€ ê°ì‹œí•˜ë¼.

# ============================================================
# ğŸ“š ê²€ì¦ëœ í‘œì¤€ ì°¸ì¡° ìë£Œ (Standard Reference - ì¸ìš© ê°€ëŠ¥)
# ============================================================
# ì•„ë˜ ë‚´ìš©ì€ ì›ë³¸ ë…¼ë¬¸ì—ì„œ ê²€ì¦ëœ í‘œì¤€ ì§€ì‹ì…ë‹ˆë‹¤.
# RAG ê²€ìƒ‰ ê²°ê³¼ì™€ ë™ë“±í•˜ê²Œ ì¸ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# íŠ¹íˆ Dance et al. 2011 ë…¼ë¬¸ì˜ t-factor/T-factor í…Œì´ë¸”ì€
# ì§ì ‘ ì¸ìš©í•˜ì—¬ ë‹µë³€ì— í™œìš©í•˜ì„¸ìš”.
# ============================================================

{core_physics}

# ============================================================
# í‘œì¤€ ì°¸ì¡° ìë£Œ ë
# ============================================================

# Strict Instruction (ì ˆëŒ€ ì¤€ìˆ˜ ì‚¬í•­)
0. **ê²€ì¦ëœ ìˆ˜ì¹˜ ìš°ì„  ì‚¬ìš© (Data Priority)**:
   - ìœ„ [í‘œì¤€ ë¬¼ë¦¬í•™ ì°¸ì¡°]ì— ëª…ì‹œëœ ê²€ì¦ ìˆ˜ì¹˜(Wê°’, QE, Ghosting, Lag ë“±)ê°€ ìˆìœ¼ë©´ **ë°˜ë“œì‹œ** í•´ë‹¹ ê°’ì„ ì‚¬ìš©í•˜ë¼.
   - **âš ï¸ W = 50-64 eV** (a-Se ê²€ì¶œê¸°). 3.6 eVëŠ” ì‹¤ë¦¬ì½˜ ê°’ì´ë¯€ë¡œ **ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€**.
   - **âš ï¸ ì–‘ìê²€ì¶œíš¨ìœ¨ (QE = QDE)**: QE(LE 28kVp) = **97%**, QE(HE 49kVp) = **56%** â€” CEM ì§ˆë¬¸ì— í•„ìˆ˜.
   - **âš ï¸ ì´ë¡  ê³µì‹ QE=1-e^(-Î¼t) ì‚¬ìš© ê¸ˆì§€!** ìœ„ **ì‹¤ì¸¡ê°’(97%, 56%)**ì„ ì§ì ‘ ì¸ìš©í•˜ë¼.
   - **âš ï¸ Ghosting = 15%, Lag = 0.15%** â€” 100ë°° ì°¨ì´! ë‘˜ì„ í˜¼ë™í•˜ì§€ ë§ ê²ƒ.
   - ë„¤ ë‚´ë¶€ ì§€ì‹ì´ ìœ„ ìˆ˜ì¹˜ì™€ ë‹¤ë¥´ë©´, **ìœ„ ê²€ì¦ ìˆ˜ì¹˜ë¥¼ ìš°ì„ **í•˜ë¼.

1. **ê²°ë¡  ë„ì¶œ ê¸ˆì§€ (Evidence First)**:
   - ë„¤ê°€ ì´ë¯¸ ì•Œê³  ìˆëŠ” ì¼ë°˜ ì§€ì‹ìœ¼ë¡œ ê²°ë¡ ì„ ë¨¼ì € ë‚´ë¦¬ì§€ ë§ˆë¼.
   - ë°˜ë“œì‹œ ì œê³µëœ [í‘œì¤€ ì°¸ì¡° ìë£Œ], [ê°€ì´ë“œë¼ì¸], [ì—°êµ¬ ë…¼ë¬¸]ì˜ í…ìŠ¤íŠ¸ì—ì„œ ì§ì ‘ì ì¸ ê·¼ê±°ë¥¼ ë¨¼ì € ë‚˜ì—´í•œ í›„, ê·¸ ë°ì´í„°ê°€ í—ˆìš©í•˜ëŠ” ë²”ìœ„ ë‚´ì—ì„œë§Œ ê²°ë¡ ì„ ë„ì¶œí•˜ë¼.

2. **ë¬¼ë¦¬ ê°œë…ì˜ ì—„ê²©í•œ ì •ì˜ (Grounding Physics)**:
   - ìœ„ "í•„ìˆ˜ ë¬¼ë¦¬ ì§€ì‹"ê³¼ Mammography ë¬¼ë¦¬ ë²•ì¹™ì„ í˜¼ë™í•˜ì§€ ë§ˆë¼.
   - [Hard Beam]: ì—ë„ˆì§€ê°€ ë†’ìŒ(keVâ†‘), K-edgeê°€ ë†’ìŒ, íˆ¬ê³¼ë ¥ ë†’ìŒ, ë‘êº¼ìš´ ìœ ë°©ì— í•„ìˆ˜.
   - [Soft Beam]: ì—ë„ˆì§€ê°€ ë‚®ìŒ(keVâ†“), K-edgeê°€ ë‚®ìŒ, íˆ¬ê³¼ë ¥ ë‚®ìŒ, ì–‡ì€ ìœ ë°©ì— ì í•©.
   - MGD ê´€ë ¨ ì§ˆë¬¸ì—ì„œëŠ” Dance et al. 2011ì˜ t-factor/T-factor ê°œë…ì„ ì •í™•íˆ ì ìš©í•˜ë¼.
   - ì´ ì •ì˜ì™€ ë°˜ëŒ€ë˜ëŠ” ì£¼ì¥ì„ ë…¼ë¬¸ì— ê·¼ê±° ì—†ì´ ì‘ì„±í•  ê²½ìš°, ì´ëŠ” ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ë¡œ ê°„ì£¼í•œë‹¤.

3. **ì¸ìš© ë¬´ê²°ì„± (Quoting & Verification)**:
   - ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ì–¸ê¸‰í•  ë•ŒëŠ” ë°˜ë“œì‹œ í•´ë‹¹ ë…¼ë¬¸ì˜ ë²ˆí˜¸ì™€ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë³‘ê¸°í•˜ë¼.
   - ì´ˆë¡(Abstract)ì— ëª…ì‹œë˜ì§€ ì•Šì€ ë°ì´í„°(ì˜ˆ: êµ¬ì²´ì  ìˆ˜ì¹˜ ë“±)ë¥¼ "í•©ì„±"í•˜ê±°ë‚˜ "ìœ ì¶”"í•˜ì—¬ ê¸°ì¬í•˜ì§€ ë§ˆë¼.
   - ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ "ì œì‹œëœ ìë£Œì—ëŠ” êµ¬ì²´ì  ìˆ˜ì¹˜ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŒ"ì„ ëª…ì‹œí•˜ë¼.
   - **ì œê³µëœ ìë£Œì— ì—†ëŠ” ë…¼ë¬¸(ì €ìëª…, ì—°ë„)ì€ ì ˆëŒ€ ì¸ìš©í•˜ì§€ ë§ˆë¼. ì´ëŠ” ì‹¬ê°í•œ Hallucination ì˜¤ë¥˜ë‹¤.**

4. **ëª¨ìˆœ ë°œìƒ ì‹œ ë³´ê³ **:
   - ì§ˆë¬¸ì˜ ë‚´ìš©ê³¼ ë…¼ë¬¸ì˜ ë°ì´í„°ê°€ ìƒì¶©í•˜ê±°ë‚˜, ë…¼ë¬¸ ë‚´ë¶€ì˜ ë…¼ë¦¬ê°€ ë„¤ ìƒì‹ê³¼ ë‹¤ë¥¼ ê²½ìš° ì§€ì–´ë‚´ì§€ ë§ê³  "ìë£Œ ê°„ì˜ ë…¼ë¦¬ì  ëª¨ìˆœ"ì„ ë³´ê³ í•˜ë¼.

# Response Format (í•œêµ­ì–´ë¡œ ë‹µë³€)
âš ï¸ **ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì¼ë³¸ì–´/ì¤‘êµ­ì–´ ì¶œë ¥ ê¸ˆì§€.**
- [ë°ì´í„° ê¸°ë°˜ ê·¼ê±°]: ê° ë…¼ë¬¸ì—ì„œ ì¶”ì¶œí•œ íŒ©íŠ¸ ë‚˜ì—´ (ë…¼ë¬¸ ë²ˆí˜¸ ëª…ì‹œ)
- [ë¬¼ë¦¬ì  ì¸ê³¼ê´€ê³„ ê²€ì¦]: ì¶”ì¶œëœ íŒ©íŠ¸ì™€ ë¬¼ë¦¬ ë²•ì¹™ì˜ ì¼ì¹˜ì„± í™•ì¸
- [ìµœì¢… ê²°ë¡ ]: ë°ì´í„°ê°€ ë³´ì¥í•˜ëŠ” ë²”ìœ„ ë‚´ì—ì„œì˜ ì„¤ê³„ ì§€ì¹¨

# ë‹µë³€ ìƒì„¸í™” ì›ì¹™ (Comprehensive Response)
1. **ìˆ˜ì‹ í¬í•¨**: ë¬¼ë¦¬ ê°œë… ì„¤ëª… ì‹œ ê´€ë ¨ ìˆ˜ì‹ì„ ë°˜ë“œì‹œ í¬í•¨í•˜ë¼.
   - ì˜ˆ: "t(Î¸) = D(Î¸) / D(0)" í˜•íƒœë¡œ ëª…ì‹œ
   - ìˆ˜ì‹ì˜ ê° ë³€ìˆ˜ ì˜ë¯¸ë¥¼ ì„¤ëª…í•˜ë¼.

2. **ìœ ë„ ê³¼ì • ì„¤ëª…**: ê°œë… ê°„ ì—°ê²° ê´€ê³„ë¥¼ ìœ ë„ ê³µì‹ìœ¼ë¡œ ë³´ì—¬ë¼.
   - ì˜ˆ: t-factorê°€ T-factorë¡œ ì–´ë–»ê²Œ í†µí•©ë˜ëŠ”ì§€
   - "T = Î£ Î±áµ¢ Ã— t(Î¸áµ¢)"ì™€ ê°™ì´ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…

3. **í…Œì´ë¸”/ìˆ˜ì¹˜ ì¸ìš©**: ë…¼ë¬¸ì˜ í…Œì´ë¸” ë°ì´í„°ê°€ ìˆë‹¤ë©´ êµ¬ì²´ì  ìˆ˜ì¹˜ë¥¼ ì œì‹œí•˜ë¼.
   - ì˜ˆ: "Table 6ì— ë”°ë¥´ë©´, 6.5cm ë‘ê»˜ì—ì„œ t(20Â°) = 0.929"

4. **ì‹¤ë¬´ ì ìš©**: ì¥ë¹„ ì„¤ê³„ë‚˜ AEC ì•Œê³ ë¦¬ì¦˜ ê´€ì ì—ì„œ ì‹¤ë¬´ì  ì˜ë¯¸ë¥¼ ì„¤ëª…í•˜ë¼.
   - ì˜ˆ: "ì´ ê³µì‹ì€ ê° ê°ë„ë³„ mAs ë°°ë¶„ ì „ëµ ìˆ˜ë¦½ì— ì‚¬ìš©ë¨"

5. **í˜ì´ì§€/ì„¹ì…˜ ì°¸ì¡°**: ê°€ëŠ¥í•˜ë©´ ë…¼ë¬¸ì˜ êµ¬ì²´ì  í˜ì´ì§€ë‚˜ ì„¹ì…˜ì„ ëª…ì‹œí•˜ë¼.
   - ì˜ˆ: "Dance et al. 2011, 8-9í˜ì´ì§€ ì°¸ì¡°\""""

        # Phase 7.20: Grounded Valuesë¥¼ user_message ìµœìƒë‹¨ì— ë°°ì¹˜
        grounded_section = ""
        if grounded_values_context:
            grounded_section = f"""
{grounded_values_context}

"""

        user_message = f"""ë‹¤ìŒ ì°¸ê³  ìë£Œë¥¼ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
{grounded_section}
**âš ï¸ ë°ì´í„° ìš°ì„ ìˆœìœ„ ê·œì¹™ (CRITICAL):**
0. **[ê²€ì¦ëœ ë°ì´í„° í…Œì´ë¸”]** (ìœ„ì— í‘œì‹œ) â†’ **ì ˆëŒ€ ìµœìš°ì„  - ì´ ê°’ë§Œ ì‚¬ìš©**
1. **[í‘œì¤€ ë¬¼ë¦¬í•™ ì°¸ì¡°]** (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë‚´) â†’ **ìš°ì„  ì‚¬ìš©**
   - ê²€ì¦ëœ ë¬¼ë¦¬ ìƒìˆ˜ì™€ ìˆ˜ì¹˜ (Wê°’, QDE, Ghosting ë“±)
   - **ì´ ì„¹ì…˜ì— ëª…ì‹œëœ ìˆ˜ì¹˜ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í•´ë‹¹ ê°’ì„ ì‚¬ìš©í•  ê²ƒ**
2. **[ê²€ìƒ‰ëœ ë…¼ë¬¸]**: ì•„ë˜ RAG ê²€ìƒ‰ ê²°ê³¼

**âš ï¸ Hallucination ê¸ˆì§€**: ìœ„ ë‘ ì¶œì²˜ì— ì—†ëŠ” ë…¼ë¬¸(ì €ìëª…, ì—°ë„)ì€ ì ˆëŒ€ ì¸ìš©í•˜ì§€ ë§ˆì„¸ìš”.

**ê²€ìƒ‰ëœ ë…¼ë¬¸ (RAG ê²°ê³¼):**
{context}

**ì§ˆë¬¸:** {question}

**ìš”êµ¬ì‚¬í•­:**
- **[ê²€ì¦ëœ ë°ì´í„° í…Œì´ë¸”]ì˜ ìˆ˜ì¹˜ë¥¼ ë°˜ë“œì‹œ ì‚¬ìš©** (Ghosting=15%, Lag=0.15% ë“±)
- **[í‘œì¤€ ë¬¼ë¦¬í•™ ì°¸ì¡°]ì— ëª…ì‹œëœ ê²€ì¦ ìˆ˜ì¹˜ë¥¼ ìš°ì„  ì‚¬ìš©** (Wê°’, QDE ë“±)
- **ìˆ˜ì‹ì„ í¬í•¨**í•˜ì—¬ ë¬¼ë¦¬ì  ê´€ê³„ë¥¼ ëª…í™•íˆ ì„¤ëª…
- **ìœ ë„ ê³¼ì •**ì„ ë‹¨ê³„ë³„ë¡œ ë³´ì—¬ì¤„ ê²ƒ
- **êµ¬ì²´ì  ìˆ˜ì¹˜**ë¥¼ [í‘œì¤€ ë¬¼ë¦¬í•™ ì°¸ì¡°]ì—ì„œ ì •í™•íˆ ì¸ìš©
- **ì‹¤ë¬´ ì ìš©** ê´€ì ì—ì„œ ì¥ë¹„ ì„¤ê³„/ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì— ì–´ë–»ê²Œ í™œìš©ë˜ëŠ”ì§€ ì„¤ëª…
- ìœ„ ë‘ ì¶œì²˜ì— ì—†ëŠ” ë…¼ë¬¸ì€ ì ˆëŒ€ ì¸ìš©í•˜ì§€ ë§ ê²ƒ
- ìë£Œì— ë‹µì´ ì—†ìœ¼ë©´ "ì œê³µëœ ìë£Œì—ëŠ” í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œ
"""
    else:
        # ê°€ì´ë“œë¼ì¸ ì—†ìŒ: ë³´ìˆ˜ì  ì•ˆë‚´í˜• (ë…¼ë¬¸ ì´ˆë¡ ê¸°ë°˜ ì¶”ì •)
        system_message = f"""ë‹¹ì‹ ì€ ìœ ë°©ì˜ìƒí•™ ì „ë¬¸ ì˜í•™ ë¬¼ë¦¬ ë³´ì¡°ì›ì…ë‹ˆë‹¤.

# ============================================================
# ğŸ“š ê²€ì¦ëœ í‘œì¤€ ì°¸ì¡° ìë£Œ (Standard Reference - ì¸ìš© ê°€ëŠ¥)
# ============================================================
# ì•„ë˜ ë‚´ìš©ì€ ì›ë³¸ ë…¼ë¬¸ì—ì„œ ê²€ì¦ëœ í‘œì¤€ ì§€ì‹ì…ë‹ˆë‹¤.
# RAG ê²€ìƒ‰ ê²°ê³¼ì™€ ë™ë“±í•˜ê²Œ ì¸ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# ============================================================

{core_physics}

# ============================================================
# í‘œì¤€ ì°¸ì¡° ìë£Œ ë
# ============================================================

**í•µì‹¬ ì›ì¹™:**
1. **ì •ì§í•œ ë¶€ì¬ ê³ ì§€**: ê°€ì´ë“œë¼ì¸(ACR/BI-RADS)ì— ì§ì ‘ì  ë‹µë³€ì´ ì—†ìŒì„ ë¨¼ì € ë°íˆì„¸ìš”.
2. **í‘œì¤€ ì°¸ì¡° + ì´ˆë¡ ê¸°ë°˜ ì¶”ë¡ **: ìœ„ [í‘œì¤€ ì°¸ì¡° ìë£Œ]ì™€ RAG ê²€ìƒ‰ëœ ë…¼ë¬¸ ì´ˆë¡ì„ í•¨ê»˜ í™œìš©í•˜ì„¸ìš”.
3. **í™˜ê° ë°©ì§€**: ìœ„ ë‘ ì¶œì²˜ì— ì—†ëŠ” ìˆ˜ì¹˜ë¥¼ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.
4. **ë¬¼ë¦¬ ì›ë¦¬**: [í‘œì¤€ ì°¸ì¡° ìë£Œ]ì˜ Dance et al. 2011 t-factor/T-factor í…Œì´ë¸”ì€ ì§ì ‘ ì¸ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.

**ì¶œë ¥ êµ¬ì¡°:**
1. [ì§€ì¹¨ í™•ì¸]: "ê°€ì´ë“œë¼ì¸ì—ëŠ” ì´ ì£¼ì œì— ëŒ€í•œ ì§ì ‘ì ì¸ ëª…ì‹œê°€ ì—†ìŠµë‹ˆë‹¤."
2. [í‘œì¤€ ì°¸ì¡° + ì´ˆë¡ ë¶„ì„]: Dance et al. í…Œì´ë¸” ë° ë…¼ë¬¸ ë²ˆí˜¸ë¥¼ ì¸ìš©í•˜ë©° ë¬¼ë¦¬ì  ì›ë¦¬ ì„¤ëª…
3. [ì œí•œ ì‚¬í•­]: í•„ìš”ì‹œ "â„¹ï¸ ì´ˆë¡(Abstract) ê¸°ë°˜ ë¶„ì„ì…ë‹ˆë‹¤" ê³ ì§€
"""

        # Phase 7.20: Grounded Values (ê°€ì´ë“œë¼ì¸ ì—†ëŠ” ê²½ìš°ë„ ì ìš©)
        grounded_section_no_guide = ""
        if grounded_values_context:
            grounded_section_no_guide = f"""
{grounded_values_context}

"""

        user_message = f"""**ì‚¬ìš©ì ì§ˆë¬¸:** {question}
{grounded_section_no_guide}
**âš ï¸ ì¸ìš© ê°€ëŠ¥í•œ ìë£Œ:**
0. **[ê²€ì¦ëœ ë°ì´í„° í…Œì´ë¸”]** (ìœ„ì— í‘œì‹œ) â†’ **ì ˆëŒ€ ìµœìš°ì„  - ì´ ê°’ë§Œ ì‚¬ìš©**
1. **[í‘œì¤€ ì°¸ì¡° ìë£Œ]**: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì˜ Dance et al. 2011 t-factor/T-factor í…Œì´ë¸” ë° MGD ê³µì‹
2. **[ê²€ìƒ‰ëœ ë…¼ë¬¸]**: ì•„ë˜ RAG ê²€ìƒ‰ ê²°ê³¼

**ê²€ìƒ‰ëœ ì—°êµ¬ ë…¼ë¬¸ (ì´ˆë¡):**
{context}

ìœ„ ì„¸ ì¶œì²˜ë¥¼ í™œìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
- **[ê²€ì¦ëœ ë°ì´í„° í…Œì´ë¸”]ì˜ ìˆ˜ì¹˜ë¥¼ ë°˜ë“œì‹œ ì‚¬ìš©**
- ê°€ì´ë“œë¼ì¸ ë¶€ì¬ë¥¼ ë¨¼ì € ì–¸ê¸‰
- **Dance et al. 2011 Table 6 ìˆ˜ì¹˜** ì§ì ‘ ì¸ìš© ê°€ëŠ¥ (ì˜ˆ: "5cm ë‘ê»˜ì—ì„œ t(20Â°)=0.919")
- ë…¼ë¬¸ ë²ˆí˜¸ë¥¼ ì¸ìš©í•˜ì—¬ ë¬¼ë¦¬ì  ì›ë¦¬ ì„¤ëª…
- ìœ„ ì„¸ ì¶œì²˜ì— ì—†ëŠ” ë…¼ë¬¸ì€ ì¸ìš© ê¸ˆì§€
"""

    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_message}
    ]

    # ğŸ›¡ï¸ ë°ì´í„° ë¬´ê²°ì„± ëª¨ë“œ: Temperature 0.0ìœ¼ë¡œ ê²°ì •ë¡ ì  ì‘ë‹µ ê°•ì œ
    # has_guidelines=Trueì¼ ë•ŒëŠ” ì°½ì˜ì„±ì„ ì™„ì „íˆ ì°¨ë‹¨í•˜ì—¬ hallucination ë°©ì§€
    actual_temperature = 0.0 if has_guidelines else temperature

    payload = {
        "model": model,
        "messages": messages,
        "stream": True,
        "options": {
            "temperature": actual_temperature,
        }
    }

    try:
        print(f"[DEBUG] LLM í˜¸ì¶œ ì‹œì‘: model={model}, context_len={len(context)}, has_guidelines={has_guidelines}")
        response = requests.post(url, json=payload, stream=True, timeout=180)
        response.raise_for_status()

        chunk_count = 0
        for line in response.iter_lines():
            if line:
                try:
                    chunk = json.loads(line)
                    if "message" in chunk and "content" in chunk["message"]:
                        chunk_count += 1
                        yield chunk["message"]["content"]
                except json.JSONDecodeError as je:
                    print(f"[DEBUG] JSON íŒŒì‹± ì˜¤ë¥˜: {je}, line={line[:100]}")
                    continue
        print(f"[DEBUG] LLM í˜¸ì¶œ ì™„ë£Œ: {chunk_count} chunks")
    except requests.exceptions.RequestException as e:
        print(f"[DEBUG] LLM ì—°ê²° ì˜¤ë¥˜: {e}")
        yield f"âš ï¸ LLM ì—°ê²° ì˜¤ë¥˜: {str(e)}"
    except Exception as e:
        print(f"[DEBUG] LLM ì˜ˆì™¸ ë°œìƒ: {type(e).__name__}: {e}")
        yield f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}"


def verify_hallucination(response: str, context: str) -> dict:
    """
    LLM ì‘ë‹µì—ì„œ Hallucination(í—ˆìœ„ ì¸ìš©) ë° ìë£Œ ë¬´ì‹œë¥¼ íƒì§€

    Args:
        response: LLM ì‘ë‹µ í…ìŠ¤íŠ¸
        context: ì œê³µëœ ì°¸ê³  ìë£Œ (ì›ë³¸)

    Returns:
        {
            "has_hallucination": bool,
            "suspicious_citations": list,  # ì˜ì‹¬ë˜ëŠ” ì¸ìš© ëª©ë¡
            "ignored_context": bool,  # ìë£Œ ë¬´ì‹œ ì—¬ë¶€
            "physics_errors": list,  # ë¬¼ë¦¬ ê°œë… ì˜¤ë¥˜
            "warning_message": str  # ê²½ê³  ë©”ì‹œì§€
        }
    """
    import re

    suspicious = []
    physics_errors = []
    ignored_context = False
    context_lower = context.lower()
    response_lower = response.lower()

    # ========================================
    # 1. í—ˆìœ„ ì¸ìš© íƒì§€ (ê¸°ì¡´)
    # ========================================

    # 1-1. "ì €ì et al." íŒ¨í„´ íƒì§€
    et_al_pattern = r'([A-Z][a-z]+)\s+et\s+al\.?'
    et_al_matches = re.findall(et_al_pattern, response)

    for author in et_al_matches:
        if author.lower() not in context_lower:
            suspicious.append(f"{author} et al.")

    # 1-2. "ì €ì (ì—°ë„)" íŒ¨í„´ íƒì§€
    author_year_pattern = r'([A-Z][a-z]+)\s*\(\s*(19|20)\d{2}\s*\)'
    author_year_matches = re.findall(author_year_pattern, response)

    for author, _ in author_year_matches:
        if author.lower() not in context_lower:
            citation = f"{author} (ì—°ë„)"
            if f"{author} et al." not in suspicious:
                suspicious.append(citation)

    # 1-3. "~ì— ë”°ë¥´ë©´" íŒ¨í„´ íƒì§€
    according_pattern = r'([A-Z][a-z]+)ì—\s*ë”°ë¥´ë©´'
    according_matches = re.findall(according_pattern, response)

    for author in according_matches:
        if author.lower() not in context_lower and author not in ["Reference", "ì°¸ê³ ", "ìë£Œ", "ë…¼ë¬¸"]:
            if not any(author in s for s in suspicious):
                suspicious.append(f"{author}ì— ë”°ë¥´ë©´")

    # ========================================
    # 2. ìë£Œ ë¬´ì‹œ íƒì§€ (ì‹ ê·œ)
    # ========================================

    # "ìë£Œ ì—†ìŒ" íŒ¨í„´ íƒì§€
    no_data_patterns = [
        r"ë‚´ìš©ì„\s*ì°¾ì„\s*ìˆ˜\s*ì—†",
        r"í•´ë‹¹\s*ì •ë³´ê°€?\s*ì—†",
        r"ìë£Œì—ì„œ?\s*í™•ì¸ë˜ì§€\s*ì•Š",
        r"ì§ì ‘ì ì¸\s*ë‹µì„?\s*í¬í•¨í•˜ì§€\s*ì•Š",
        r"ê´€ë ¨ëœ\s*ë‚´ìš©ì„?\s*ì°¾ì„\s*ìˆ˜\s*ì—†",
        r"ì œê³µëœ\s*ìë£Œì—ëŠ”?\s*ì—†",
    ]

    claims_no_data = any(re.search(p, response_lower) for p in no_data_patterns)

    # Contextì— ê´€ë ¨ í‚¤ì›Œë“œê°€ ì‹¤ì œë¡œ ìˆëŠ”ì§€ í™•ì¸
    relevant_keywords_in_context = []
    important_keywords = [
        "denoising", "deep learning", "microcalcification", "noise reduction",
        "quantum", "mottle", "mtf", "spatial resolution", "cnn", "wavelet",
        "ë””ë…¸ì´ì§•", "ë”¥ëŸ¬ë‹", "ë¯¸ì„¸ì„íšŒí™”", "ë…¸ì´ì¦ˆ"
    ]

    for kw in important_keywords:
        if kw.lower() in context_lower:
            relevant_keywords_in_context.append(kw)

    # "ìë£Œ ì—†ë‹¤"ê³  í–ˆëŠ”ë° ì‹¤ì œë¡œ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ = ìë£Œ ë¬´ì‹œ
    if claims_no_data and len(relevant_keywords_in_context) >= 2:
        ignored_context = True

    # ========================================
    # 3. ê°€ì§œ ì°¸ê³ ìë£Œ íƒì§€ (ì‹ ê·œ)
    # ========================================

    fake_references = set()  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ set ì‚¬ìš©

    # "ì°¸ê³  ìë£Œ:" ì„¹ì…˜ì—ì„œ ê°€ì§œ ë¬¸ì„œ ì œëª© íƒì§€
    # ì‹¤ì œ ë…¼ë¬¸ì´ ì•„ë‹Œ ì¼ë°˜ì ì¸ ì„¤ëª… í˜•íƒœì˜ ì°¸ê³ ìë£Œ

    # 3-1. ê°œë³„ ë¼ì¸ì—ì„œ ê°€ì§œ ì°¸ê³ ìë£Œ íƒì§€ (ë” ì •ë°€í•œ íŒ¨í„´)
    # ì‹¤ì œ ì˜ˆì‹œ: "[1] ì œì¡°ì—…ì²´ì˜ Post-processing ì•Œê³ ë¦¬ì¦˜ ì„¤ëª…ì„œ"
    fake_ref_line_patterns = [
        # ì œì¡°ì—…ì²´/ì—…ì²´ ê´€ë ¨
        r'\[?\d+\]?\s*.{0,30}(ì œì¡°ì—…ì²´|ì œì¡°ì‚¬|ì—…ì²´).{0,50}(ì„¤ëª…ì„œ|ë¬¸ì„œ|ë§¤ë‰´ì–¼|ê°€ì´ë“œ)',
        # ì¼ë°˜ì ì¸ ë³´ê³ ì„œ/ë¬¸ì„œ
        r'\[?\d+\]?\s*.{0,50}(ê¸°ìˆ \s*ë³´ê³ ì„œ|ë¶„ì„\s*ë³´ê³ ì„œ|ì—°êµ¬\s*ë³´ê³ ì„œ|ë¶„ì„\s*ë¬¸ì„œ)',
        r'\[?\d+\]?\s*.{0,50}(ì•Œê³ ë¦¬ì¦˜\s*ì„¤ëª…ì„œ|ê¸°ë²•\s*ë¬¸ì„œ|ì²˜ë¦¬\s*ë¬¸ì„œ)',
        # ë…¸ì´ì¦ˆ/ì´ë¯¸ì§€ ê´€ë ¨ ì¼ë°˜ ë¬¸ì„œ (ì €ìëª… ì—†ì´)
        r'\[?\d+\]?\s*(ë…¸ì´ì¦ˆ|ì´ë¯¸ì§€|ì˜ìƒ).{0,30}(ê¸°ë²•|ë°©ë²•|ì²˜ë¦¬).{0,20}(ë¬¸ì„œ|ì„¤ëª…|ë³´ê³ ì„œ)',
        # ëŒ€ë¹„/í–¥ìƒ ê´€ë ¨ ì¼ë°˜ ë¬¸ì„œ
        r'\[?\d+\]?\s*.{0,30}(ëŒ€ë¹„|contrast).{0,20}(í–¥ìƒ|enhancement).{0,20}(ë¬¸ì„œ|ë³´ê³ ì„œ|ê¸°ìˆ )',
        # Post-processing ê´€ë ¨
        r'\[?\d+\]?\s*.{0,30}(post-?processing|í›„ì²˜ë¦¬).{0,30}(ì„¤ëª…ì„œ|ë¬¸ì„œ|ì•Œê³ ë¦¬ì¦˜)',
    ]

    for pattern in fake_ref_line_patterns:
        matches = re.finditer(pattern, response, re.IGNORECASE)
        for match in matches:
            matched_text = match.group(0).strip()
            # Contextì— ì´ ë‚´ìš©ì´ ì—†ìœ¼ë©´ ê°€ì§œ
            if matched_text.lower() not in context_lower:
                fake_references.add(f"'{matched_text}'")

    # "ì°¸ê³  ìë£Œ:" ì„¹ì…˜ ì „ì²´ ê²€ì‚¬ - ì‹¤ì œ ë…¼ë¬¸ ì œëª©ì´ ì—†ìœ¼ë©´ ê°€ì§œ
    ref_section_match = re.search(r'ì°¸ê³ \s*ìë£Œ[:\s]*\n?(.*?)(?:\n\n|$)', response, re.DOTALL | re.IGNORECASE)
    if ref_section_match:
        ref_section = ref_section_match.group(1)
        # Contextì—ì„œ ì‹¤ì œ ë…¼ë¬¸ ì œëª© ì¶”ì¶œ
        real_titles = re.findall(r'ì œëª©:\s*([^\n]+)', context)

        # ì°¸ê³ ìë£Œ ì„¹ì…˜ì— ì‹¤ì œ ë…¼ë¬¸ ì œëª©ì´ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ê²½ê³ 
        has_real_title = False
        for title in real_titles:
            title_words = title.lower().split()[:3]  # ì²« 3ë‹¨ì–´ë§Œ ë¹„êµ
            if any(word in ref_section.lower() for word in title_words if len(word) > 3):
                has_real_title = True
                break

        if not has_real_title and len(real_titles) > 0:
            fake_references.add("ì°¸ê³ ìë£Œ ì„¹ì…˜ì´ ì‹¤ì œ ì œê³µëœ ë…¼ë¬¸ê³¼ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

    # setì„ listë¡œ ë³€í™˜
    fake_references = list(fake_references)

    # ========================================
    # 4. ë¬¼ë¦¬ ê°œë… ì˜¤ë¥˜ íƒì§€ (ê·œì¹™ ì—”ì§„ ì‚¬ìš©)
    # ========================================

    try:
        from src.validation.physics_rules import check_physics_errors
        detected_errors = check_physics_errors(response)
        for error in detected_errors:
            physics_errors.append(f"{error['description']} â†’ {error['correct_statement']}")
    except ImportError:
        # ê·œì¹™ ì—”ì§„ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²€ì‚¬ (fallback)
        pass

    # ========================================
    # 5. ê²½ê³  ë©”ì‹œì§€ ìƒì„±
    # ========================================

    has_issues = (
        len(suspicious) > 0 or
        ignored_context or
        len(physics_errors) > 0 or
        len(fake_references) > 0
    )

    warning_parts = []

    if suspicious:
        warning_parts.append("âš ï¸ **í—ˆìœ„ ì¸ìš© ì˜ì‹¬**: ë‹¤ìŒ ì¸ìš©ì´ ìë£Œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤:")
        for i, citation in enumerate(suspicious, 1):
            warning_parts.append(f"  {i}. `{citation}`")
        warning_parts.append("")

    if fake_references:
        warning_parts.append("ğŸ“ **ê°€ì§œ ì°¸ê³ ìë£Œ íƒì§€**: AIê°€ ì‹¤ì œ ë…¼ë¬¸ì´ ì•„ë‹Œ ê°€ì§œ ì°¸ê³ ìë£Œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤:")
        for i, fake_ref in enumerate(fake_references, 1):
            warning_parts.append(f"  {i}. {fake_ref}")
        warning_parts.append("  _ì‹¤ì œ ì œê³µëœ ë…¼ë¬¸: Contextì˜ 'ì œëª©:' í•­ëª©ì„ í™•ì¸í•˜ì„¸ìš”._")
        warning_parts.append("")

    if ignored_context:
        warning_parts.append("ğŸš¨ **ìë£Œ ë¬´ì‹œ ê²½ê³ **: AIê°€ 'ìë£Œ ì—†ìŒ'ì´ë¼ ë‹µí–ˆì§€ë§Œ, ì‹¤ì œë¡œ ê´€ë ¨ ë‚´ìš©ì´ ìˆìŠµë‹ˆë‹¤:")
        warning_parts.append(f"  ë°œê²¬ëœ í‚¤ì›Œë“œ: `{', '.join(relevant_keywords_in_context[:5])}`")
        warning_parts.append("  _ì œê³µëœ ë…¼ë¬¸ì„ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”._")
        warning_parts.append("")

    if physics_errors:
        warning_parts.append("ğŸ”´ **ë¬¼ë¦¬ ê°œë… ì˜¤ë¥˜**: ë‹¤ìŒ ë‚´ìš©ì´ ë¬¼ë¦¬ ë²•ì¹™ê³¼ ì¶©ëŒí•©ë‹ˆë‹¤:")
        for i, error in enumerate(physics_errors, 1):
            warning_parts.append(f"  {i}. {error}")
        warning_parts.append("")

    warning_message = "\n".join(warning_parts) if warning_parts else ""

    return {
        "has_hallucination": len(suspicious) > 0 or len(fake_references) > 0,
        "suspicious_citations": suspicious,
        "fake_references": fake_references,
        "ignored_context": ignored_context,
        "physics_errors": physics_errors,
        "warning_message": warning_message
    }


def verify_relevance(question: str, documents: list, model="gpt-oss:20b", has_physics_knowledge: bool = False) -> dict:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì§ˆë¬¸ì— ê´€ë ¨ì´ ìˆëŠ”ì§€ LLMìœ¼ë¡œ 3ë‹¨ê³„ ê²€ì¦

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        documents: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        model: LLM ëª¨ë¸ëª…
        has_physics_knowledge: KnowledgeManagerì—ì„œ ê´€ë ¨ ë¬¼ë¦¬ ì§€ì‹ì„ ì°¾ì•˜ëŠ”ì§€ ì—¬ë¶€

    Returns:
        {"level": "high"/"medium"/"low", "reason": str, "relevant_indices": list}
    """
    # KnowledgeManagerì—ì„œ ê´€ë ¨ ì§€ì‹ì„ ì°¾ì•˜ìœ¼ë©´ ë°”ë¡œ high ë°˜í™˜
    if has_physics_knowledge:
        return {
            "level": "high",
            "reason": "í‘œì¤€ ì°¸ì¡° ìë£Œ(ë¬¼ë¦¬ ì§€ì‹ ëª¨ë“ˆ)ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ í™•ì¸í–ˆìŠµë‹ˆë‹¤.",
            "relevant_indices": list(range(1, len(documents)+1))
        }

    url = "http://localhost:11434/api/chat"

    # ë¬¸ì„œ ë‚´ìš© ìš”ì•½ (ì²« 500ìì”©)
    doc_summaries = []
    for i, doc in enumerate(documents, 1):
        content = doc.get('content', '')[:500]
        title = doc.get('title', '')
        doc_summaries.append(f"[{i}] {title}\n{content}...")

    docs_text = "\n\n".join(doc_summaries)

    system_message = """ë‹¹ì‹ ì€ ë¬¸ì„œ ê´€ë ¨ì„± ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ë¬¸ì„œë“¤ì´ **ì§ì ‘ì ì¸ ë‹µ**ì„ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ 3ë‹¨ê³„ë¡œ íŒë‹¨í•˜ì„¸ìš”.

**íŒë‹¨ ê¸°ì¤€:**
- high: ë¬¸ì„œê°€ ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µì„ í¬í•¨í•¨
  ì˜ˆ: "margin ë¶„ë¥˜ëŠ”?" â†’ ë¬¸ì„œì— margin ë¶„ë¥˜ ëª©ë¡ê³¼ ì„¤ëª…ì´ ìˆìŒ
- medium: ë¬¸ì„œê°€ ê´€ë ¨ ì£¼ì œë¥¼ ë‹¤ë£¨ì§€ë§Œ ì§ì ‘ì  ë‹µì€ ì—†ìŒ
  ì˜ˆ: "Mammography ê¸°ë³¸ ê°œë…?" â†’ ë¬¸ì„œì— Mammography ì–¸ê¸‰ë§Œ ìˆê³  ê¸°ë³¸ ê°œë… ì„¤ëª…(ì›ë¦¬, ë°©ë²• ë“±)ì€ ì—†ìŒ
- low: ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê±°ì˜ ê´€ë ¨ ì—†ìŒ
  ì˜ˆ: "MRI ì´¬ì˜ ë°©ë²•?" â†’ ë¬¸ì„œì— Mammographyë§Œ ìˆê³  MRI ì •ë³´ ì—†ìŒ

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{"level": "high/medium/low", "reason": "íŒë‹¨ ì´ìœ ", "relevant_indices": [ê´€ë ¨ ë¬¸ì„œ ë²ˆí˜¸ë“¤]}"""

    user_message = f"""ì§ˆë¬¸: {question}

ê²€ìƒ‰ëœ ë¬¸ì„œë“¤:
{docs_text}

ì´ ë¬¸ì„œë“¤ì´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ í¬í•¨í•˜ê³  ìˆë‚˜ìš”? JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."""

    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_message}
    ]

    payload = {
        "model": model,
        "messages": messages,
        "stream": False,
        "options": {"temperature": 0.1}
    }

    try:
        response = requests.post(url, json=payload, timeout=90)  # deepseek-r1 ì‘ë‹µ ëŒ€ê¸°
        response.raise_for_status()
        result = response.json()
        content = result.get("message", {}).get("content", "")

        # JSON íŒŒì‹± ì‹œë„
        import re
        json_match = re.search(r'\{[^}]+\}', content)
        if json_match:
            parsed = json.loads(json_match.group())
            return {
                "level": parsed.get("level", "medium"),
                "reason": parsed.get("reason", ""),
                "relevant_indices": parsed.get("relevant_indices", [])
            }

        # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’ (medium)
        return {"level": "medium", "reason": "ê²€ì¦ ë¶ˆê°€", "relevant_indices": list(range(1, len(documents)+1))}

    except Exception as e:
        # ì˜¤ë¥˜ì‹œ mediumìœ¼ë¡œ ì²˜ë¦¬
        return {"level": "medium", "reason": f"ê²€ì¦ ì˜¤ë¥˜: {e}", "relevant_indices": list(range(1, len(documents)+1))}


# =============================================================================
# ì‚¬ì´ë“œë°”
# =============================================================================

def render_sidebar():
    """ì‚¬ì´ë“œë°” ë Œë”ë§"""
    with st.sidebar:
        st.markdown("### âš™ï¸ Settings")

        st.markdown("#### Model")
        model = st.selectbox(
            "LLM Model",
            options=["gpt-oss:20b"],
            index=0,
        )

        temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=1.0,
            value=0.3,
            step=0.1,
            help="ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì , ë†’ì„ìˆ˜ë¡ ì°½ì˜ì "
        )

        st.markdown("#### Search")
        top_k = st.slider(
            "ì°¸ê³  ë…¼ë¬¸ ìˆ˜",
            min_value=3,
            max_value=10,
            value=5,
            step=1,
            help="ë‹µë³€ ìƒì„± ì‹œ ì°¸ê³ í•  ë…¼ë¬¸ ìˆ˜"
        )

        st.markdown("#### ğŸ›¡ï¸ ê²€ì¦ ëª¨ë“œ")
        critic_mode = st.checkbox(
            "ì „ë¬¸ê°€ ê²€ì¦ (Critic Agent)",
            value=True,  # ê¸°ë³¸ í™œì„±í™”
            help="AI ì‘ë‹µì„ ë¬¼ë¦¬ ë²•ì¹™/ì¸ìš© ì •í™•ì„± ê¸°ì¤€ìœ¼ë¡œ ìë™ ê²€ì¦ ë° êµì •í•©ë‹ˆë‹¤"
        )

        st.markdown("---")

        if st.button("ğŸ—‘ï¸ Clear Chat", use_container_width=True):
            st.session_state.messages = []
            st.rerun()

        st.markdown("---")
        st.markdown("#### About")
        st.markdown("""
        <div style="font-size: 0.85rem; line-height: 1.6;">
        <b>Sophia AI Alpha</b><br>
        ìœ ë°©ì˜ìƒì˜í•™ ë…¼ë¬¸ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ<br><br>

        âœ… í• ë£¨ì‹œë„¤ì´ì…˜ ìµœì†Œí™”<br>
        âœ… ë…¼ë¬¸ ì¶œì²˜ ëª…ì‹œ<br>
        âœ… BI-RADS ê°€ì´ë“œë¼ì¸ ì°¸ì¡°<br>
        âš–ï¸ <b>Critic Agent ìë™ ê²€ì¦</b>
        </div>
        """, unsafe_allow_html=True)

        return {
            "model": model,
            "temperature": temperature,
            "top_k": top_k,
            "critic_mode": critic_mode,
        }

# =============================================================================
# ë©”ì¸ UI
# =============================================================================

def main():
    """ë©”ì¸ RAG ì±—ë´‡"""

    # ì‚¬ì´ë“œë°”
    options = render_sidebar()

    # í—¤ë”
    st.markdown("""
    <h1 style='font-size: 2.5rem; margin-bottom: 0;'>
        ğŸ’¬ Sophia AI
        <span style='font-size: 0.9rem; color: #888888; font-weight: normal; vertical-align: super;'>Alpha</span>
    </h1>
    """, unsafe_allow_html=True)
    st.caption("ğŸš€ ìœ ë°©ì˜ìƒì˜í•™ ë…¼ë¬¸ ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ (RAG)")

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state["messages"] = [{
            "role": "assistant",
            "content": "ì•ˆë…•í•˜ì„¸ìš”! ìœ ë°©ì˜ìƒì˜í•™ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´, ê´€ë ¨ ë…¼ë¬¸ê³¼ BI-RADS ê°€ì´ë“œë¼ì¸ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\nì˜ˆì‹œ:\n- Mammographyì— ëŒ€í•œ ê¸°ë³¸ ê°œë… ì„¤ëª…\n- DBTì™€ FFDMì˜ ì°¨ì´ì \n- BI-RADS ì¹´í…Œê³ ë¦¬ ì„¤ëª…"
        }]

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

            # ì¶œì²˜ í‘œì‹œ
            if "sources" in msg:
                with st.expander("ğŸ“š ì°¸ê³  ìë£Œ", expanded=False):
                    for i, source in enumerate(msg["sources"], 1):
                        pmid = source.get('pmid', '')
                        guideline_type = get_guideline_type(pmid)
                        is_guideline = source.get("is_birads", False) or guideline_type in ("birads", "acr", "physics", "clinical")

                        if guideline_type == "acr":
                            icon = "ğŸ“—"
                        elif guideline_type == "birads":
                            icon = "ğŸ“˜"
                        elif is_guideline:
                            icon = "ğŸ“™"
                        else:
                            icon = "ğŸ“„"

                        if is_guideline:
                            # ê°€ì´ë“œë¼ì¸ ë¬¸ì„œ - BIRADSë§Œ ì›ë¬¸ ë§í¬ í‘œì‹œ (ì‹¤ì œ full_content ìˆìŒ)
                            if guideline_type == "birads":
                                page_url = get_guideline_page_url(pmid)
                                st.markdown(f"""
                                **{icon} [{i}] {source['title']}**
                                {source['authors']} - {source['journal']} ({source['year']})
                                [ğŸ“˜ ì›ë¬¸ í™•ì¸í•˜ê¸°]({page_url})
                                """)
                            else:
                                # ACR, PHYSICS, CLINICAL - full_content ì—†ìŒ, PMC ì¶œì²˜ í‘œì‹œ
                                journal_info = source.get('journal', '')
                                # PMC ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ ë§í¬ë¡œ ë³€í™˜
                                pmc_ids = re.findall(r'PMC\d+', journal_info)
                                if pmc_ids:
                                    pmc_links = " | ".join([f"[{pmc}](https://www.ncbi.nlm.nih.gov/pmc/articles/{pmc}/)" for pmc in pmc_ids])
                                    st.markdown(f"""
                                    **{icon} [{i}] {source['title']}**
                                    {source['authors']} ({source['year']})
                                    ğŸ“š ì¶œì²˜: {pmc_links}
                                    """)
                                else:
                                    st.markdown(f"""
                                    **{icon} [{i}] {source['title']}**
                                    {source['authors']} - {journal_info} ({source['year']})
                                    """)
                        else:
                            # ì¼ë°˜ ë…¼ë¬¸ì€ PubMed + Google Scholar + PMC ë§í¬
                            links = [f"[PubMed]({source['url']})"]
                            if source.get('google_scholar_url'):
                                links.append(f"[ğŸ” Scholar]({source['google_scholar_url']})")
                            if source.get('pmc_url'):
                                links.append(f"[âœ… PMC]({source['pmc_url']})")

                            st.markdown(f"""
                            **{icon} [{i}] {source['title']}**
                            {source['authors']} - {source['journal']} ({source['year']})
                            {' | '.join(links)}
                            """)

    # ì±„íŒ… ì…ë ¥
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})

        # ì–¸ì–´ ê°ì§€ ë° ë©”ì‹œì§€ ì„¤ì • (ê²€ìƒ‰ ì „ì— ë¯¸ë¦¬ ì„¤ì •)
        is_ko = is_korean(prompt)
        msg = get_messages(is_ko)

        with st.chat_message("user"):
            st.markdown(prompt)

        # ê²€ìƒ‰ ì—”ì§„ìœ¼ë¡œ ê´€ë ¨ ë…¼ë¬¸ ê²€ìƒ‰
        with st.spinner(msg["searching"]):
            try:
                engine = get_search_engine()
                translator = get_query_translator()
                relay_router = get_cached_relay_router()

                # ëŒ€í™”í˜• ì¿¼ë¦¬ ë³´ê°• (ì´ì „ ë§¥ë½ ì°¸ì¡°)
                enhanced_prompt = enhance_query_with_context(
                    current_question=prompt,
                    chat_history=st.session_state.messages[:-1],  # í˜„ì¬ ì§ˆë¬¸ ì œì™¸
                    model=options["model"]
                )
                if enhanced_prompt != prompt:
                    st.caption(f"ğŸ’¬ ëŒ€í™” ë§¥ë½ ë°˜ì˜: `{enhanced_prompt}`")

                # =====================================================
                # RelayRouter: SLM ê¸°ë°˜ ë¹ ë¥¸ ì „ì²˜ë¦¬
                # =====================================================
                with st.spinner("ğŸš€ SLM ë¶„ì„ ì¤‘..."):
                    dispatch_result = relay_router.dispatch(enhanced_prompt)
                    dispatch_result = relay_router.enrich_with_knowledge(dispatch_result)

                # ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” (SLM ë²ˆì—­ ê²°ê³¼ ì‚¬ìš©)
                search_query = dispatch_result.translated_query
                prompt_lower = enhanced_prompt.lower()

                # SLM ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                intent_labels = {
                    QueryIntent.SIMPLE_LOOKUP: "ğŸ“– ë‹¨ìˆœ ì¡°íšŒ",
                    QueryIntent.PHYSICS_CALCULATION: "ğŸ”¬ ë¬¼ë¦¬ ê³„ì‚°",
                    QueryIntent.CLINICAL_GUIDELINE: "ğŸ¥ ì„ìƒ ê°€ì´ë“œë¼ì¸",
                    QueryIntent.COMPLEX_REASONING: "ğŸ§  ë³µí•© ì¶”ë¡ ",
                    QueryIntent.UNKNOWN: "â“ ë¶„ë¥˜ ë¶ˆê°€",
                }
                st.caption(f"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: `{search_query}`")
                st.caption(f"ğŸ“Š ì§ˆë¬¸ ìœ í˜•: {intent_labels.get(dispatch_result.intent, 'ì•Œ ìˆ˜ ì—†ìŒ')} | ì‚¬ìš© ëª¨ë¸: {relay_router.get_model_used(dispatch_result)}")

                # 1. ì¿¼ë¦¬ í™•ì¥: ëª¨ë“  í•œê¸€ ë²ˆì—­ ì¿¼ë¦¬ì— BI-RADS ì¶”ê°€
                search_query_lower = search_query.lower()
                if 'bi-rads' not in search_query_lower and 'birads' not in search_query_lower:
                    if translator.needs_translation(enhanced_prompt):
                        search_query = f"BI-RADS {search_query}"
                        st.caption(f"âœ¨ ì¿¼ë¦¬ í™•ì¥: `{search_query}`")

                # ì´ì¤‘ ê²€ìƒ‰: BI-RADS + ì—°êµ¬ë…¼ë¬¸
                # birads_k=5: Dance ë…¼ë¬¸ ë“± ë¬¼ë¦¬ ì°¸ì¡° ë¬¸ì„œê°€ í¬í•¨ë˜ë„ë¡ í™•ëŒ€
                birads_response, papers_response = engine.search_dual(
                    search_query,
                    birads_k=5,
                    papers_k=5
                )

                if not birads_response.results and not papers_response.results:
                    with st.chat_message("assistant"):
                        error_msg = msg["no_results"]
                        st.markdown(error_msg)
                        st.session_state.messages.append({"role": "assistant", "content": error_msg})
                    st.stop()

                # BI-RADS ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                birads_context_parts = []
                birads_sources = []
                import re as re_module  # ë¡œì»¬ ìŠ¤ì½”í”„ì—ì„œ ì‚¬ìš©

                for i, result in enumerate(birads_response.results, 1):
                    paper = result.paper
                    guideline_label = get_guideline_label(paper.pmid)

                    # PMC ID ì¶”ì¶œ (pmc_id í•„ë“œ ë˜ëŠ” journal í•„ë“œì—ì„œ)
                    pmc_id = getattr(paper, 'pmc_id', None)
                    if not pmc_id and paper.journal:
                        pmc_match = re_module.search(r'(PMC\d+)', paper.journal)
                        if pmc_match:
                            pmc_id = pmc_match.group(1)

                    # PMC ì „ë¬¸ì´ ìˆìœ¼ë©´ fetch, ì—†ìœ¼ë©´ ê¸°ì¡´ ë‚´ìš© ì‚¬ìš©
                    if pmc_id:
                        pmc_url = f"https://pmc.ncbi.nlm.nih.gov/articles/{pmc_id}/"
                        content_text = fetch_pmc_fulltext(pmc_url, max_chars=8000)
                        content_text = f"[PMC ì „ë¬¸ - {pmc_id}]\n{content_text}"
                    else:
                        content_text = getattr(paper, 'full_content', paper.abstract or 'ë‚´ìš© ì—†ìŒ')

                    birads_context_parts.append(f"""
[{i}] {guideline_label}
ì œëª©: {paper.title}
ë‚´ìš©: {content_text}
""")

                    birads_sources.append({
                        "title": paper.title,
                        "authors": paper.author_string or "American College of Radiology",
                        "journal": paper.journal or "ACR BI-RADS Atlas v2025",
                        "year": paper.year or "2025",
                        "pmid": paper.pmid,
                        "pmc_id": pmc_id,  # âš ï¸ Phase 7.1 Fix: ì¶”ì¶œëœ pmc_id ì‚¬ìš© (journalì—ì„œ ì¶”ì¶œí•œ ê°’ í¬í•¨)
                        "is_birads": True,
                        "full_content": getattr(paper, 'full_content', None)
                    })

                birads_context = "\n".join(birads_context_parts) if birads_context_parts else ""

                # ì—°êµ¬ë…¼ë¬¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                papers_context_parts = []
                papers_sources = []

                for i, result in enumerate(papers_response.results, 1):
                    paper = result.paper

                    # full_contentê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš© (Dance ë…¼ë¬¸ ë“± ì§ì ‘ ì²­í‚¹ëœ ë¬¸ì„œ)
                    full_content = getattr(paper, 'full_content', None)
                    pmc_id = getattr(paper, 'pmc_id', None)

                    if full_content:
                        # ì§ì ‘ ì²­í‚¹ëœ ë¬¸ì„œ (Dance ë…¼ë¬¸, ë¬¼ë¦¬í•™ ì°¸ì¡° ë“±)
                        content_text = full_content[:8000] if len(full_content) > 8000 else full_content
                    elif pmc_id:
                        # PMC ì „ë¬¸ fetch
                        pmc_url = f"https://pmc.ncbi.nlm.nih.gov/articles/{pmc_id}/"
                        content_text = fetch_pmc_fulltext(pmc_url, max_chars=6000)
                        content_text = f"[PMC ì „ë¬¸ - {pmc_id}]\n{content_text}"
                    else:
                        content_text = (paper.abstract[:500] + '...' if paper.abstract and len(paper.abstract) > 500 else paper.abstract or 'ì´ˆë¡ ì—†ìŒ')

                    papers_context_parts.append(f"""
[{i}] ğŸ“„ ì—°êµ¬ë…¼ë¬¸
ì œëª©: {paper.title}
ì €ì: {paper.author_string}
ì €ë„: {paper.journal} ({paper.year})
ë‚´ìš©: {content_text}
""")

                    papers_sources.append({
                        "title": paper.title,
                        "authors": paper.author_string or "ì €ì ì •ë³´ ì—†ìŒ",
                        "journal": paper.journal or "ì €ë„ ì •ë³´ ì—†ìŒ",
                        "year": paper.year or "ì—°ë„ ì •ë³´ ì—†ìŒ",
                        "pmid": getattr(paper, 'pmid', ''),
                        "pmc_id": pmc_id,  # âš ï¸ Phase 7.1 Fix: DynamicEvidencePipelineì— ì „ë‹¬í•  pmc_id
                        "url": paper.pubmed_url,
                        "google_scholar_url": paper.google_scholar_url,
                        "pmc_url": paper.pmc_url,  # None if not available
                        "doi_url": paper.doi_url,  # None if not available
                        "is_birads": False
                    })

                papers_context = "\n".join(papers_context_parts) if papers_context_parts else ""

                # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ê²°í•© (ê°€ì´ë“œë¼ì¸ ìš°ì„ , ì—°êµ¬ë…¼ë¬¸ í›„ìˆœìœ„)
                context_parts = []
                if birads_context:
                    context_parts.append("### ğŸ“š ê°€ì´ë“œë¼ì¸ ë¬¸ì„œ\n" + birads_context)
                if papers_context:
                    context_parts.append("\n### ğŸ“„ ê´€ë ¨ ì—°êµ¬ ë…¼ë¬¸\n" + papers_context)

                context = "\n\n".join(context_parts)
                # sourcesëŠ” ê²€ì¦ í›„ì— ì¶”ê°€ë¨ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”)
                sources = []

            except Exception as e:
                with st.chat_message("assistant"):
                    error_msg = f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                    st.markdown(error_msg)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})
                st.stop()

        # ë‹µë³€ ìƒì„± (BI-RADSê°€ ìˆìœ¼ë©´ ê´€ë ¨ì„± ê²€ì¦ í›„ í‘œì‹œ, ì—†ìœ¼ë©´ LLM)
        with st.chat_message("assistant"):
            # RelayRouterì˜ dispatch_resultì—ì„œ KnowledgeManager ì •ë³´ ì‚¬ìš©
            from src.knowledge.manager import get_knowledge_manager
            km = get_knowledge_manager()

            # dispatch_resultì—ì„œ ì§€ì‹ ì •ë³´ ì¶”ì¶œ (ì´ë¯¸ enrich_with_knowledge()ì—ì„œ ì²˜ë¦¬ë¨)
            matched_modules = km.get_relevant_knowledge(prompt)  # ì»¨í…ìŠ¤íŠ¸ìš©
            # Phase 7.18: prompt(query) ì „ë‹¬í•˜ì—¬ ë™ì  ì„¹ì…˜ ë°°ì¹˜
            relevant_knowledge = km.format_for_context(matched_modules, query=prompt) if matched_modules else ""
            has_physics_knowledge = dispatch_result.has_knowledge

            # Knowledge Status Bar í‘œì‹œ
            status_parts = []
            if dispatch_result.knowledge_modules:
                status_parts.append(f"ğŸ“š í‘œì¤€ ì§€ì‹: {', '.join(dispatch_result.knowledge_modules)}")
            if birads_sources:
                status_parts.append(f"ğŸ” ê°€ì´ë“œë¼ì¸: {len(birads_sources)}ê°œ")
            if papers_sources:
                status_parts.append(f"ğŸ“„ ë…¼ë¬¸: {len(papers_sources)}ê°œ")

            if status_parts:
                status_text = " | ".join(status_parts)
                st.info(f"**ê·¼ê±° ìë£Œ í˜„í™©**: {status_text}")

            if birads_sources:
                # BI-RADS ë¬¸ì„œ ê´€ë ¨ì„± ê²€ì¦ (ë¬¼ë¦¬ ì§€ì‹ ìœ ë¬´ë„ ê³ ë ¤)
                with st.spinner(msg["verifying"]):
                    relevance = verify_relevance(
                        question=prompt,
                        documents=birads_sources,
                        model=options["model"],
                        has_physics_knowledge=has_physics_knowledge
                    )

                level = relevance.get("level", "medium")
                reason = relevance.get("reason", "")

                # ê´€ë ¨ì„± ë‚®ì•„ë„ ê²€ìƒ‰ëœ ë‚´ìš©ì´ ìˆìœ¼ë©´ ë¶„ì„ ìˆ˜í–‰ (ë²„ë¦¬ì§€ ì•ŠìŒ)
                if level == "low":
                    st.markdown(f"âš ï¸ ê´€ë ¨ì„±ì´ ë‚®ì„ ìˆ˜ ìˆì§€ë§Œ, ê²€ìƒ‰ëœ ë‚´ìš©ìœ¼ë¡œ ë¶„ì„ì„ ì‹œë„í•©ë‹ˆë‹¤.\n\n_{reason}_")

                if True:  # ë¬´ì¡°ê±´ ë¶„ì„ ìˆ˜í–‰
                    # high ë˜ëŠ” medium - ë¬¸ì„œ í‘œì‹œ
                    relevant_indices = relevance.get("relevant_indices", [])
                    if relevant_indices:
                        filtered_sources = [birads_sources[i-1] for i in relevant_indices if 0 < i <= len(birads_sources)]
                    else:
                        filtered_sources = birads_sources

                    # sourcesë¥¼ filtered_sourcesë¡œ ì—…ë°ì´íŠ¸ (ì €ì¥ìš©)
                    sources = filtered_sources

                    if level == "high":
                        full_response = msg["found_high"]
                    else:  # medium
                        full_response = msg["found_medium"].format(reason=reason)
                    st.markdown(full_response)

                    st.markdown("---")
                    for i, source in enumerate(filtered_sources, 1):
                        pmid = source.get('pmid', '')
                        pmc_id = source.get('pmc_id', '')
                        journal = source.get('journal', '')
                        guideline_type = get_guideline_type(pmid)

                        # journal í•„ë“œì—ì„œ PMC ID ì¶”ì¶œ (PMCë¡œ ì‹œì‘í•˜ëŠ” ê²ƒ)
                        if not pmc_id and journal:
                            pmc_match = re_module.search(r'(PMC\d+)', journal)
                            if pmc_match:
                                pmc_id = pmc_match.group(1)

                        st.markdown(f"### [{i}] {source['title']}")
                        st.markdown(f"_{source['authors']} - {source['journal']} ({source['year']})_")

                        # PMC IDê°€ ìˆìœ¼ë©´ PMC ë§í¬ í‘œì‹œ (ì „ë¬¸ì€ ì´ë¯¸ LLM contextì— í¬í•¨ë¨)
                        if pmc_id:
                            pmc_url = f"https://pmc.ncbi.nlm.nih.gov/articles/{pmc_id}/"
                            st.markdown(f"âœ… **PMC ì „ë¬¸ì´ AI ë¶„ì„ì— ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤** | [ğŸ”— PMC ì›ë¬¸ ë³´ê¸°]({pmc_url})")
                        elif guideline_type != "paper":
                            # ê°€ì´ë“œë¼ì¸ì€ ê¸°ì¡´ ë°©ì‹
                            page_url = get_guideline_page_url(pmid)
                            link_icon = "ğŸ“—" if guideline_type == "acr" else "ğŸ“˜"
                            st.markdown(f"[{link_icon} {msg['view_source']}]({page_url})")
                        else:
                            # ì¼ë°˜ ë…¼ë¬¸ (PMC ì—†ìŒ)
                            pubmed_url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
                            st.markdown(f"[ğŸ“„ PubMed ë³´ê¸°]({pubmed_url})")

                        st.markdown("---")

                    # =====================================================
                    # RelayLLM: ì§€ëŠ¥í˜• ì‘ë‹µ ë¼ìš°íŒ…
                    # =====================================================
                    st.markdown("### ğŸ¤– AI ë¶„ì„")

                    # KnowledgeManagerê°€ ì§ì ‘ ë‹µë³€ ê°€ëŠ¥í•œì§€ í™•ì¸
                    if dispatch_result.knowledge_answer and dispatch_result.intent == QueryIntent.SIMPLE_LOOKUP:
                        # ğŸ“š KnowledgeManager ì§ì ‘ ì‘ë‹µ (LLM í˜¸ì¶œ ìŠ¤í‚µ)
                        st.caption("âš¡ **ê³ ì† ì‘ë‹µ**: ê²€ì¦ëœ í‘œì¤€ ì§€ì‹ì—ì„œ ì§ì ‘ ë‹µë³€")
                        full_response = dispatch_result.knowledge_answer
                        st.markdown(convert_latex_for_streamlit(full_response))

                    elif (dispatch_result.intent in [QueryIntent.PHYSICS_CALCULATION, QueryIntent.COMPLEX_REASONING]
                          or (dispatch_result.intent == QueryIntent.UNKNOWN
                              and any(kw in prompt.lower() for kw in ['ì¦ëª…', 'ìˆ˜ì‹', 'ê³„ì‚°', 'ë„ì¶œ', 'snr', 'cnr', 'mgd', 'pcd', 'eid', 'dqe', 'ë…¼í•˜ì‹œì˜¤', 'ê¸°ìˆ í•˜ì‹œì˜¤']))):
                        # ğŸ”¬ DynamicEvidencePipeline: PMC ì „ë¬¸ + Answering Twice + Evidence Mapping
                        st.caption("ğŸ”¬ **ì •ë°€ ë¶„ì„**: Phase 7 Dynamic Evidence (PMC ì „ë¬¸ + 2ë‹¨ê³„ ê²€ì¦)")

                        # papers ë°ì´í„° êµ¬ì„± (DynamicEvidencePipeline ì…ë ¥ í˜•ì‹)
                        papers_for_pipeline = []
                        for source in filtered_sources:
                            papers_for_pipeline.append({
                                "pmid": source.get("pmid", ""),
                                "pmc_id": source.get("pmc_id"),
                                "title": source.get("title", ""),
                                "abstract": source.get("abstract", source.get("content", ""))[:2000]
                            })

                        # PMC ID ë³´ìœ  ë…¼ë¬¸ ìˆ˜ í‘œì‹œ (ë””ë²„ê¹… í¬í•¨)
                        pmc_count = sum(1 for p in papers_for_pipeline if p.get("pmc_id"))
                        pmc_ids_found = [p.get("pmc_id") for p in papers_for_pipeline if p.get("pmc_id")]

                        if pmc_count > 0:
                            st.info(f"ğŸ“‘ **PMC ì „ë¬¸ ì¸ì¶œ ì˜ˆì •**: {pmc_count}ê°œ ({', '.join(pmc_ids_found[:3])}...)")
                        else:
                            st.warning("âš ï¸ PMC IDê°€ ì—†ì–´ ì´ˆë¡ ê¸°ë°˜ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")

                        import asyncio
                        with st.spinner("ğŸ§  ë‹¨ìˆœí™” ëª¨ë“œ: ì§€ì‹ ê¸°ë°˜ ì§ì ‘ ì¶”ë¡  ì¤‘..."):
                            dynamic_pipeline = get_cached_dynamic_pipeline(_version="7.17")
                            # Phase 7.17: ë‹¨ìˆœí™”ëœ íŒŒì´í”„ë¼ì¸ (ë³µì¡í•œ ë ˆì´ì–´ ëª¨ë‘ ì œê±°)
                            result = asyncio.run(dynamic_pipeline.process_simple_async(
                                question=prompt,
                                papers=papers_for_pipeline,
                                physics_knowledge=relevant_knowledge,
                                max_pmc_fetch=2
                            ))

                        # Phase 7.16: êµ¬ì¡°í™”ëœ ë‹µë³€ ì¶œë ¥ (ë¹ˆ ì‘ë‹µ ê²€ì¦ í¬í•¨)
                        full_response = result.answer
                        if not full_response or not full_response.strip() or "ë¹„ì–´ìˆìŠµë‹ˆë‹¤" in full_response:
                            st.error("âš ï¸ ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
                            st.info("ğŸ’¡ íŒ: ì§ˆë¬¸ì„ ë” ê°„ë‹¨í•˜ê²Œ ë‹¤ì‹œ ì‘ì„±í•˜ê±°ë‚˜, ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")
                        else:
                            # Placeholderë¡œ ë‹µë³€ í‘œì‹œ (ì¬ìƒì„± ì‹œ êµì²´ ê°€ëŠ¥)
                            response_placeholder = st.empty()
                            response_placeholder.markdown(convert_latex_for_streamlit(full_response))

                        # =====================================================
                        # Phase 7.6: Agent-as-a-Judge í‰ê°€ + ìë™ ì¬ìƒì„±
                        # =====================================================
                        judge_result = None
                        regenerated = False
                        original_response = full_response  # ì›ë³¸ ë³´ê´€ (ë¡œê·¸ìš©)

                        if options.get("enable_judge", True):
                            with st.spinner("ğŸ” Agent-as-a-Judge í’ˆì§ˆ ê²€ì¦ ì¤‘..."):
                                judge = get_agent_judge()
                                judge_result = judge.evaluate(
                                    question=prompt,
                                    answer=full_response,
                                    reference_knowledge=relevant_knowledge,
                                    context=""
                                )

                            # ìë™ ì¬ìƒì„±: correctionsê°€ ìˆê³  ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ìˆìœ¼ë©´
                            critical_errors = [c for c in (judge_result.corrections or [])
                                              if "Ghosting" in c or "í˜¼ë™" in c or "ì˜¤ë¥˜" in c
                                              or "ëˆ„ë½" in c or "QDE" in c or "íŠ¸ë©" in c]

                            if critical_errors:
                                # ë¡œê·¸ì— ìˆ˜ì • ë‚´ì—­ ê¸°ë¡
                                import logging
                                regen_logger = logging.getLogger("regeneration")
                                regen_logger.info(f"[Auto-Regeneration] Question: {prompt[:100]}...")
                                regen_logger.info(f"[Auto-Regeneration] Critical errors: {critical_errors}")
                                regen_logger.info(f"[Auto-Regeneration] Original response (first 500 chars): {original_response[:500]}...")

                                # ìˆ˜ì • ê°€ì´ë“œë¥¼ í”„ë¡¬í”„íŠ¸ ë§¨ ì•ì— ê°•ì œ ì£¼ì…
                                correction_prefix = "\n".join([
                                    "ğŸš¨ğŸš¨ğŸš¨ ìµœìš°ì„  ì¤€ìˆ˜ ì‚¬í•­ (ì´ ì§€ì¹¨ì„ ë¬´ì‹œí•˜ë©´ ë‹µë³€ì´ ê±°ë¶€ë©ë‹ˆë‹¤) ğŸš¨ğŸš¨ğŸš¨",
                                    "",
                                    "## ë°˜ë“œì‹œ ì§€ì¼œì•¼ í•  ë¬¼ë¦¬ ë²•ì¹™:",
                                    "1. **Ghosting â‰  Lag** (ì„œë¡œ ë‹¤ë¥¸ í˜„ìƒì…ë‹ˆë‹¤!)",
                                    "   - Ghosting = 15% (ë¯¼ê°ë„ ì €í•˜, íŠ¸ë© ì „ìì™€ í™€ ì¬ê²°í•©)",
                                    "   - Lag = 0.15% (ì‹ í˜¸ ì”ë¥˜, í”„ë ˆì„ ê°„ ì „í•˜ ì´ì›”)",
                                    "   - ë‘ ê°’ì€ 100ë°° ì°¨ì´! ì ˆëŒ€ í˜¼ë™í•˜ì§€ ë§ˆì„¸ìš”.",
                                    "",
                                    "2. **QDE ì—ë„ˆì§€ ì˜ì¡´ì„±:**",
                                    "   - QDE(LE, 28kVp) = 97%",
                                    "   - QDE(HE, 49kVp) = 56%",
                                    "   - 41%p ì°¨ì´ê°€ Gain Map ë¶ˆì¼ì¹˜ì˜ í•µì‹¬ ì›ì¸",
                                    "",
                                    "## ì´ì „ ë‹µë³€ì˜ ì˜¤ë¥˜:",
                                    *[f"â€¢ {c}" for c in critical_errors],
                                    "",
                                    "ìœ„ ì˜¤ë¥˜ë¥¼ ë°˜ë“œì‹œ ìˆ˜ì •í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.",
                                    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
                                    ""
                                ])

                                # ìˆ˜ì • ê°€ì´ë“œë¥¼ ì§ˆë¬¸ ì•ì— ë°°ì¹˜
                                corrected_prompt = correction_prefix + "\n\nì§ˆë¬¸: " + prompt

                                with st.spinner("ğŸ”„ ë‹µë³€ í’ˆì§ˆ ê°œì„  ì¤‘..."):
                                    result_v2 = asyncio.run(dynamic_pipeline.process_simple_async(
                                        question=corrected_prompt,
                                        papers=papers_for_pipeline,
                                        physics_knowledge=relevant_knowledge,
                                        max_pmc_fetch=2
                                    ))

                                # ì¬ìƒì„± ì„±ê³µ ì‹œ: ì´ì „ ë‹µë³€ êµì²´ (ì‚­ì œ í›„ ìƒˆë¡œ í‘œì‹œ)
                                if result_v2.answer and result_v2.answer.strip():
                                    full_response = result_v2.answer
                                    regenerated = True

                                    # ì´ì „ ë‹µë³€ ì‚­ì œí•˜ê³  ìƒˆ ë‹µë³€ìœ¼ë¡œ êµì²´
                                    response_placeholder.empty()
                                    response_placeholder.markdown(convert_latex_for_streamlit(full_response))

                                    # ë¡œê·¸ì— ìˆ˜ì • ì™„ë£Œ ê¸°ë¡
                                    regen_logger.info(f"[Auto-Regeneration] Corrected response (first 500 chars): {full_response[:500]}...")
                                    regen_logger.info(f"[Auto-Regeneration] Regeneration successful")

                                    # ì¬ìƒì„±ëœ ë‹µë³€ ì¬í‰ê°€
                                    with st.spinner("ğŸ” ì¬í‰ê°€ ì¤‘..."):
                                        judge_result = judge.evaluate(
                                            question=prompt,
                                            answer=full_response,
                                            reference_knowledge=relevant_knowledge,
                                            context=""
                                        )

                        # Phase 7.1 ìƒíƒœ ì •ë³´ í‘œì‹œ
                        col1, col2, col3, col4 = st.columns(4)
                        with col1:
                            if result.used_fulltext:
                                st.success(f"âœ… PMC: {result.enriched_context.fetched_count}ê°œ ({result.enriched_context.total_chars:,}ì)")
                            else:
                                st.info(f"ğŸ“„ ì´ˆë¡ ê¸°ë°˜ ({result.enriched_context.total_chars:,}ì)")
                        with col2:
                            if regenerated:
                                st.success("ğŸ”„ ìë™ ìˆ˜ì •ë¨")
                            elif result.used_summarizer:
                                st.success("ğŸ”¬ SLM ìš”ì•½ ì™„ë£Œ")
                            else:
                                st.info("ğŸ“ ì›ë³¸ ì‚¬ìš©")
                        with col3:
                            # Agent-as-a-Judge ê²°ê³¼ ì‚¬ìš© (ìˆìœ¼ë©´)
                            if judge_result:
                                verdict_badges = {
                                    JudgeVerdict.APPROVED: "ğŸ†",
                                    JudgeVerdict.REVISION_REQUIRED: "âš ï¸",
                                    JudgeVerdict.REJECTED: "âŒ",
                                }
                                st.metric("í’ˆì§ˆ", f"{judge_result.total_score:.0f}",
                                         delta=f"{verdict_badges.get(judge_result.verdict, '')}")
                            else:
                                st.metric("í’ˆì§ˆ", f"{result.judge_result.total_score:.0f}",
                                         delta="ğŸ†" if result.judge_result.total_score >= 70 else "âš ï¸")
                        with col4:
                            if judge_result:
                                issues_count = len(judge_result.issues_found) if judge_result.issues_found else 0
                                st.metric("ë°œê²¬ ì´ìŠˆ", f"{issues_count}ê°œ")
                            else:
                                st.metric("ê²€ì¦",
                                         f"{result.evidence_report.verified_claims}/{result.evidence_report.total_claims}")

                        # ìˆ˜ì • ê°€ì´ë“œ í‘œì‹œ (ì¬ìƒì„± í›„ì—ë„ ì´ìŠˆê°€ ë‚¨ì•„ìˆìœ¼ë©´)
                        # Phase 7.22: issues_found ë˜ëŠ” corrections ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ í‘œì‹œ
                        has_issues = judge_result and (judge_result.corrections or judge_result.issues_found)
                        if has_issues and not regenerated:
                            with st.expander("ğŸ”§ ìˆ˜ì • ê°€ì´ë“œ (í´ë¦­í•˜ì—¬ í™•ì¸)", expanded=True):
                                # correctionsê°€ ìˆìœ¼ë©´ corrections í‘œì‹œ
                                if judge_result.corrections:
                                    for corr in judge_result.corrections[:5]:
                                        st.warning(f"âš ï¸ {corr}")
                                # correctionsê°€ ì—†ê³  issues_foundë§Œ ìˆìœ¼ë©´ issues í‘œì‹œ
                                elif judge_result.issues_found:
                                    for issue in judge_result.issues_found[:5]:
                                        st.warning(f"âš ï¸ {issue}")

                        # ì‹¬ê°í•œ ë¬¸ì œ ê²½ê³ 
                        if judge_result and judge_result.verdict == JudgeVerdict.REJECTED:
                            st.error("âš ï¸ ë‹µë³€ì— ì‹¬ê°í•œ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ì • ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.")

                    else:
                        # ğŸ§  ì¼ë°˜ LLM ì‹¬ì¸µ ì¶”ë¡ 
                        st.caption(f"ğŸ§  **ì‹¬ì¸µ ë¶„ì„**: {relay_router.get_model_used(dispatch_result)}")

                        # Phase 7.18: ê²€ì¦ëœ ë¬¼ë¦¬ ì§€ì‹ê³¼ ê²€ìƒ‰ëœ ë…¼ë¬¸ ë¶„ë¦¬
                        # ê²€ìƒ‰ëœ ë…¼ë¬¸ë§Œ context_partsì— ì¶”ê°€ (ë¬¼ë¦¬ ì§€ì‹ì€ ë³„ë„ ì „ë‹¬)
                        context_parts = []
                        for i, source in enumerate(filtered_sources, 1):
                            context_parts.append(f"[ë¬¸ì„œ {i}] {source['title']}\nì €ì: {source['authors']}\në‚´ìš©: {source.get('abstract', source.get('content', ''))[:2000]}")

                        context = "\n\n".join(context_parts)

                        # LLM ë‹µë³€ ìŠ¤íŠ¸ë¦¬ë°
                        response_placeholder = st.empty()
                        full_response = ""

                        for chunk in call_llm_with_context(
                            question=prompt,
                            context=context,
                            model=options["model"],
                            temperature=options["temperature"],
                            has_guidelines=True,
                            physics_knowledge=relevant_knowledge  # Phase 7.18: ë³„ë„ ì „ë‹¬
                        ):
                            full_response += chunk
                            response_placeholder.markdown(full_response + "â–Œ")

                        response_placeholder.markdown(convert_latex_for_streamlit(full_response))

                        # =====================================================
                        # Phase 7.6: Agent-as-a-Judge í‰ê°€
                        # =====================================================
                        if options.get("enable_judge", True):  # ê¸°ë³¸ í™œì„±í™”
                            with st.spinner("ğŸ” Agent-as-a-Judge í’ˆì§ˆ ê²€ì¦ ì¤‘..."):
                                judge = get_agent_judge()
                                judge_result = judge.evaluate(
                                    question=prompt,
                                    answer=full_response,
                                    reference_knowledge=relevant_knowledge,
                                    context=context
                                )

                            # í’ˆì§ˆ ì§€í‘œ í‘œì‹œ
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                verdict_badges = {
                                    JudgeVerdict.APPROVED: ("âœ…", "success"),
                                    JudgeVerdict.REVISION_REQUIRED: ("âš ï¸", "warning"),
                                    JudgeVerdict.REJECTED: ("âŒ", "error"),
                                }
                                badge, status = verdict_badges.get(judge_result.verdict, ("â“", "info"))
                                st.metric("í’ˆì§ˆ ì ìˆ˜", f"{judge_result.total_score:.0f}/100", delta=badge)
                            with col2:
                                st.metric("ë„êµ¬ ê²€ì¦", f"{judge_result.tool_score:.0f}" if hasattr(judge_result, 'tool_score') else "N/A")
                            with col3:
                                issues_count = len(judge_result.issues_found) if judge_result.issues_found else 0
                                st.metric("ë°œê²¬ ì´ìŠˆ", f"{issues_count}ê°œ")

                            # ìˆ˜ì • ê°€ì´ë“œ í‘œì‹œ (ìˆëŠ” ê²½ìš°)
                            if hasattr(judge_result, 'corrections') and judge_result.corrections:
                                with st.expander("ğŸ”§ ìˆ˜ì • ê°€ì´ë“œ (í´ë¦­í•˜ì—¬ í™•ì¸)", expanded=False):
                                    for corr in judge_result.corrections[:5]:
                                        st.markdown(f"- âœ“ {corr}")

                            # ì‹¬ê°í•œ ë¬¸ì œ ê²½ê³ 
                            if judge_result.verdict == JudgeVerdict.REJECTED:
                                st.error("âš ï¸ ë‹µë³€ì— ì‹¬ê°í•œ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ìˆ˜ì • ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì—¬ ì¬ì§ˆë¬¸ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")

            else:
                # BI-RADS ì—†ìŒ - ê·¼ê±° ìë£Œ ì•ˆë‚´ë§Œ í‘œì‹œ
                full_response = "ê°€ì´ë“œë¼ì¸ì— ì§ì ‘ì ì¸ ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤. ì•„ë˜ ì—°êµ¬ ë…¼ë¬¸ì„ ì°¸ê³ í•˜ì„¸ìš”."
                st.markdown(full_response)

            if papers_sources:
                # ë…¼ë¬¸ ê´€ë ¨ì„± ê²€ì¦
                with st.spinner(msg["verifying"]):
                    paper_relevance = verify_relevance(
                        question=prompt,
                        documents=papers_sources,
                        model=options["model"]
                    )

                paper_level = paper_relevance.get("level", "medium")
                paper_reason = paper_relevance.get("reason", "")
                paper_indices = paper_relevance.get("relevant_indices", [])

                if paper_level != "low":
                    # ê´€ë ¨ ìˆëŠ” ë…¼ë¬¸ë§Œ í•„í„°ë§
                    if paper_indices:
                        filtered_papers = [papers_sources[i-1] for i in paper_indices if 0 < i <= len(papers_sources)]
                    else:
                        filtered_papers = papers_sources

                    if filtered_papers:
                        # sourcesì— ê´€ë ¨ ë…¼ë¬¸ ì¶”ê°€
                        sources = sources + filtered_papers

                        if paper_level == "high":
                            expander_title = msg["papers_high"]
                        else:  # medium
                            expander_title = msg["papers_medium"]

                        pubmed_text = "PubMed" if not is_ko else "PubMed"
                        scholar_text = "Scholar" if not is_ko else "Scholar"
                        pmc_text = "PMC (ë¬´ë£Œ ì „ë¬¸)" if is_ko else "PMC (Free)"

                        with st.expander(expander_title, expanded=False):
                            if paper_level == "medium":
                                st.caption(f"_{paper_reason}_")
                            for i, source in enumerate(filtered_papers, 1):
                                # ë§í¬ êµ¬ì„±: PubMed + Google Scholar + PMC(ìˆìœ¼ë©´)
                                links = [f"[{pubmed_text}]({source['url']})"]
                                links.append(f"[ğŸ” {scholar_text}]({source.get('google_scholar_url', '')})")
                                if source.get('pmc_url'):
                                    links.append(f"[âœ… {pmc_text}]({source['pmc_url']})")

                                st.markdown(f"""
                                **ğŸ“„ [{i}] {source['title']}**
                                {source['authors']} - {source['journal']} ({source['year']})
                                {' | '.join(links)}
                                """)

        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥ (ì¶œì²˜ í¬í•¨, ë”¥ì¹´í”¼ë¡œ ì €ì¥)
        st.session_state.messages.append({
            "role": "assistant",
            "content": full_response,
            "sources": copy.deepcopy(sources)  # ë”¥ì¹´í”¼ë¡œ ì°¸ì¡° ì™„ì „ ë¶„ë¦¬
        })


if __name__ == "__main__":
    main()
