"""
Sophia AI Alpha: RAG-based Medical AI Assistant
===============================================
ë…¼ë¬¸ ê²€ìƒ‰ ê¸°ë°˜ ì˜ë£Œ AI ì–´ì‹œìŠ¤í„´íŠ¸ (í• ë£¨ì‹œë„¤ì´ì…˜ ìµœì†Œí™”)
"""

import os
import sys
from pathlib import Path
import streamlit as st
import requests
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.search.engine import SearchEngine
from src.search.query_translator import get_translator

# =============================================================================
# í˜ì´ì§€ ì„¤ì •
# =============================================================================

st.set_page_config(
    page_title="Sophia AI Alpha",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded",
)

# =============================================================================
# ê²€ìƒ‰ ì—”ì§„ & LLM ì´ˆê¸°í™”
# =============================================================================

@st.cache_resource(ttl=3600)  # 1ì‹œê°„ë§ˆë‹¤ ìºì‹œ ê°±ì‹ 
def get_search_engine():
    """ê²€ìƒ‰ ì—”ì§„ ì‹±ê¸€í†¤ (BI-RADS í¬í•¨)"""
    return SearchEngine(
        db_path=Path("data/index"),
        parser_mode="smart",
        ollama_url="http://localhost:11434",
        llm_model="qwen2.5:14b",
        use_reranker=True,
    )

@st.cache_resource(ttl=3600)
def get_query_translator():
    """ì¿¼ë¦¬ ë²ˆì—­ê¸° ì‹±ê¸€í†¤"""
    return get_translator(
        ollama_url="http://localhost:11434",
        model="qwen2.5:14b"
    )

def call_llm_with_context(question: str, context: str, model="qwen2.5:14b", temperature=0.7):
    """
    RAG: ê²€ìƒ‰ëœ ë…¼ë¬¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLM ë‹µë³€ ìƒì„±

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        context: ê²€ìƒ‰ëœ ë…¼ë¬¸ ë‚´ìš©
        model: LLM ëª¨ë¸ëª…
        temperature: ì˜¨ë„ ì„¤ì •

    Returns:
        LLM ë‹µë³€ (generator)
    """
    url = "http://localhost:11434/api/chat"

    system_message = """ë‹¹ì‹ ì€ ìœ ë°©ì˜ìƒì˜í•™ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ì¤‘ìš”í•œ ê·œì¹™:**
1. ì œê³µëœ ë…¼ë¬¸ê³¼ BI-RADS ê°€ì´ë“œë¼ì¸ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
2. ì¶”ì¸¡í•˜ì§€ ë§ê³ , ì œê³µëœ ì •ë³´ì— ì—†ìœ¼ë©´ "ì œê³µëœ ìë£Œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”
3. ë‹µë³€ ì‹œ ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš” (ì˜ˆ: [1], [2] í˜•ì‹)
4. ì˜í•™ì  ì¡°ì–¸ì´ ì•„ë‹Œ ì—°êµ¬ ì •ë³´ ì œê³µì„ì„ ëª…í™•íˆ í•˜ì„¸ìš”
5. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”"""

    user_message = f"""ë‹¤ìŒ ë…¼ë¬¸ë“¤ê³¼ ê°€ì´ë“œë¼ì¸ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

**ì°¸ê³  ìë£Œ:**
{context}

**ì§ˆë¬¸:** {question}

**ë‹µë³€ í˜•ì‹:**
- ì œê³µëœ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€
- ì¶œì²˜ ë²ˆí˜¸ [1], [2] ë“±ìœ¼ë¡œ ëª…ì‹œ
- ì˜í•™ì  ì¡°ì–¸ì´ ì•„ë‹Œ ì—°êµ¬ ì •ë³´ì„ì„ ëª…í™•íˆ í‘œì‹œ"""

    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_message}
    ]

    payload = {
        "model": model,
        "messages": messages,
        "stream": True,
        "options": {
            "temperature": temperature,
        }
    }

    try:
        response = requests.post(url, json=payload, stream=True, timeout=180)
        response.raise_for_status()

        for line in response.iter_lines():
            if line:
                chunk = json.loads(line)
                if "message" in chunk and "content" in chunk["message"]:
                    yield chunk["message"]["content"]
    except requests.exceptions.RequestException as e:
        yield f"âš ï¸ LLM ì—°ê²° ì˜¤ë¥˜: {str(e)}"

# =============================================================================
# ì‚¬ì´ë“œë°”
# =============================================================================

def render_sidebar():
    """ì‚¬ì´ë“œë°” ë Œë”ë§"""
    with st.sidebar:
        st.markdown("### âš™ï¸ Settings")

        st.markdown("#### Model")
        model = st.selectbox(
            "LLM Model",
            options=["qwen2.5:14b"],
            index=0,
        )

        temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=1.0,
            value=0.3,
            step=0.1,
            help="ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì , ë†’ì„ìˆ˜ë¡ ì°½ì˜ì "
        )

        st.markdown("#### Search")
        top_k = st.slider(
            "ì°¸ê³  ë…¼ë¬¸ ìˆ˜",
            min_value=3,
            max_value=10,
            value=5,
            step=1,
            help="ë‹µë³€ ìƒì„± ì‹œ ì°¸ê³ í•  ë…¼ë¬¸ ìˆ˜"
        )

        st.markdown("---")

        if st.button("ğŸ—‘ï¸ Clear Chat", use_container_width=True):
            st.session_state.messages = []
            st.rerun()

        st.markdown("---")
        st.markdown("#### About")
        st.markdown("""
        <div style="font-size: 0.85rem; line-height: 1.6;">
        <b>Sophia AI Alpha</b><br>
        ìœ ë°©ì˜ìƒì˜í•™ ë…¼ë¬¸ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ<br><br>

        âœ… í• ë£¨ì‹œë„¤ì´ì…˜ ìµœì†Œí™”<br>
        âœ… ë…¼ë¬¸ ì¶œì²˜ ëª…ì‹œ<br>
        âœ… BI-RADS ê°€ì´ë“œë¼ì¸ ì°¸ì¡°
        </div>
        """, unsafe_allow_html=True)

        return {
            "model": model,
            "temperature": temperature,
            "top_k": top_k,
        }

# =============================================================================
# ë©”ì¸ UI
# =============================================================================

def main():
    """ë©”ì¸ RAG ì±—ë´‡"""

    # ì‚¬ì´ë“œë°”
    options = render_sidebar()

    # í—¤ë”
    st.markdown("""
    <h1 style='font-size: 2.5rem; margin-bottom: 0;'>
        ğŸ’¬ Sophia AI
        <span style='font-size: 0.9rem; color: #888888; font-weight: normal; vertical-align: super;'>Alpha</span>
    </h1>
    """, unsafe_allow_html=True)
    st.caption("ğŸš€ ìœ ë°©ì˜ìƒì˜í•™ ë…¼ë¬¸ ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ (RAG)")

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state["messages"] = [{
            "role": "assistant",
            "content": "ì•ˆë…•í•˜ì„¸ìš”! ìœ ë°©ì˜ìƒì˜í•™ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´, ê´€ë ¨ ë…¼ë¬¸ê³¼ BI-RADS ê°€ì´ë“œë¼ì¸ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\nì˜ˆì‹œ:\n- Mammographyì— ëŒ€í•œ ê¸°ë³¸ ê°œë… ì„¤ëª…\n- DBTì™€ FFDMì˜ ì°¨ì´ì \n- BI-RADS ì¹´í…Œê³ ë¦¬ ì„¤ëª…"
        }]

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

            # ì¶œì²˜ í‘œì‹œ
            if "sources" in msg:
                with st.expander("ğŸ“š ì°¸ê³  ìë£Œ", expanded=False):
                    for i, source in enumerate(msg["sources"], 1):
                        icon = "ğŸ“˜" if source.get("is_birads", False) else "ğŸ“„"

                        if source.get("is_birads", False):
                            # BI-RADS ë¬¸ì„œëŠ” ì›ë¬¸ ë§í¬ ì œê³µ
                            st.markdown(f"**{icon} [{i}] {source['title']}**")
                            st.markdown(f"{source['authors']} - {source['journal']} ({source['year']})")

                            # ì›ë¬¸ ë³´ê¸° ë§í¬ (Streamlit í˜ì´ì§€ë¡œ ì´ë™)
                            st.markdown(
                                f"ğŸ’¡ ì „ë¬¸ ë³´ê¸°: ì¢Œì¸¡ ì‚¬ì´ë“œë°” 'ğŸ“˜ BI-RADS ê°€ì´ë“œë¼ì¸' í˜ì´ì§€ì—ì„œ í™•ì¸ "
                                f"| ê¸¸ì´: {len(source.get('full_content', '')):,}ì"
                            )

                            # ê°„ë‹¨íˆ ìš”ì•½ë§Œ í‘œì‹œ
                            if source.get("full_content"):
                                preview = source["full_content"][:300] + "..." if len(source["full_content"]) > 300 else source["full_content"]
                                st.caption(f"ğŸ’¡ ë¯¸ë¦¬ë³´ê¸°: {preview}")
                        else:
                            # ì¼ë°˜ ë…¼ë¬¸ì€ PubMed ë§í¬
                            st.markdown(f"""
                            **{icon} [{i}] {source['title']}**
                            {source['authors']} - {source['journal']} ({source['year']})
                            [PubMed ë³´ê¸°]({source['url']})
                            """)

    # ì±„íŒ… ì…ë ¥
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})

        with st.chat_message("user"):
            st.markdown(prompt)

        # ê²€ìƒ‰ ì—”ì§„ìœ¼ë¡œ ê´€ë ¨ ë…¼ë¬¸ ê²€ìƒ‰
        with st.spinner("ğŸ“š ê´€ë ¨ ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘..."):
            try:
                engine = get_search_engine()
                translator = get_query_translator()

                # ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”
                search_query = prompt
                prompt_lower = prompt.lower()

                # 0. LLM ê¸°ë°˜ ì¿¼ë¦¬ ë²ˆì—­ (í•œê¸€ â†’ ì˜ë¬¸ ì˜í•™ í‚¤ì›Œë“œ)
                if translator.needs_translation(prompt):
                    with st.spinner("ğŸ”„ ì¿¼ë¦¬ ìµœì í™” ì¤‘..."):
                        translated_query = translator.translate(prompt)
                        if translated_query != prompt:
                            search_query = translated_query
                            # ë””ë²„ê¹…ìš©: ë²ˆì—­ ê²°ê³¼ í‘œì‹œ
                            st.caption(f"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: `{translated_query}`")

                # 1. BI-RADS ì¹´í…Œê³ ë¦¬ ì§ˆë¬¸ ì²˜ë¦¬ (ë²ˆì—­ í›„ì—ë„ í™•ì¸)
                search_query_lower = search_query.lower()
                is_birads_concept = (
                    'bi-rads' in search_query_lower or 'birads' in search_query_lower or
                    'ì¹´í…Œê³ ë¦¬' in prompt_lower or 'category' in search_query_lower
                ) and any(keyword in prompt_lower or keyword in search_query_lower
                         for keyword in ['ê¸°ë³¸', 'ê°œë…', 'ì •ì˜', 'ì„¤ëª…', 'basic', 'concept', 'definition', 'ë¬´ì—‡', 'what'])

                if is_birads_concept and 'bi-rads' not in search_query_lower:
                    search_query = f"BI-RADS {search_query}"

                # ì´ì¤‘ ê²€ìƒ‰: BI-RADS + ì—°êµ¬ë…¼ë¬¸
                birads_response, papers_response = engine.search_dual(
                    search_query,
                    birads_k=3,
                    papers_k=5
                )

                if not birads_response.results and not papers_response.results:
                    with st.chat_message("assistant"):
                        error_msg = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."
                        st.markdown(error_msg)
                        st.session_state.messages.append({"role": "assistant", "content": error_msg})
                    st.stop()

                # BI-RADS ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                birads_context_parts = []
                birads_sources = []

                for i, result in enumerate(birads_response.results, 1):
                    paper = result.paper
                    content_text = getattr(paper, 'full_content', paper.abstract or 'ë‚´ìš© ì—†ìŒ')

                    birads_context_parts.append(f"""
[{i}] ğŸ“˜ BI-RADS ê°€ì´ë“œë¼ì¸
ì œëª©: {paper.title}
ë‚´ìš©: {content_text}
""")

                    birads_sources.append({
                        "title": paper.title,
                        "authors": paper.author_string or "American College of Radiology",
                        "journal": paper.journal or "ACR BI-RADS Atlas v2025",
                        "year": paper.year or "2025",
                        "is_birads": True,
                        "full_content": getattr(paper, 'full_content', None)
                    })

                birads_context = "\n".join(birads_context_parts) if birads_context_parts else ""

                # ì—°êµ¬ë…¼ë¬¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                papers_context_parts = []
                papers_sources = []

                for i, result in enumerate(papers_response.results, 1):
                    paper = result.paper
                    content_text = (paper.abstract[:500] + '...' if paper.abstract and len(paper.abstract) > 500 else paper.abstract or 'ì´ˆë¡ ì—†ìŒ')

                    papers_context_parts.append(f"""
[{i}] ğŸ“„ ì—°êµ¬ë…¼ë¬¸
ì œëª©: {paper.title}
ì €ì: {paper.author_string}
ì €ë„: {paper.journal} ({paper.year})
ë‚´ìš©: {content_text}
""")

                    papers_sources.append({
                        "title": paper.title,
                        "authors": paper.author_string or "ì €ì ì •ë³´ ì—†ìŒ",
                        "journal": paper.journal or "ì €ë„ ì •ë³´ ì—†ìŒ",
                        "year": paper.year or "ì—°ë„ ì •ë³´ ì—†ìŒ",
                        "url": paper.pubmed_url,
                        "is_birads": False
                    })

                papers_context = "\n".join(papers_context_parts) if papers_context_parts else ""

                # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ê²°í•© (BI-RADS ìš°ì„ , ì—°êµ¬ë…¼ë¬¸ í›„ìˆœìœ„)
                context_parts = []
                if birads_context:
                    context_parts.append("### ğŸ“˜ BI-RADS ê°€ì´ë“œë¼ì¸\n" + birads_context)
                if papers_context:
                    context_parts.append("\n### ğŸ“„ ê´€ë ¨ ì—°êµ¬ ë…¼ë¬¸\n" + papers_context)

                context = "\n\n".join(context_parts)
                sources = birads_sources + papers_sources

            except Exception as e:
                with st.chat_message("assistant"):
                    error_msg = f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                    st.markdown(error_msg)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})
                st.stop()

        # LLMìœ¼ë¡œ ë‹µë³€ ìƒì„± (RAG)
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""

            # ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€
            for chunk in call_llm_with_context(
                question=prompt,
                context=context,
                model=options["model"],
                temperature=options["temperature"]
            ):
                full_response += chunk
                message_placeholder.markdown(full_response + "â–Œ")

            message_placeholder.markdown(full_response)

            # ì¶œì²˜ í‘œì‹œ (BI-RADSì™€ ì—°êµ¬ë…¼ë¬¸ ë¶„ë¦¬)
            if birads_sources:
                with st.expander("ğŸ“˜ BI-RADS ê°€ì´ë“œë¼ì¸", expanded=False):
                    for i, source in enumerate(birads_sources, 1):
                        st.markdown(f"**ğŸ“˜ [{i}] {source['title']}**")
                        st.markdown(f"{source['authors']} - {source['journal']} ({source['year']})")

                        # ì›ë¬¸ ë³´ê¸° ë§í¬
                        st.markdown(
                            f"ğŸ’¡ ì „ë¬¸ ë³´ê¸°: ì¢Œì¸¡ ì‚¬ì´ë“œë°” 'ğŸ“˜ BI-RADS ê°€ì´ë“œë¼ì¸' í˜ì´ì§€ì—ì„œ í™•ì¸ "
                            f"| ê¸¸ì´: {len(source.get('full_content', '')):,}ì"
                        )

                        # ë¯¸ë¦¬ë³´ê¸°
                        if source.get("full_content"):
                            preview = source["full_content"][:300] + "..." if len(source["full_content"]) > 300 else source["full_content"]
                            st.caption(f"ğŸ’¡ ë¯¸ë¦¬ë³´ê¸°: {preview}")
                        st.markdown("---")

            if papers_sources:
                with st.expander("ğŸ“„ ê´€ë ¨ ì—°êµ¬ ë…¼ë¬¸", expanded=False):
                    for i, source in enumerate(papers_sources, 1):
                        st.markdown(f"""
                        **ğŸ“„ [{i}] {source['title']}**
                        {source['authors']} - {source['journal']} ({source['year']})
                        [PubMed ë³´ê¸°]({source['url']})
                        """)

        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥ (ì¶œì²˜ í¬í•¨)
        st.session_state.messages.append({
            "role": "assistant",
            "content": full_response,
            "sources": sources
        })


if __name__ == "__main__":
    main()
