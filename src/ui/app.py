"""
Sophia AI Alpha: RAG-based Medical AI Assistant
===============================================
ë…¼ë¬¸ ê²€ìƒ‰ ê¸°ë°˜ ì˜ë£Œ AI ì–´ì‹œìŠ¤í„´íŠ¸ (í• ë£¨ì‹œë„¤ì´ì…˜ ìµœì†Œí™”)
"""

import os
import sys
import copy
from pathlib import Path
import streamlit as st
import requests
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.search.engine import SearchEngine
from src.search.query_translator import get_translator

# =============================================================================
# í˜ì´ì§€ ì„¤ì •
# =============================================================================

st.set_page_config(
    page_title="Sophia AI Alpha",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="expanded",
)

# =============================================================================
# ê²€ìƒ‰ ì—”ì§„ & LLM ì´ˆê¸°í™”
# =============================================================================

@st.cache_resource(ttl=3600)  # 1ì‹œê°„ë§ˆë‹¤ ìºì‹œ ê°±ì‹ 
def get_search_engine():
    """ê²€ìƒ‰ ì—”ì§„ ì‹±ê¸€í†¤ (BI-RADS í¬í•¨)"""
    return SearchEngine(
        db_path=Path("data/index"),
        parser_mode="smart",
        ollama_url="http://localhost:11434",
        llm_model="qwen2.5:14b",
        use_reranker=True,
    )

@st.cache_resource(ttl=3600)
def get_query_translator():
    """ì¿¼ë¦¬ ë²ˆì—­ê¸° ì‹±ê¸€í†¤"""
    return get_translator(
        ollama_url="http://localhost:11434",
        model="qwen2.5:14b"
    )

def get_birads_nav_params(pmid: str) -> dict:
    """
    PMIDë¡œë¶€í„° BI-RADS ê°€ì´ë“œë¼ì¸ í˜ì´ì§€ ë„¤ë¹„ê²Œì´ì…˜ íŒŒë¼ë¯¸í„° ìƒì„±

    ì˜ˆ: BIRADS_2025_SECTION_IV_A_CHUNK_MARGIN
    â†’ {"modality": "mammography", "section": "BIRADS_2025_SECTION_IV", "sub": "BIRADS_2025_SECTION_IV_A", "chunk": "MARGIN"}
    """
    if not pmid or not pmid.startswith("BIRADS"):
        return {"modality": "mammography"}

    params = {"modality": "mammography"}

    if "_CHUNK_" in pmid:
        parts = pmid.split("_CHUNK_")
        parent = parts[0]
        chunk_name = parts[1]

        if "SECTION_IV" in parent:
            params["section"] = "BIRADS_2025_SECTION_IV"
            params["sub"] = parent
            params["chunk"] = chunk_name
        elif "SECTION_V" in parent:
            params["section"] = "BIRADS_2025_SECTION_V"
            params["sub"] = parent
            params["chunk"] = chunk_name
    else:
        if "SECTION_IV" in pmid and len(pmid) > len("BIRADS_2025_SECTION_IV"):
            params["section"] = "BIRADS_2025_SECTION_IV"
            params["sub"] = pmid
        elif "SECTION_V" in pmid and len(pmid) > len("BIRADS_2025_SECTION_V"):
            params["section"] = "BIRADS_2025_SECTION_V"
            params["sub"] = pmid
        else:
            params["section"] = pmid
            params["view"] = "content"

    return params


def is_korean(text: str) -> bool:
    """í…ìŠ¤íŠ¸ì— í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    import re
    return bool(re.search(r'[ê°€-í£]', text))


def get_messages(is_ko: bool) -> dict:
    """ì–¸ì–´ë³„ ë©”ì‹œì§€ ë°˜í™˜"""
    if is_ko:
        return {
            "found_high": "ğŸ“˜ **BI-RADS ê°€ì´ë“œë¼ì¸ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.**\n\nì•„ë˜ ì›ë¬¸ì„ í™•ì¸í•´ì£¼ì„¸ìš”.",
            "found_medium": "ğŸ“‹ **BI-RADS ê°€ì´ë“œë¼ì¸ì—ì„œ ê´€ë ¨ë  ìˆ˜ ìˆëŠ” ë‚´ìš©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.**\n\nâš ï¸ _{reason}_\n\nì•„ë˜ ì›ë¬¸ì„ í™•ì¸í•´ì£¼ì„¸ìš”.",
            "not_found": "ğŸ“­ **BI-RADS ê°€ì´ë“œë¼ì¸ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.**\n\n_{reason}_",
            "no_results": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ ì£¼ì„¸ìš”.",
            "view_source": "ğŸ“˜ ì›ë¬¸ í™•ì¸í•˜ê¸°",
            "papers_high": "ğŸ“„ ê´€ë ¨ ì—°êµ¬ ë…¼ë¬¸",
            "papers_medium": "ğŸ“„ ê´€ë ¨ë  ìˆ˜ ìˆëŠ” ì—°êµ¬ ë…¼ë¬¸ âš ï¸",
            "verifying": "ğŸ” ë¬¸ì„œ ê´€ë ¨ì„± ê²€ì¦ ì¤‘...",
            "searching": "ğŸ“š ê´€ë ¨ ë…¼ë¬¸ ê²€ìƒ‰ ì¤‘..."
        }
    else:
        return {
            "found_high": "ğŸ“˜ **Found relevant content in BI-RADS Guidelines.**\n\nPlease check the original text below.",
            "found_medium": "ğŸ“‹ **Found possibly relevant content in BI-RADS Guidelines.**\n\nâš ï¸ _{reason}_\n\nPlease check the original text below.",
            "not_found": "ğŸ“­ **No relevant content found in BI-RADS Guidelines.**\n\n_{reason}_",
            "no_results": "No search results. Please try different keywords.",
            "view_source": "ğŸ“˜ View Original",
            "papers_high": "ğŸ“„ Related Research Papers",
            "papers_medium": "ğŸ“„ Possibly Related Research Papers âš ï¸",
            "verifying": "ğŸ” Verifying document relevance...",
            "searching": "ğŸ“š Searching for related papers..."
        }


def enhance_query_with_context(current_question: str, chat_history: list, model="qwen2.5:14b") -> str:
    """
    ì´ì „ ëŒ€í™” ë§¥ë½ì„ ì°¸ê³ í•´ì„œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ë³´ê°•

    Args:
        current_question: í˜„ì¬ ì§ˆë¬¸
        chat_history: ì´ì „ ëŒ€í™” ê¸°ë¡
        model: LLM ëª¨ë¸ëª…

    Returns:
        ë³´ê°•ëœ ê²€ìƒ‰ ì¿¼ë¦¬
    """
    # ì´ì „ ëŒ€í™”ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì§ˆë¬¸ ë°˜í™˜
    if not chat_history:
        return current_question

    # ì§ˆë¬¸ì´ ì¶©ë¶„íˆ ê¸¸ê³  ì°¸ì¡° ë‹¨ì–´ê°€ ì—†ìœ¼ë©´ ìƒˆ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼ (ëŒ€í™” ë³´ê°• ê±´ë„ˆë›°ê¸°)
    reference_words = ["ë”", "ê·¸ê±°", "ê·¸ê²ƒ", "ìœ„ì—ì„œ", "ì•„ê¹Œ", "ë°©ê¸ˆ", "ì´ì „", "ê·¸ëŸ¬ë©´", "ê·¸ëŸ¼"]
    has_reference = any(word in current_question for word in reference_words)
    if len(current_question) > 20 and not has_reference:
        return current_question

    # ìµœê·¼ 4ê°œ ë©”ì‹œì§€ë§Œ ì‚¬ìš© (í† í° ì ˆì•½)
    recent_history = chat_history[-4:]

    # ëŒ€í™” ê¸°ë¡ í¬ë§·íŒ…
    history_text = ""
    for msg in recent_history:
        role = "ì‚¬ìš©ì" if msg["role"] == "user" else "ì–´ì‹œìŠ¤í„´íŠ¸"
        content = msg["content"][:200]  # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
        history_text += f"{role}: {content}\n"

    url = "http://localhost:11434/api/chat"

    system_message = """ë‹¹ì‹ ì€ ê²€ìƒ‰ ì¿¼ë¦¬ ë³´ê°• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ë¥¼ ì°¸ì¡°í•˜ëŠ”ì§€ íŒë‹¨í•˜ê³ , ê²€ìƒ‰ì— ì í•©í•œ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”.

ê·œì¹™:
1. í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ë§¥ë½ì„ ì°¸ì¡°í•˜ë©´ â†’ ë§¥ë½ì„ í¬í•¨í•œ ì™„ì „í•œ ì¿¼ë¦¬ ìƒì„±
2. í˜„ì¬ ì§ˆë¬¸ì´ ìƒˆë¡œìš´ ì£¼ì œë©´ â†’ ì›ë³¸ ì§ˆë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
3. ê²€ìƒ‰ ì¿¼ë¦¬ë§Œ ì¶œë ¥ (ì„¤ëª… ì—†ì´)
4. **ì¤‘ìš”**: ì…ë ¥ ì–¸ì–´(í•œêµ­ì–´)ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€. ì ˆëŒ€ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ì§€ ë§ˆì„¸ìš”!

ì˜ˆì‹œ:
- ì´ì „: "massì˜ margin ë¶„ë¥˜ëŠ”?" / í˜„ì¬: "ë” ìì„¸íˆ" â†’ "mass margin ë¶„ë¥˜ ìƒì„¸ ì„¤ëª…"
- ì´ì „: "calcification ì¢…ë¥˜" / í˜„ì¬: "MRI ì›ë¦¬ëŠ”?" â†’ "MRI ì›ë¦¬" (ìƒˆ ì£¼ì œ)
- ì´ì „: "BI-RADS ì¹´í…Œê³ ë¦¬" / í˜„ì¬: "3ì€ ë­ì•¼?" â†’ "BI-RADS ì¹´í…Œê³ ë¦¬ 3 ì˜ë¯¸"
"""

    user_message = f"""ì´ì „ ëŒ€í™”:
{history_text}

í˜„ì¬ ì§ˆë¬¸: {current_question}

ê²€ìƒ‰ ì¿¼ë¦¬:"""

    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_message}
    ]

    payload = {
        "model": model,
        "messages": messages,
        "stream": False,
        "options": {"temperature": 0.1}
    }

    try:
        response = requests.post(url, json=payload, timeout=15)
        response.raise_for_status()
        result = response.json()
        enhanced = result.get("message", {}).get("content", "").strip()

        # ë¹ˆ ê²°ê³¼ë©´ ì›ë³¸ ë°˜í™˜
        if not enhanced:
            return current_question

        return enhanced
    except Exception:
        # ì˜¤ë¥˜ì‹œ ì›ë³¸ ì§ˆë¬¸ ë°˜í™˜
        return current_question


def call_llm_with_context(question: str, context: str, model="qwen2.5:14b", temperature=0.7):
    """
    RAG: ê²€ìƒ‰ëœ ë…¼ë¬¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLM ë‹µë³€ ìƒì„±

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        context: ê²€ìƒ‰ëœ ë…¼ë¬¸ ë‚´ìš©
        model: LLM ëª¨ë¸ëª…
        temperature: ì˜¨ë„ ì„¤ì •

    Returns:
        LLM ë‹µë³€ (generator)
    """
    url = "http://localhost:11434/api/chat"

    system_message = """ë‹¹ì‹ ì€ ìœ ë°©ì˜ìƒì˜í•™ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

**ì¤‘ìš”í•œ ê·œì¹™:**
1. ì ˆëŒ€ë¡œ ë‚´ìš©ì„ ìš”ì•½í•˜ê±°ë‚˜ í•´ì„í•˜ì§€ ë§ˆì„¸ìš” - í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€
2. ì§ˆë¬¸ì— í•´ë‹¹í•˜ëŠ” ì¶œì²˜ ë²ˆí˜¸ë§Œ ì•ˆë‚´í•˜ì„¸ìš” (ì˜ˆ: "[1]ë²ˆ BI-RADS ê°€ì´ë“œë¼ì¸ì„ ì°¸ì¡°í•˜ì„¸ìš”")
3. ì•„ë˜ì— ì›ë¬¸ì´ í‘œì‹œë˜ë‹ˆ ì‚¬ìš©ìê°€ ì§ì ‘ í™•ì¸í•˜ë„ë¡ ì•ˆë‚´í•˜ì„¸ìš”
4. í•œêµ­ì–´ë¡œ ê°„ë‹¨íˆ ë‹µë³€í•˜ì„¸ìš” (1-2ë¬¸ì¥)"""

    user_message = f"""ë‹¤ìŒ ìë£Œ ì¤‘ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆëŠ” ì¶œì²˜ ë²ˆí˜¸ë¥¼ ì•ˆë‚´í•´ì£¼ì„¸ìš”.
ë‚´ìš©ì„ ìš”ì•½í•˜ì§€ ë§ê³ , ì¶œì²˜ ë²ˆí˜¸ë§Œ ì•Œë ¤ì£¼ì„¸ìš”.

**ì°¸ê³  ìë£Œ:**
{context}

**ì§ˆë¬¸:** {question}

**ë‹µë³€ ì˜ˆì‹œ:**
"[1]ë²ˆ BI-RADS ê°€ì´ë“œë¼ì¸ì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ì›ë¬¸ì„ ì°¸ì¡°í•´ì£¼ì„¸ìš”."
"""

    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_message}
    ]

    payload = {
        "model": model,
        "messages": messages,
        "stream": True,
        "options": {
            "temperature": temperature,
        }
    }

    try:
        response = requests.post(url, json=payload, stream=True, timeout=180)
        response.raise_for_status()

        for line in response.iter_lines():
            if line:
                chunk = json.loads(line)
                if "message" in chunk and "content" in chunk["message"]:
                    yield chunk["message"]["content"]
    except requests.exceptions.RequestException as e:
        yield f"âš ï¸ LLM ì—°ê²° ì˜¤ë¥˜: {str(e)}"


def verify_relevance(question: str, documents: list, model="qwen2.5:14b") -> dict:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì§ˆë¬¸ì— ê´€ë ¨ì´ ìˆëŠ”ì§€ LLMìœ¼ë¡œ 3ë‹¨ê³„ ê²€ì¦

    Returns:
        {"level": "high"/"medium"/"low", "reason": str, "relevant_indices": list}
    """
    url = "http://localhost:11434/api/chat"

    # ë¬¸ì„œ ë‚´ìš© ìš”ì•½ (ì²« 500ìì”©)
    doc_summaries = []
    for i, doc in enumerate(documents, 1):
        content = doc.get('content', '')[:500]
        title = doc.get('title', '')
        doc_summaries.append(f"[{i}] {title}\n{content}...")

    docs_text = "\n\n".join(doc_summaries)

    system_message = """ë‹¹ì‹ ì€ ë¬¸ì„œ ê´€ë ¨ì„± ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì œê³µëœ ë¬¸ì„œë“¤ì´ **ì§ì ‘ì ì¸ ë‹µ**ì„ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ 3ë‹¨ê³„ë¡œ íŒë‹¨í•˜ì„¸ìš”.

**íŒë‹¨ ê¸°ì¤€:**
- high: ë¬¸ì„œê°€ ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µì„ í¬í•¨í•¨
  ì˜ˆ: "margin ë¶„ë¥˜ëŠ”?" â†’ ë¬¸ì„œì— margin ë¶„ë¥˜ ëª©ë¡ê³¼ ì„¤ëª…ì´ ìˆìŒ
- medium: ë¬¸ì„œê°€ ê´€ë ¨ ì£¼ì œë¥¼ ë‹¤ë£¨ì§€ë§Œ ì§ì ‘ì  ë‹µì€ ì—†ìŒ
  ì˜ˆ: "Mammography ê¸°ë³¸ ê°œë…?" â†’ ë¬¸ì„œì— Mammography ì–¸ê¸‰ë§Œ ìˆê³  ê¸°ë³¸ ê°œë… ì„¤ëª…(ì›ë¦¬, ë°©ë²• ë“±)ì€ ì—†ìŒ
- low: ë¬¸ì„œê°€ ì§ˆë¬¸ê³¼ ê±°ì˜ ê´€ë ¨ ì—†ìŒ
  ì˜ˆ: "MRI ì´¬ì˜ ë°©ë²•?" â†’ ë¬¸ì„œì— Mammographyë§Œ ìˆê³  MRI ì •ë³´ ì—†ìŒ

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{"level": "high/medium/low", "reason": "íŒë‹¨ ì´ìœ ", "relevant_indices": [ê´€ë ¨ ë¬¸ì„œ ë²ˆí˜¸ë“¤]}"""

    user_message = f"""ì§ˆë¬¸: {question}

ê²€ìƒ‰ëœ ë¬¸ì„œë“¤:
{docs_text}

ì´ ë¬¸ì„œë“¤ì´ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ í¬í•¨í•˜ê³  ìˆë‚˜ìš”? JSONìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."""

    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_message}
    ]

    payload = {
        "model": model,
        "messages": messages,
        "stream": False,
        "options": {"temperature": 0.1}
    }

    try:
        response = requests.post(url, json=payload, timeout=30)
        response.raise_for_status()
        result = response.json()
        content = result.get("message", {}).get("content", "")

        # JSON íŒŒì‹± ì‹œë„
        import re
        json_match = re.search(r'\{[^}]+\}', content)
        if json_match:
            parsed = json.loads(json_match.group())
            return {
                "level": parsed.get("level", "medium"),
                "reason": parsed.get("reason", ""),
                "relevant_indices": parsed.get("relevant_indices", [])
            }

        # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’ (medium)
        return {"level": "medium", "reason": "ê²€ì¦ ë¶ˆê°€", "relevant_indices": list(range(1, len(documents)+1))}

    except Exception as e:
        # ì˜¤ë¥˜ì‹œ mediumìœ¼ë¡œ ì²˜ë¦¬
        return {"level": "medium", "reason": f"ê²€ì¦ ì˜¤ë¥˜: {e}", "relevant_indices": list(range(1, len(documents)+1))}


# =============================================================================
# ì‚¬ì´ë“œë°”
# =============================================================================

def render_sidebar():
    """ì‚¬ì´ë“œë°” ë Œë”ë§"""
    with st.sidebar:
        st.markdown("### âš™ï¸ Settings")

        st.markdown("#### Model")
        model = st.selectbox(
            "LLM Model",
            options=["qwen2.5:14b"],
            index=0,
        )

        temperature = st.slider(
            "Temperature",
            min_value=0.0,
            max_value=1.0,
            value=0.3,
            step=0.1,
            help="ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì , ë†’ì„ìˆ˜ë¡ ì°½ì˜ì "
        )

        st.markdown("#### Search")
        top_k = st.slider(
            "ì°¸ê³  ë…¼ë¬¸ ìˆ˜",
            min_value=3,
            max_value=10,
            value=5,
            step=1,
            help="ë‹µë³€ ìƒì„± ì‹œ ì°¸ê³ í•  ë…¼ë¬¸ ìˆ˜"
        )

        st.markdown("---")

        if st.button("ğŸ—‘ï¸ Clear Chat", use_container_width=True):
            st.session_state.messages = []
            st.rerun()

        st.markdown("---")
        st.markdown("#### About")
        st.markdown("""
        <div style="font-size: 0.85rem; line-height: 1.6;">
        <b>Sophia AI Alpha</b><br>
        ìœ ë°©ì˜ìƒì˜í•™ ë…¼ë¬¸ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ<br><br>

        âœ… í• ë£¨ì‹œë„¤ì´ì…˜ ìµœì†Œí™”<br>
        âœ… ë…¼ë¬¸ ì¶œì²˜ ëª…ì‹œ<br>
        âœ… BI-RADS ê°€ì´ë“œë¼ì¸ ì°¸ì¡°
        </div>
        """, unsafe_allow_html=True)

        return {
            "model": model,
            "temperature": temperature,
            "top_k": top_k,
        }

# =============================================================================
# ë©”ì¸ UI
# =============================================================================

def main():
    """ë©”ì¸ RAG ì±—ë´‡"""

    # ì‚¬ì´ë“œë°”
    options = render_sidebar()

    # í—¤ë”
    st.markdown("""
    <h1 style='font-size: 2.5rem; margin-bottom: 0;'>
        ğŸ’¬ Sophia AI
        <span style='font-size: 0.9rem; color: #888888; font-weight: normal; vertical-align: super;'>Alpha</span>
    </h1>
    """, unsafe_allow_html=True)
    st.caption("ğŸš€ ìœ ë°©ì˜ìƒì˜í•™ ë…¼ë¬¸ ê¸°ë°˜ AI ì–´ì‹œìŠ¤í„´íŠ¸ (RAG)")

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "messages" not in st.session_state:
        st.session_state["messages"] = [{
            "role": "assistant",
            "content": "ì•ˆë…•í•˜ì„¸ìš”! ìœ ë°©ì˜ìƒì˜í•™ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´, ê´€ë ¨ ë…¼ë¬¸ê³¼ BI-RADS ê°€ì´ë“œë¼ì¸ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n\nì˜ˆì‹œ:\n- Mammographyì— ëŒ€í•œ ê¸°ë³¸ ê°œë… ì„¤ëª…\n- DBTì™€ FFDMì˜ ì°¨ì´ì \n- BI-RADS ì¹´í…Œê³ ë¦¬ ì„¤ëª…"
        }]

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

            # ì¶œì²˜ í‘œì‹œ
            if "sources" in msg:
                with st.expander("ğŸ“š ì°¸ê³  ìë£Œ", expanded=False):
                    for i, source in enumerate(msg["sources"], 1):
                        icon = "ğŸ“˜" if source.get("is_birads", False) else "ğŸ“„"

                        if source.get("is_birads", False):
                            # BI-RADS ë¬¸ì„œëŠ” ë§ˆí¬ë‹¤ìš´ ë§í¬
                            pmid = source.get('pmid', '')
                            nav_params = get_birads_nav_params(pmid)
                            param_str = "&".join([f"{k}={v}" for k, v in nav_params.items()])
                            page_url = f"/BI-RADS_Guidelines?{param_str}"
                            st.markdown(f"""
                            **{icon} [{i}] {source['title']}**
                            {source['authors']} - {source['journal']} ({source['year']})
                            [ğŸ“˜ ì›ë¬¸ í™•ì¸í•˜ê¸°]({page_url})
                            """)
                        else:
                            # ì¼ë°˜ ë…¼ë¬¸ì€ PubMed ë§í¬
                            st.markdown(f"""
                            **{icon} [{i}] {source['title']}**
                            {source['authors']} - {source['journal']} ({source['year']})
                            [PubMed ë³´ê¸°]({source['url']})
                            """)

    # ì±„íŒ… ì…ë ¥
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})

        # ì–¸ì–´ ê°ì§€ ë° ë©”ì‹œì§€ ì„¤ì • (ê²€ìƒ‰ ì „ì— ë¯¸ë¦¬ ì„¤ì •)
        is_ko = is_korean(prompt)
        msg = get_messages(is_ko)

        with st.chat_message("user"):
            st.markdown(prompt)

        # ê²€ìƒ‰ ì—”ì§„ìœ¼ë¡œ ê´€ë ¨ ë…¼ë¬¸ ê²€ìƒ‰
        with st.spinner(msg["searching"]):
            try:
                engine = get_search_engine()
                translator = get_query_translator()

                # ëŒ€í™”í˜• ì¿¼ë¦¬ ë³´ê°• (ì´ì „ ë§¥ë½ ì°¸ì¡°)
                enhanced_prompt = enhance_query_with_context(
                    current_question=prompt,
                    chat_history=st.session_state.messages[:-1],  # í˜„ì¬ ì§ˆë¬¸ ì œì™¸
                    model=options["model"]
                )
                if enhanced_prompt != prompt:
                    st.caption(f"ğŸ’¬ ëŒ€í™” ë§¥ë½ ë°˜ì˜: `{enhanced_prompt}`")

                # ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”
                search_query = enhanced_prompt
                prompt_lower = enhanced_prompt.lower()

                # 0. LLM ê¸°ë°˜ ì¿¼ë¦¬ ë²ˆì—­ (í•œê¸€ â†’ ì˜ë¬¸ ì˜í•™ í‚¤ì›Œë“œ)
                if translator.needs_translation(enhanced_prompt):
                    with st.spinner("ğŸ”„ ì¿¼ë¦¬ ìµœì í™” ì¤‘..."):
                        translated_query = translator.translate(enhanced_prompt)
                        if translated_query != enhanced_prompt:
                            search_query = translated_query
                            st.caption(f"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: `{translated_query}`")

                # 1. ì¿¼ë¦¬ í™•ì¥: ëª¨ë“  í•œê¸€ ë²ˆì—­ ì¿¼ë¦¬ì— BI-RADS ì¶”ê°€
                search_query_lower = search_query.lower()
                if 'bi-rads' not in search_query_lower and 'birads' not in search_query_lower:
                    if translator.needs_translation(enhanced_prompt):
                        search_query = f"BI-RADS {search_query}"
                        st.caption(f"âœ¨ ì¿¼ë¦¬ í™•ì¥: `{search_query}`")

                # ì´ì¤‘ ê²€ìƒ‰: BI-RADS + ì—°êµ¬ë…¼ë¬¸
                birads_response, papers_response = engine.search_dual(
                    search_query,
                    birads_k=3,
                    papers_k=5
                )

                if not birads_response.results and not papers_response.results:
                    with st.chat_message("assistant"):
                        error_msg = msg["no_results"]
                        st.markdown(error_msg)
                        st.session_state.messages.append({"role": "assistant", "content": error_msg})
                    st.stop()

                # BI-RADS ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                birads_context_parts = []
                birads_sources = []

                for i, result in enumerate(birads_response.results, 1):
                    paper = result.paper
                    content_text = getattr(paper, 'full_content', paper.abstract or 'ë‚´ìš© ì—†ìŒ')

                    birads_context_parts.append(f"""
[{i}] ğŸ“˜ BI-RADS ê°€ì´ë“œë¼ì¸
ì œëª©: {paper.title}
ë‚´ìš©: {content_text}
""")

                    birads_sources.append({
                        "title": paper.title,
                        "authors": paper.author_string or "American College of Radiology",
                        "journal": paper.journal or "ACR BI-RADS Atlas v2025",
                        "year": paper.year or "2025",
                        "pmid": paper.pmid,
                        "is_birads": True,
                        "full_content": getattr(paper, 'full_content', None)
                    })

                birads_context = "\n".join(birads_context_parts) if birads_context_parts else ""

                # ì—°êµ¬ë…¼ë¬¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
                papers_context_parts = []
                papers_sources = []

                for i, result in enumerate(papers_response.results, 1):
                    paper = result.paper
                    content_text = (paper.abstract[:500] + '...' if paper.abstract and len(paper.abstract) > 500 else paper.abstract or 'ì´ˆë¡ ì—†ìŒ')

                    papers_context_parts.append(f"""
[{i}] ğŸ“„ ì—°êµ¬ë…¼ë¬¸
ì œëª©: {paper.title}
ì €ì: {paper.author_string}
ì €ë„: {paper.journal} ({paper.year})
ë‚´ìš©: {content_text}
""")

                    papers_sources.append({
                        "title": paper.title,
                        "authors": paper.author_string or "ì €ì ì •ë³´ ì—†ìŒ",
                        "journal": paper.journal or "ì €ë„ ì •ë³´ ì—†ìŒ",
                        "year": paper.year or "ì—°ë„ ì •ë³´ ì—†ìŒ",
                        "url": paper.pubmed_url,
                        "is_birads": False
                    })

                papers_context = "\n".join(papers_context_parts) if papers_context_parts else ""

                # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ê²°í•© (BI-RADS ìš°ì„ , ì—°êµ¬ë…¼ë¬¸ í›„ìˆœìœ„)
                context_parts = []
                if birads_context:
                    context_parts.append("### ğŸ“˜ BI-RADS ê°€ì´ë“œë¼ì¸\n" + birads_context)
                if papers_context:
                    context_parts.append("\n### ğŸ“„ ê´€ë ¨ ì—°êµ¬ ë…¼ë¬¸\n" + papers_context)

                context = "\n\n".join(context_parts)
                # sourcesëŠ” ê²€ì¦ í›„ì— ì¶”ê°€ë¨ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”)
                sources = []

            except Exception as e:
                with st.chat_message("assistant"):
                    error_msg = f"âš ï¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
                    st.markdown(error_msg)
                    st.session_state.messages.append({"role": "assistant", "content": error_msg})
                st.stop()

        # ë‹µë³€ ìƒì„± (BI-RADSê°€ ìˆìœ¼ë©´ ê´€ë ¨ì„± ê²€ì¦ í›„ í‘œì‹œ, ì—†ìœ¼ë©´ LLM)
        with st.chat_message("assistant"):
            if birads_sources:
                # BI-RADS ë¬¸ì„œ ê´€ë ¨ì„± ê²€ì¦
                with st.spinner(msg["verifying"]):
                    relevance = verify_relevance(
                        question=prompt,
                        documents=birads_sources,
                        model=options["model"]
                    )

                level = relevance.get("level", "medium")
                reason = relevance.get("reason", "")

                if level == "low":
                    # ê´€ë ¨ ì—†ìŒ - BI-RADSë¥¼ ê±´ë„ˆë›°ê³  ì¼ë°˜ ë…¼ë¬¸ìœ¼ë¡œ ì§„í–‰
                    full_response = msg["not_found"].format(reason=reason)
                    st.markdown(full_response)
                    birads_sources = []  # ì†ŒìŠ¤ì—ì„œ ì œê±° (ë…¼ë¬¸ì€ ì•„ë˜ì—ì„œ ê²€ì¦ í›„ ì¶”ê°€ë¨)
                else:
                    # high ë˜ëŠ” medium - ë¬¸ì„œ í‘œì‹œ
                    relevant_indices = relevance.get("relevant_indices", [])
                    if relevant_indices:
                        filtered_sources = [birads_sources[i-1] for i in relevant_indices if 0 < i <= len(birads_sources)]
                    else:
                        filtered_sources = birads_sources

                    # sourcesë¥¼ filtered_sourcesë¡œ ì—…ë°ì´íŠ¸ (ì €ì¥ìš©)
                    sources = filtered_sources

                    if level == "high":
                        full_response = msg["found_high"]
                    else:  # medium
                        full_response = msg["found_medium"].format(reason=reason)
                    st.markdown(full_response)

                    st.markdown("---")
                    for i, source in enumerate(filtered_sources, 1):
                        pmid = source.get('pmid', '')
                        nav_params = get_birads_nav_params(pmid)
                        param_str = "&".join([f"{k}={v}" for k, v in nav_params.items()])
                        page_url = f"/BI-RADS_Guidelines?{param_str}"

                        st.markdown(f"### [{i}] {source['title']}")
                        st.markdown(f"_{source['authors']} - {source['journal']} ({source['year']})_")
                        st.markdown(f"[{msg['view_source']}]({page_url})")
                        st.markdown("---")
            else:
                # BI-RADS ì—†ìœ¼ë©´ LLMìœ¼ë¡œ ë‹µë³€ ìƒì„±
                message_placeholder = st.empty()
                full_response = ""

                for chunk in call_llm_with_context(
                    question=prompt,
                    context=context,
                    model=options["model"],
                    temperature=options["temperature"]
                ):
                    full_response += chunk
                    message_placeholder.markdown(full_response + "â–Œ")

                message_placeholder.markdown(full_response)

            if papers_sources:
                # ë…¼ë¬¸ ê´€ë ¨ì„± ê²€ì¦
                with st.spinner(msg["verifying"]):
                    paper_relevance = verify_relevance(
                        question=prompt,
                        documents=papers_sources,
                        model=options["model"]
                    )

                paper_level = paper_relevance.get("level", "medium")
                paper_reason = paper_relevance.get("reason", "")
                paper_indices = paper_relevance.get("relevant_indices", [])

                if paper_level != "low":
                    # ê´€ë ¨ ìˆëŠ” ë…¼ë¬¸ë§Œ í•„í„°ë§
                    if paper_indices:
                        filtered_papers = [papers_sources[i-1] for i in paper_indices if 0 < i <= len(papers_sources)]
                    else:
                        filtered_papers = papers_sources

                    if filtered_papers:
                        # sourcesì— ê´€ë ¨ ë…¼ë¬¸ ì¶”ê°€
                        sources = sources + filtered_papers

                        if paper_level == "high":
                            expander_title = msg["papers_high"]
                        else:  # medium
                            expander_title = msg["papers_medium"]

                        pubmed_text = "PubMed" if not is_ko else "PubMed ë³´ê¸°"
                        with st.expander(expander_title, expanded=False):
                            if paper_level == "medium":
                                st.caption(f"_{paper_reason}_")
                            for i, source in enumerate(filtered_papers, 1):
                                st.markdown(f"""
                                **ğŸ“„ [{i}] {source['title']}**
                                {source['authors']} - {source['journal']} ({source['year']})
                                [{pubmed_text}]({source['url']})
                                """)

        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥ (ì¶œì²˜ í¬í•¨, ë”¥ì¹´í”¼ë¡œ ì €ì¥)
        st.session_state.messages.append({
            "role": "assistant",
            "content": full_response,
            "sources": copy.deepcopy(sources)  # ë”¥ì¹´í”¼ë¡œ ì°¸ì¡° ì™„ì „ ë¶„ë¦¬
        })


if __name__ == "__main__":
    main()
